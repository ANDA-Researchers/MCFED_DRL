{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "result_dir = os.path.join(os.getcwd(), \"results_True\")\n",
    "results = os.listdir(result_dir)\n",
    "\n",
    "\n",
    "def load(f):\n",
    "    file_dir = os.path.join(result_dir, f, \"results.json\")\n",
    "    with open(file_dir, \"r\") as f:\n",
    "\n",
    "        results = json.load(f)\n",
    "    return results\n",
    "\n",
    "\n",
    "# create a function that captalizes the first letter of a word\n",
    "def capitalize(string):\n",
    "    return string[0].upper() + string[1:]\n",
    "\n",
    "\n",
    "def load_from_query(query):\n",
    "    results = []\n",
    "    for f in os.listdir(result_dir):\n",
    "        if f.find(query) != -1:\n",
    "            return load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def show(data, scale=1, labels=None):\n",
    "    for i, d in enumerate(data):\n",
    "        print(\n",
    "            (\"\" if not labels else labels[i])\n",
    "            + \" \"\n",
    "            + \"|\".join([f\"{x*scale if x != None else -1:.4f}\" for x in d])\n",
    "        )\n",
    "\n",
    "    # Create a folder called \"csv\" if it doesn't exist\n",
    "    csv_dir = os.path.join(os.getcwd(), \"csv\")\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "    # Write the data to CSV files in the \"csv\" folder\n",
    "    pd.DataFrame(avg_delay, index=labels, columns=cache_sizes).to_csv(\n",
    "        os.path.join(csv_dir, \"avg_delay.csv\")\n",
    "    )\n",
    "    pd.DataFrame(hit_ratio, index=labels, columns=cache_sizes).to_csv(\n",
    "        os.path.join(csv_dir, \"hit_ratio.csv\")\n",
    "    )\n",
    "    pd.DataFrame(success_ratio, index=labels, columns=cache_sizes).to_csv(\n",
    "        os.path.join(csv_dir, \"success_ratio.csv\")\n",
    "    )\n",
    "\n",
    "\n",
    "cache_sizes = [\n",
    "    50,\n",
    "    100,\n",
    "    150,\n",
    "    200,\n",
    "    250,\n",
    "    300,\n",
    "]\n",
    "NR = 3\n",
    "Nr = 30\n",
    "\n",
    "avg_delay = []\n",
    "hit_ratio = []\n",
    "success_ratio = []\n",
    "labels = []\n",
    "\n",
    "for cache in [\n",
    "    \"random\",\n",
    "    \"mcfed\",\n",
    "    \"avgfed\",\n",
    "    \"nocache\",\n",
    "]:  \n",
    "    for vehicle_num in [\n",
    "            # 10,\n",
    "            30,\n",
    "            # 50,\n",
    "            # 70,\n",
    "            # 90,\n",
    "            # 110\n",
    "        ]:\n",
    "        for delivery in [\"random\", \"greedy\", \"drl\", \"norsu\"]:\n",
    "            if cache in [\"random\", \"avgfed\", \"nocache\"] and delivery not in [\n",
    "                \"drl\",\n",
    "                \"norsu\",\n",
    "                # \"greedy\"\n",
    "            ]:\n",
    "                continue\n",
    "            if delivery == \"norsu\" and cache != \"random\":\n",
    "                continue\n",
    "\n",
    "            labels.append(f\"{vehicle_num}_{capitalize(cache)}_{capitalize(delivery)}\")\n",
    "            sub_data1 = []\n",
    "            sub_data2 = []\n",
    "            sub_data3 = []\n",
    "            for cache_size in cache_sizes:\n",
    "                query = f\"{NR}_{cache_size}_{Nr}_{cache}_{delivery}\"\n",
    "                result = load_from_query(query)\n",
    "                if result:\n",
    "                    sub_data1.append(np.mean(result[\"round_avg_delay_tracking\"]))\n",
    "                    sub_data2.append(np.mean(result[\"round_hit_ratio_tracking\"]))\n",
    "                    sub_data3.append(np.mean(result[\"round_success_ratio_tracking\"]))\n",
    "                else:\n",
    "                    sub_data1.append(None)\n",
    "                    sub_data2.append(None)\n",
    "                    sub_data3.append(None)\n",
    "            avg_delay.append(sub_data1)\n",
    "            hit_ratio.append(sub_data2)\n",
    "            success_ratio.append(sub_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['30_Random_Drl',\n",
       " '30_Random_Norsu',\n",
       " '30_Mcfed_Random',\n",
       " '30_Mcfed_Greedy',\n",
       " '30_Mcfed_Drl',\n",
       " '30_Avgfed_Drl',\n",
       " '30_Nocache_Drl']"
      ]
     },
     "execution_count": 3302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30_Random_Drl 273.8830|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Random_Norsu 363.2236|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Mcfed_Random 282.0756|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Mcfed_Greedy 272.6786|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Mcfed_Drl 271.6521|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Avgfed_Drl 268.4120|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Nocache_Drl 280.1246|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n"
     ]
    }
   ],
   "source": [
    "show(avg_delay, 1000, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30_Random_Drl 1.6056|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Random_Norsu 0.0000|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Mcfed_Random 5.4222|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Mcfed_Greedy 20.8278|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Mcfed_Drl 9.1111|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Avgfed_Drl 9.5778|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Nocache_Drl 0.0000|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n"
     ]
    }
   ],
   "source": [
    "show(hit_ratio, 100, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30_Random_Drl 90.8056|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Random_Norsu 100.0000|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Mcfed_Random 76.6000|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Mcfed_Greedy 78.0444|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Mcfed_Drl 89.2056|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Avgfed_Drl 90.2333|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n",
      "30_Nocache_Drl 88.4278|-1.0000|-1.0000|-1.0000|-1.0000|-1.0000\n"
     ]
    }
   ],
   "source": [
    "show(success_ratio, 100, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcfed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
