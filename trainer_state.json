{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 100,
  "global_step": 2815,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0017761989342806395,
      "grad_norm": 1.3606815338134766,
      "learning_rate": 4e-05,
      "loss": 3.7642,
      "step": 1
    },
    {
      "epoch": 0.003552397868561279,
      "grad_norm": 1.6146169900894165,
      "learning_rate": 8e-05,
      "loss": 3.7115,
      "step": 2
    },
    {
      "epoch": 0.0053285968028419185,
      "grad_norm": 1.3478877544403076,
      "learning_rate": 0.00012,
      "loss": 3.7045,
      "step": 3
    },
    {
      "epoch": 0.007104795737122558,
      "grad_norm": 1.1370067596435547,
      "learning_rate": 0.00016,
      "loss": 3.574,
      "step": 4
    },
    {
      "epoch": 0.008880994671403197,
      "grad_norm": 0.828605055809021,
      "learning_rate": 0.0002,
      "loss": 3.1633,
      "step": 5
    },
    {
      "epoch": 0.010657193605683837,
      "grad_norm": 1.2127867937088013,
      "learning_rate": 0.0001999999375033031,
      "loss": 3.1808,
      "step": 6
    },
    {
      "epoch": 0.012433392539964476,
      "grad_norm": 1.3905673027038574,
      "learning_rate": 0.0001999997500132904,
      "loss": 2.8369,
      "step": 7
    },
    {
      "epoch": 0.014209591474245116,
      "grad_norm": 1.1407901048660278,
      "learning_rate": 0.0001999994375301963,
      "loss": 2.7985,
      "step": 8
    },
    {
      "epoch": 0.015985790408525755,
      "grad_norm": 1.22878098487854,
      "learning_rate": 0.0001999990000544114,
      "loss": 2.4646,
      "step": 9
    },
    {
      "epoch": 0.017761989342806393,
      "grad_norm": 1.1621127128601074,
      "learning_rate": 0.00019999843758648252,
      "loss": 2.376,
      "step": 10
    },
    {
      "epoch": 0.019538188277087035,
      "grad_norm": 0.9163625836372375,
      "learning_rate": 0.00019999775012711268,
      "loss": 2.4247,
      "step": 11
    },
    {
      "epoch": 0.021314387211367674,
      "grad_norm": 4.730112075805664,
      "learning_rate": 0.00019999693767716115,
      "loss": 2.6629,
      "step": 12
    },
    {
      "epoch": 0.023090586145648313,
      "grad_norm": 0.7234392762184143,
      "learning_rate": 0.00019999600023764346,
      "loss": 2.2666,
      "step": 13
    },
    {
      "epoch": 0.02486678507992895,
      "grad_norm": 1.0343822240829468,
      "learning_rate": 0.00019999493780973134,
      "loss": 2.2102,
      "step": 14
    },
    {
      "epoch": 0.02664298401420959,
      "grad_norm": 0.7522276639938354,
      "learning_rate": 0.00019999375039475277,
      "loss": 1.7529,
      "step": 15
    },
    {
      "epoch": 0.028419182948490232,
      "grad_norm": 0.614875853061676,
      "learning_rate": 0.0001999924379941919,
      "loss": 2.3516,
      "step": 16
    },
    {
      "epoch": 0.03019538188277087,
      "grad_norm": 0.6663753390312195,
      "learning_rate": 0.0001999910006096892,
      "loss": 2.0282,
      "step": 17
    },
    {
      "epoch": 0.03197158081705151,
      "grad_norm": 0.559457004070282,
      "learning_rate": 0.00019998943824304125,
      "loss": 2.0634,
      "step": 18
    },
    {
      "epoch": 0.03374777975133215,
      "grad_norm": 0.6383615136146545,
      "learning_rate": 0.00019998775089620095,
      "loss": 2.1931,
      "step": 19
    },
    {
      "epoch": 0.035523978685612786,
      "grad_norm": 0.544040322303772,
      "learning_rate": 0.00019998593857127736,
      "loss": 2.2068,
      "step": 20
    },
    {
      "epoch": 0.037300177619893425,
      "grad_norm": 0.585866391658783,
      "learning_rate": 0.00019998400127053576,
      "loss": 1.765,
      "step": 21
    },
    {
      "epoch": 0.03907637655417407,
      "grad_norm": 0.5446944236755371,
      "learning_rate": 0.00019998193899639765,
      "loss": 2.2011,
      "step": 22
    },
    {
      "epoch": 0.04085257548845471,
      "grad_norm": 0.5579026937484741,
      "learning_rate": 0.00019997975175144075,
      "loss": 2.0889,
      "step": 23
    },
    {
      "epoch": 0.04262877442273535,
      "grad_norm": 0.5472596883773804,
      "learning_rate": 0.00019997743953839892,
      "loss": 2.4469,
      "step": 24
    },
    {
      "epoch": 0.04440497335701599,
      "grad_norm": 0.5493944883346558,
      "learning_rate": 0.0001999750023601623,
      "loss": 1.8482,
      "step": 25
    },
    {
      "epoch": 0.046181172291296625,
      "grad_norm": 0.5710822939872742,
      "learning_rate": 0.00019997244021977725,
      "loss": 2.2598,
      "step": 26
    },
    {
      "epoch": 0.047957371225577264,
      "grad_norm": 0.5521088242530823,
      "learning_rate": 0.00019996975312044625,
      "loss": 2.094,
      "step": 27
    },
    {
      "epoch": 0.0497335701598579,
      "grad_norm": 0.6419857740402222,
      "learning_rate": 0.00019996694106552796,
      "loss": 2.1339,
      "step": 28
    },
    {
      "epoch": 0.05150976909413854,
      "grad_norm": 0.6349115967750549,
      "learning_rate": 0.0001999640040585373,
      "loss": 2.0681,
      "step": 29
    },
    {
      "epoch": 0.05328596802841918,
      "grad_norm": 0.5916793346405029,
      "learning_rate": 0.00019996094210314528,
      "loss": 1.9226,
      "step": 30
    },
    {
      "epoch": 0.055062166962699825,
      "grad_norm": 0.5252842903137207,
      "learning_rate": 0.00019995775520317922,
      "loss": 2.2911,
      "step": 31
    },
    {
      "epoch": 0.056838365896980464,
      "grad_norm": 0.6066681742668152,
      "learning_rate": 0.0001999544433626225,
      "loss": 2.0815,
      "step": 32
    },
    {
      "epoch": 0.0586145648312611,
      "grad_norm": 0.6341618299484253,
      "learning_rate": 0.00019995100658561473,
      "loss": 2.2802,
      "step": 33
    },
    {
      "epoch": 0.06039076376554174,
      "grad_norm": 0.5672897696495056,
      "learning_rate": 0.00019994744487645158,
      "loss": 2.3265,
      "step": 34
    },
    {
      "epoch": 0.06216696269982238,
      "grad_norm": 0.6424731612205505,
      "learning_rate": 0.00019994375823958503,
      "loss": 2.0198,
      "step": 35
    },
    {
      "epoch": 0.06394316163410302,
      "grad_norm": 0.6805105805397034,
      "learning_rate": 0.00019993994667962306,
      "loss": 2.1252,
      "step": 36
    },
    {
      "epoch": 0.06571936056838366,
      "grad_norm": 0.6620866060256958,
      "learning_rate": 0.00019993601020132994,
      "loss": 2.138,
      "step": 37
    },
    {
      "epoch": 0.0674955595026643,
      "grad_norm": 0.5748094916343689,
      "learning_rate": 0.00019993194880962595,
      "loss": 2.4332,
      "step": 38
    },
    {
      "epoch": 0.06927175843694494,
      "grad_norm": 0.6389364004135132,
      "learning_rate": 0.00019992776250958761,
      "loss": 2.0819,
      "step": 39
    },
    {
      "epoch": 0.07104795737122557,
      "grad_norm": 0.7206365466117859,
      "learning_rate": 0.0001999234513064475,
      "loss": 1.865,
      "step": 40
    },
    {
      "epoch": 0.07282415630550622,
      "grad_norm": 0.6969729065895081,
      "learning_rate": 0.0001999190152055943,
      "loss": 1.9353,
      "step": 41
    },
    {
      "epoch": 0.07460035523978685,
      "grad_norm": 0.8611202836036682,
      "learning_rate": 0.0001999144542125729,
      "loss": 1.7712,
      "step": 42
    },
    {
      "epoch": 0.0763765541740675,
      "grad_norm": 0.6672370433807373,
      "learning_rate": 0.0001999097683330842,
      "loss": 1.9582,
      "step": 43
    },
    {
      "epoch": 0.07815275310834814,
      "grad_norm": 0.6486310958862305,
      "learning_rate": 0.00019990495757298528,
      "loss": 2.1245,
      "step": 44
    },
    {
      "epoch": 0.07992895204262877,
      "grad_norm": 0.6275030374526978,
      "learning_rate": 0.00019990002193828922,
      "loss": 2.3342,
      "step": 45
    },
    {
      "epoch": 0.08170515097690942,
      "grad_norm": 0.9801149964332581,
      "learning_rate": 0.00019989496143516528,
      "loss": 1.6783,
      "step": 46
    },
    {
      "epoch": 0.08348134991119005,
      "grad_norm": 0.8703701496124268,
      "learning_rate": 0.00019988977606993875,
      "loss": 1.7901,
      "step": 47
    },
    {
      "epoch": 0.0852575488454707,
      "grad_norm": 0.6517236232757568,
      "learning_rate": 0.000199884465849091,
      "loss": 2.2342,
      "step": 48
    },
    {
      "epoch": 0.08703374777975133,
      "grad_norm": 0.8320302367210388,
      "learning_rate": 0.0001998790307792594,
      "loss": 1.856,
      "step": 49
    },
    {
      "epoch": 0.08880994671403197,
      "grad_norm": 0.5991219878196716,
      "learning_rate": 0.0001998734708672375,
      "loss": 2.3633,
      "step": 50
    },
    {
      "epoch": 0.0905861456483126,
      "grad_norm": 0.9047343134880066,
      "learning_rate": 0.00019986778611997478,
      "loss": 1.9491,
      "step": 51
    },
    {
      "epoch": 0.09236234458259325,
      "grad_norm": 0.6126389503479004,
      "learning_rate": 0.0001998619765445768,
      "loss": 2.3048,
      "step": 52
    },
    {
      "epoch": 0.0941385435168739,
      "grad_norm": 0.754489004611969,
      "learning_rate": 0.00019985604214830516,
      "loss": 1.945,
      "step": 53
    },
    {
      "epoch": 0.09591474245115453,
      "grad_norm": 0.8084533214569092,
      "learning_rate": 0.00019984998293857747,
      "loss": 1.8105,
      "step": 54
    },
    {
      "epoch": 0.09769094138543517,
      "grad_norm": 0.6542416214942932,
      "learning_rate": 0.0001998437989229673,
      "loss": 2.021,
      "step": 55
    },
    {
      "epoch": 0.0994671403197158,
      "grad_norm": 0.6887090802192688,
      "learning_rate": 0.00019983749010920435,
      "loss": 2.0477,
      "step": 56
    },
    {
      "epoch": 0.10124333925399645,
      "grad_norm": 0.6697255373001099,
      "learning_rate": 0.0001998310565051741,
      "loss": 1.8571,
      "step": 57
    },
    {
      "epoch": 0.10301953818827708,
      "grad_norm": 0.507536768913269,
      "learning_rate": 0.00019982449811891824,
      "loss": 1.7287,
      "step": 58
    },
    {
      "epoch": 0.10479573712255773,
      "grad_norm": 0.4850851893424988,
      "learning_rate": 0.00019981781495863424,
      "loss": 1.9943,
      "step": 59
    },
    {
      "epoch": 0.10657193605683836,
      "grad_norm": 0.5533100366592407,
      "learning_rate": 0.00019981100703267565,
      "loss": 2.0182,
      "step": 60
    },
    {
      "epoch": 0.108348134991119,
      "grad_norm": 0.4985298812389374,
      "learning_rate": 0.00019980407434955192,
      "loss": 1.9634,
      "step": 61
    },
    {
      "epoch": 0.11012433392539965,
      "grad_norm": 0.6369288563728333,
      "learning_rate": 0.00019979701691792844,
      "loss": 1.9983,
      "step": 62
    },
    {
      "epoch": 0.11190053285968028,
      "grad_norm": 0.542351245880127,
      "learning_rate": 0.00019978983474662652,
      "loss": 2.2171,
      "step": 63
    },
    {
      "epoch": 0.11367673179396093,
      "grad_norm": 0.5864589214324951,
      "learning_rate": 0.00019978252784462344,
      "loss": 2.1865,
      "step": 64
    },
    {
      "epoch": 0.11545293072824156,
      "grad_norm": 0.5928520560264587,
      "learning_rate": 0.00019977509622105233,
      "loss": 1.7599,
      "step": 65
    },
    {
      "epoch": 0.1172291296625222,
      "grad_norm": 0.5720934271812439,
      "learning_rate": 0.0001997675398852022,
      "loss": 1.6824,
      "step": 66
    },
    {
      "epoch": 0.11900532859680284,
      "grad_norm": 0.5780119299888611,
      "learning_rate": 0.00019975985884651803,
      "loss": 2.0813,
      "step": 67
    },
    {
      "epoch": 0.12078152753108348,
      "grad_norm": 0.5710111260414124,
      "learning_rate": 0.00019975205311460053,
      "loss": 2.0135,
      "step": 68
    },
    {
      "epoch": 0.12255772646536411,
      "grad_norm": 0.637725293636322,
      "learning_rate": 0.0001997441226992064,
      "loss": 2.2077,
      "step": 69
    },
    {
      "epoch": 0.12433392539964476,
      "grad_norm": 0.595778226852417,
      "learning_rate": 0.00019973606761024813,
      "loss": 2.0404,
      "step": 70
    },
    {
      "epoch": 0.1261101243339254,
      "grad_norm": 0.5477076172828674,
      "learning_rate": 0.00019972788785779403,
      "loss": 1.6866,
      "step": 71
    },
    {
      "epoch": 0.12788632326820604,
      "grad_norm": 0.5296264290809631,
      "learning_rate": 0.00019971958345206826,
      "loss": 1.9961,
      "step": 72
    },
    {
      "epoch": 0.12966252220248667,
      "grad_norm": 0.568881630897522,
      "learning_rate": 0.0001997111544034508,
      "loss": 2.1162,
      "step": 73
    },
    {
      "epoch": 0.13143872113676733,
      "grad_norm": 0.5359049439430237,
      "learning_rate": 0.0001997026007224774,
      "loss": 1.8294,
      "step": 74
    },
    {
      "epoch": 0.13321492007104796,
      "grad_norm": 0.5227463841438293,
      "learning_rate": 0.00019969392241983957,
      "loss": 2.3483,
      "step": 75
    },
    {
      "epoch": 0.1349911190053286,
      "grad_norm": 0.5109757781028748,
      "learning_rate": 0.00019968511950638464,
      "loss": 1.9574,
      "step": 76
    },
    {
      "epoch": 0.13676731793960922,
      "grad_norm": 0.5607638955116272,
      "learning_rate": 0.00019967619199311564,
      "loss": 2.0784,
      "step": 77
    },
    {
      "epoch": 0.13854351687388988,
      "grad_norm": 0.5734109282493591,
      "learning_rate": 0.00019966713989119143,
      "loss": 2.2864,
      "step": 78
    },
    {
      "epoch": 0.14031971580817051,
      "grad_norm": 0.4957180917263031,
      "learning_rate": 0.00019965796321192648,
      "loss": 1.987,
      "step": 79
    },
    {
      "epoch": 0.14209591474245115,
      "grad_norm": 0.5269530415534973,
      "learning_rate": 0.00019964866196679103,
      "loss": 2.0645,
      "step": 80
    },
    {
      "epoch": 0.1438721136767318,
      "grad_norm": 0.5919597148895264,
      "learning_rate": 0.00019963923616741106,
      "loss": 2.0381,
      "step": 81
    },
    {
      "epoch": 0.14564831261101244,
      "grad_norm": 0.5078924894332886,
      "learning_rate": 0.0001996296858255682,
      "loss": 1.9305,
      "step": 82
    },
    {
      "epoch": 0.14742451154529307,
      "grad_norm": 0.5710830688476562,
      "learning_rate": 0.0001996200109531997,
      "loss": 2.1333,
      "step": 83
    },
    {
      "epoch": 0.1492007104795737,
      "grad_norm": 0.5336270332336426,
      "learning_rate": 0.00019961021156239856,
      "loss": 1.8455,
      "step": 84
    },
    {
      "epoch": 0.15097690941385436,
      "grad_norm": 0.512915313243866,
      "learning_rate": 0.00019960028766541335,
      "loss": 1.7508,
      "step": 85
    },
    {
      "epoch": 0.152753108348135,
      "grad_norm": 0.5059733986854553,
      "learning_rate": 0.00019959023927464828,
      "loss": 2.2871,
      "step": 86
    },
    {
      "epoch": 0.15452930728241562,
      "grad_norm": 0.5591479539871216,
      "learning_rate": 0.0001995800664026632,
      "loss": 2.142,
      "step": 87
    },
    {
      "epoch": 0.15630550621669628,
      "grad_norm": 0.5256248712539673,
      "learning_rate": 0.00019956976906217348,
      "loss": 2.0703,
      "step": 88
    },
    {
      "epoch": 0.15808170515097691,
      "grad_norm": 0.47411078214645386,
      "learning_rate": 0.00019955934726605018,
      "loss": 1.7754,
      "step": 89
    },
    {
      "epoch": 0.15985790408525755,
      "grad_norm": 0.5065747499465942,
      "learning_rate": 0.0001995488010273198,
      "loss": 1.9478,
      "step": 90
    },
    {
      "epoch": 0.16163410301953818,
      "grad_norm": 0.4927048683166504,
      "learning_rate": 0.00019953813035916443,
      "loss": 1.8597,
      "step": 91
    },
    {
      "epoch": 0.16341030195381884,
      "grad_norm": 0.6021456122398376,
      "learning_rate": 0.00019952733527492177,
      "loss": 2.0057,
      "step": 92
    },
    {
      "epoch": 0.16518650088809947,
      "grad_norm": 0.4884112775325775,
      "learning_rate": 0.00019951641578808493,
      "loss": 1.6935,
      "step": 93
    },
    {
      "epoch": 0.1669626998223801,
      "grad_norm": 0.5052005052566528,
      "learning_rate": 0.00019950537191230252,
      "loss": 1.7997,
      "step": 94
    },
    {
      "epoch": 0.16873889875666073,
      "grad_norm": 0.5119022130966187,
      "learning_rate": 0.0001994942036613787,
      "loss": 2.1542,
      "step": 95
    },
    {
      "epoch": 0.1705150976909414,
      "grad_norm": 0.5095562934875488,
      "learning_rate": 0.000199482911049273,
      "loss": 2.0142,
      "step": 96
    },
    {
      "epoch": 0.17229129662522202,
      "grad_norm": 0.4861847758293152,
      "learning_rate": 0.0001994714940901005,
      "loss": 1.9334,
      "step": 97
    },
    {
      "epoch": 0.17406749555950266,
      "grad_norm": 0.49960047006607056,
      "learning_rate": 0.00019945995279813157,
      "loss": 2.1062,
      "step": 98
    },
    {
      "epoch": 0.17584369449378331,
      "grad_norm": 0.45143675804138184,
      "learning_rate": 0.00019944828718779212,
      "loss": 1.805,
      "step": 99
    },
    {
      "epoch": 0.17761989342806395,
      "grad_norm": 0.3813634514808655,
      "learning_rate": 0.00019943649727366335,
      "loss": 1.3542,
      "step": 100
    },
    {
      "epoch": 0.17761989342806395,
      "eval_loss": 1.9064041376113892,
      "eval_runtime": 17.5399,
      "eval_samples_per_second": 57.07,
      "eval_steps_per_second": 28.563,
      "step": 100
    },
    {
      "epoch": 0.17939609236234458,
      "grad_norm": 0.5255076289176941,
      "learning_rate": 0.00019942458307048192,
      "loss": 2.0936,
      "step": 101
    },
    {
      "epoch": 0.1811722912966252,
      "grad_norm": 0.5400596261024475,
      "learning_rate": 0.00019941254459313978,
      "loss": 1.661,
      "step": 102
    },
    {
      "epoch": 0.18294849023090587,
      "grad_norm": 0.5050174593925476,
      "learning_rate": 0.0001994003818566842,
      "loss": 2.3869,
      "step": 103
    },
    {
      "epoch": 0.1847246891651865,
      "grad_norm": 0.6338909268379211,
      "learning_rate": 0.00019938809487631784,
      "loss": 1.7198,
      "step": 104
    },
    {
      "epoch": 0.18650088809946713,
      "grad_norm": 0.4585017263889313,
      "learning_rate": 0.00019937568366739858,
      "loss": 2.3039,
      "step": 105
    },
    {
      "epoch": 0.1882770870337478,
      "grad_norm": 0.49629560112953186,
      "learning_rate": 0.00019936314824543964,
      "loss": 2.2219,
      "step": 106
    },
    {
      "epoch": 0.19005328596802842,
      "grad_norm": 0.5742344260215759,
      "learning_rate": 0.00019935048862610946,
      "loss": 1.8119,
      "step": 107
    },
    {
      "epoch": 0.19182948490230906,
      "grad_norm": 0.4849453866481781,
      "learning_rate": 0.0001993377048252317,
      "loss": 1.8812,
      "step": 108
    },
    {
      "epoch": 0.1936056838365897,
      "grad_norm": 0.48134133219718933,
      "learning_rate": 0.0001993247968587853,
      "loss": 1.9638,
      "step": 109
    },
    {
      "epoch": 0.19538188277087035,
      "grad_norm": 0.49113133549690247,
      "learning_rate": 0.00019931176474290437,
      "loss": 1.9503,
      "step": 110
    },
    {
      "epoch": 0.19715808170515098,
      "grad_norm": 0.48999834060668945,
      "learning_rate": 0.00019929860849387816,
      "loss": 1.6382,
      "step": 111
    },
    {
      "epoch": 0.1989342806394316,
      "grad_norm": 0.4928407669067383,
      "learning_rate": 0.00019928532812815114,
      "loss": 1.7679,
      "step": 112
    },
    {
      "epoch": 0.20071047957371227,
      "grad_norm": 0.4963860809803009,
      "learning_rate": 0.00019927192366232287,
      "loss": 2.1362,
      "step": 113
    },
    {
      "epoch": 0.2024866785079929,
      "grad_norm": 0.4692034125328064,
      "learning_rate": 0.0001992583951131481,
      "loss": 2.0111,
      "step": 114
    },
    {
      "epoch": 0.20426287744227353,
      "grad_norm": 0.4754531681537628,
      "learning_rate": 0.00019924474249753655,
      "loss": 2.3111,
      "step": 115
    },
    {
      "epoch": 0.20603907637655416,
      "grad_norm": 0.5069610476493835,
      "learning_rate": 0.00019923096583255312,
      "loss": 2.0997,
      "step": 116
    },
    {
      "epoch": 0.20781527531083482,
      "grad_norm": 0.4914354085922241,
      "learning_rate": 0.00019921706513541772,
      "loss": 1.8297,
      "step": 117
    },
    {
      "epoch": 0.20959147424511546,
      "grad_norm": 0.5038297772407532,
      "learning_rate": 0.00019920304042350532,
      "loss": 2.05,
      "step": 118
    },
    {
      "epoch": 0.2113676731793961,
      "grad_norm": 0.4465586543083191,
      "learning_rate": 0.00019918889171434588,
      "loss": 2.0244,
      "step": 119
    },
    {
      "epoch": 0.21314387211367672,
      "grad_norm": 0.4640624225139618,
      "learning_rate": 0.00019917461902562434,
      "loss": 1.9746,
      "step": 120
    },
    {
      "epoch": 0.21492007104795738,
      "grad_norm": 0.47969332337379456,
      "learning_rate": 0.00019916022237518064,
      "loss": 2.1167,
      "step": 121
    },
    {
      "epoch": 0.216696269982238,
      "grad_norm": 0.46891728043556213,
      "learning_rate": 0.00019914570178100963,
      "loss": 1.9804,
      "step": 122
    },
    {
      "epoch": 0.21847246891651864,
      "grad_norm": 0.4815308451652527,
      "learning_rate": 0.00019913105726126107,
      "loss": 1.905,
      "step": 123
    },
    {
      "epoch": 0.2202486678507993,
      "grad_norm": 0.45982739329338074,
      "learning_rate": 0.00019911628883423967,
      "loss": 1.9874,
      "step": 124
    },
    {
      "epoch": 0.22202486678507993,
      "grad_norm": 0.49811920523643494,
      "learning_rate": 0.00019910139651840497,
      "loss": 2.2195,
      "step": 125
    },
    {
      "epoch": 0.22380106571936056,
      "grad_norm": 0.6553320288658142,
      "learning_rate": 0.00019908638033237143,
      "loss": 2.1251,
      "step": 126
    },
    {
      "epoch": 0.2255772646536412,
      "grad_norm": 0.4624321460723877,
      "learning_rate": 0.00019907124029490826,
      "loss": 2.1357,
      "step": 127
    },
    {
      "epoch": 0.22735346358792186,
      "grad_norm": 0.48921123147010803,
      "learning_rate": 0.00019905597642493948,
      "loss": 2.0813,
      "step": 128
    },
    {
      "epoch": 0.2291296625222025,
      "grad_norm": 0.47739872336387634,
      "learning_rate": 0.0001990405887415439,
      "loss": 1.519,
      "step": 129
    },
    {
      "epoch": 0.23090586145648312,
      "grad_norm": 0.49068933725357056,
      "learning_rate": 0.00019902507726395522,
      "loss": 1.9346,
      "step": 130
    },
    {
      "epoch": 0.23268206039076378,
      "grad_norm": 0.53465336561203,
      "learning_rate": 0.00019900944201156162,
      "loss": 1.8731,
      "step": 131
    },
    {
      "epoch": 0.2344582593250444,
      "grad_norm": 0.4828990697860718,
      "learning_rate": 0.00019899368300390624,
      "loss": 1.7585,
      "step": 132
    },
    {
      "epoch": 0.23623445825932504,
      "grad_norm": 0.4632841646671295,
      "learning_rate": 0.00019897780026068674,
      "loss": 1.8068,
      "step": 133
    },
    {
      "epoch": 0.23801065719360567,
      "grad_norm": 0.4546014070510864,
      "learning_rate": 0.00019896179380175555,
      "loss": 1.5966,
      "step": 134
    },
    {
      "epoch": 0.23978685612788633,
      "grad_norm": 0.5171592831611633,
      "learning_rate": 0.00019894566364711964,
      "loss": 2.2132,
      "step": 135
    },
    {
      "epoch": 0.24156305506216696,
      "grad_norm": 0.4793371558189392,
      "learning_rate": 0.00019892940981694067,
      "loss": 1.665,
      "step": 136
    },
    {
      "epoch": 0.2433392539964476,
      "grad_norm": 0.4771358370780945,
      "learning_rate": 0.0001989130323315348,
      "loss": 1.8537,
      "step": 137
    },
    {
      "epoch": 0.24511545293072823,
      "grad_norm": 0.44712236523628235,
      "learning_rate": 0.00019889653121137285,
      "loss": 1.9126,
      "step": 138
    },
    {
      "epoch": 0.2468916518650089,
      "grad_norm": 0.5153189301490784,
      "learning_rate": 0.00019887990647708014,
      "loss": 2.0212,
      "step": 139
    },
    {
      "epoch": 0.24866785079928952,
      "grad_norm": 0.4318281412124634,
      "learning_rate": 0.00019886315814943647,
      "loss": 2.1411,
      "step": 140
    },
    {
      "epoch": 0.25044404973357015,
      "grad_norm": 0.48623180389404297,
      "learning_rate": 0.00019884628624937614,
      "loss": 2.2084,
      "step": 141
    },
    {
      "epoch": 0.2522202486678508,
      "grad_norm": 0.5409252047538757,
      "learning_rate": 0.00019882929079798793,
      "loss": 1.8314,
      "step": 142
    },
    {
      "epoch": 0.2539964476021314,
      "grad_norm": 0.4503491520881653,
      "learning_rate": 0.00019881217181651502,
      "loss": 1.4617,
      "step": 143
    },
    {
      "epoch": 0.2557726465364121,
      "grad_norm": 0.5354145765304565,
      "learning_rate": 0.000198794929326355,
      "loss": 1.7445,
      "step": 144
    },
    {
      "epoch": 0.25754884547069273,
      "grad_norm": 0.4973542094230652,
      "learning_rate": 0.00019877756334905984,
      "loss": 1.6662,
      "step": 145
    },
    {
      "epoch": 0.25932504440497334,
      "grad_norm": 0.4266776740550995,
      "learning_rate": 0.00019876007390633586,
      "loss": 2.0064,
      "step": 146
    },
    {
      "epoch": 0.261101243339254,
      "grad_norm": 0.4539612829685211,
      "learning_rate": 0.00019874246102004374,
      "loss": 1.8313,
      "step": 147
    },
    {
      "epoch": 0.26287744227353466,
      "grad_norm": 0.4709239602088928,
      "learning_rate": 0.0001987247247121984,
      "loss": 2.2397,
      "step": 148
    },
    {
      "epoch": 0.26465364120781526,
      "grad_norm": 0.45346367359161377,
      "learning_rate": 0.00019870686500496904,
      "loss": 1.8433,
      "step": 149
    },
    {
      "epoch": 0.2664298401420959,
      "grad_norm": 0.4608617126941681,
      "learning_rate": 0.00019868888192067915,
      "loss": 2.3524,
      "step": 150
    },
    {
      "epoch": 0.2682060390763766,
      "grad_norm": 0.47535520792007446,
      "learning_rate": 0.0001986707754818064,
      "loss": 1.8428,
      "step": 151
    },
    {
      "epoch": 0.2699822380106572,
      "grad_norm": 0.4438779354095459,
      "learning_rate": 0.0001986525457109826,
      "loss": 2.3737,
      "step": 152
    },
    {
      "epoch": 0.27175843694493784,
      "grad_norm": 0.46376553177833557,
      "learning_rate": 0.00019863419263099375,
      "loss": 2.0702,
      "step": 153
    },
    {
      "epoch": 0.27353463587921845,
      "grad_norm": 0.5438401699066162,
      "learning_rate": 0.00019861571626478007,
      "loss": 1.9328,
      "step": 154
    },
    {
      "epoch": 0.2753108348134991,
      "grad_norm": 0.46315065026283264,
      "learning_rate": 0.00019859711663543572,
      "loss": 1.664,
      "step": 155
    },
    {
      "epoch": 0.27708703374777977,
      "grad_norm": 0.45238879323005676,
      "learning_rate": 0.00019857839376620905,
      "loss": 2.2071,
      "step": 156
    },
    {
      "epoch": 0.27886323268206037,
      "grad_norm": 0.4509004056453705,
      "learning_rate": 0.00019855954768050236,
      "loss": 2.0947,
      "step": 157
    },
    {
      "epoch": 0.28063943161634103,
      "grad_norm": 0.4626876413822174,
      "learning_rate": 0.00019854057840187203,
      "loss": 2.1444,
      "step": 158
    },
    {
      "epoch": 0.2824156305506217,
      "grad_norm": 0.4454987347126007,
      "learning_rate": 0.0001985214859540285,
      "loss": 1.8363,
      "step": 159
    },
    {
      "epoch": 0.2841918294849023,
      "grad_norm": 0.44985294342041016,
      "learning_rate": 0.0001985022703608359,
      "loss": 1.743,
      "step": 160
    },
    {
      "epoch": 0.28596802841918295,
      "grad_norm": 0.4251282215118408,
      "learning_rate": 0.0001984829316463126,
      "loss": 1.9378,
      "step": 161
    },
    {
      "epoch": 0.2877442273534636,
      "grad_norm": 0.4673730731010437,
      "learning_rate": 0.00019846346983463063,
      "loss": 1.6219,
      "step": 162
    },
    {
      "epoch": 0.2895204262877442,
      "grad_norm": 0.4760516881942749,
      "learning_rate": 0.000198443884950116,
      "loss": 1.7899,
      "step": 163
    },
    {
      "epoch": 0.2912966252220249,
      "grad_norm": 0.4833865463733673,
      "learning_rate": 0.00019842417701724854,
      "loss": 1.7896,
      "step": 164
    },
    {
      "epoch": 0.29307282415630553,
      "grad_norm": 0.4449723958969116,
      "learning_rate": 0.0001984043460606618,
      "loss": 2.0747,
      "step": 165
    },
    {
      "epoch": 0.29484902309058614,
      "grad_norm": 0.4446113407611847,
      "learning_rate": 0.00019838439210514324,
      "loss": 1.7053,
      "step": 166
    },
    {
      "epoch": 0.2966252220248668,
      "grad_norm": 0.4472447633743286,
      "learning_rate": 0.00019836431517563394,
      "loss": 1.4583,
      "step": 167
    },
    {
      "epoch": 0.2984014209591474,
      "grad_norm": 0.47006919980049133,
      "learning_rate": 0.00019834411529722875,
      "loss": 1.725,
      "step": 168
    },
    {
      "epoch": 0.30017761989342806,
      "grad_norm": 0.42637670040130615,
      "learning_rate": 0.00019832379249517618,
      "loss": 1.9257,
      "step": 169
    },
    {
      "epoch": 0.3019538188277087,
      "grad_norm": 0.4558171331882477,
      "learning_rate": 0.00019830334679487842,
      "loss": 2.2405,
      "step": 170
    },
    {
      "epoch": 0.3037300177619893,
      "grad_norm": 0.4493999481201172,
      "learning_rate": 0.00019828277822189118,
      "loss": 2.007,
      "step": 171
    },
    {
      "epoch": 0.30550621669627,
      "grad_norm": 0.44785118103027344,
      "learning_rate": 0.00019826208680192385,
      "loss": 2.1813,
      "step": 172
    },
    {
      "epoch": 0.30728241563055064,
      "grad_norm": 0.4593670666217804,
      "learning_rate": 0.00019824127256083936,
      "loss": 2.021,
      "step": 173
    },
    {
      "epoch": 0.30905861456483125,
      "grad_norm": 0.4476518929004669,
      "learning_rate": 0.00019822033552465413,
      "loss": 2.2503,
      "step": 174
    },
    {
      "epoch": 0.3108348134991119,
      "grad_norm": 0.43868955969810486,
      "learning_rate": 0.00019819927571953805,
      "loss": 2.3044,
      "step": 175
    },
    {
      "epoch": 0.31261101243339257,
      "grad_norm": 0.45108944177627563,
      "learning_rate": 0.0001981780931718145,
      "loss": 2.0398,
      "step": 176
    },
    {
      "epoch": 0.31438721136767317,
      "grad_norm": 0.4161747395992279,
      "learning_rate": 0.00019815678790796026,
      "loss": 1.8855,
      "step": 177
    },
    {
      "epoch": 0.31616341030195383,
      "grad_norm": 0.45716363191604614,
      "learning_rate": 0.0001981353599546055,
      "loss": 2.1075,
      "step": 178
    },
    {
      "epoch": 0.31793960923623443,
      "grad_norm": 0.42757993936538696,
      "learning_rate": 0.00019811380933853375,
      "loss": 2.184,
      "step": 179
    },
    {
      "epoch": 0.3197158081705151,
      "grad_norm": 0.43287554383277893,
      "learning_rate": 0.00019809213608668188,
      "loss": 2.0325,
      "step": 180
    },
    {
      "epoch": 0.32149200710479575,
      "grad_norm": 0.47792336344718933,
      "learning_rate": 0.00019807034022613994,
      "loss": 1.9784,
      "step": 181
    },
    {
      "epoch": 0.32326820603907636,
      "grad_norm": 0.4553424119949341,
      "learning_rate": 0.00019804842178415143,
      "loss": 1.8735,
      "step": 182
    },
    {
      "epoch": 0.325044404973357,
      "grad_norm": 0.41315704584121704,
      "learning_rate": 0.00019802638078811287,
      "loss": 2.0891,
      "step": 183
    },
    {
      "epoch": 0.3268206039076377,
      "grad_norm": 0.4501994252204895,
      "learning_rate": 0.0001980042172655741,
      "loss": 1.996,
      "step": 184
    },
    {
      "epoch": 0.3285968028419183,
      "grad_norm": 0.46339136362075806,
      "learning_rate": 0.00019798193124423804,
      "loss": 2.2729,
      "step": 185
    },
    {
      "epoch": 0.33037300177619894,
      "grad_norm": 0.4335190951824188,
      "learning_rate": 0.00019795952275196072,
      "loss": 2.2189,
      "step": 186
    },
    {
      "epoch": 0.3321492007104796,
      "grad_norm": 0.45126596093177795,
      "learning_rate": 0.00019793699181675132,
      "loss": 2.0129,
      "step": 187
    },
    {
      "epoch": 0.3339253996447602,
      "grad_norm": 0.4451168477535248,
      "learning_rate": 0.000197914338466772,
      "loss": 1.8096,
      "step": 188
    },
    {
      "epoch": 0.33570159857904086,
      "grad_norm": 0.45490482449531555,
      "learning_rate": 0.00019789156273033792,
      "loss": 1.6799,
      "step": 189
    },
    {
      "epoch": 0.33747779751332146,
      "grad_norm": 0.4627436101436615,
      "learning_rate": 0.0001978686646359173,
      "loss": 1.8884,
      "step": 190
    },
    {
      "epoch": 0.3392539964476021,
      "grad_norm": 0.4451692998409271,
      "learning_rate": 0.00019784564421213122,
      "loss": 2.018,
      "step": 191
    },
    {
      "epoch": 0.3410301953818828,
      "grad_norm": 0.4236283302307129,
      "learning_rate": 0.00019782250148775367,
      "loss": 2.3517,
      "step": 192
    },
    {
      "epoch": 0.3428063943161634,
      "grad_norm": 0.45046183466911316,
      "learning_rate": 0.00019779923649171154,
      "loss": 1.7388,
      "step": 193
    },
    {
      "epoch": 0.34458259325044405,
      "grad_norm": 0.47201672196388245,
      "learning_rate": 0.00019777584925308457,
      "loss": 2.1332,
      "step": 194
    },
    {
      "epoch": 0.3463587921847247,
      "grad_norm": 0.4369264543056488,
      "learning_rate": 0.00019775233980110524,
      "loss": 2.0444,
      "step": 195
    },
    {
      "epoch": 0.3481349911190053,
      "grad_norm": 0.5011683106422424,
      "learning_rate": 0.00019772870816515877,
      "loss": 1.7519,
      "step": 196
    },
    {
      "epoch": 0.34991119005328597,
      "grad_norm": 0.46893975138664246,
      "learning_rate": 0.0001977049543747832,
      "loss": 1.8242,
      "step": 197
    },
    {
      "epoch": 0.35168738898756663,
      "grad_norm": 0.43081167340278625,
      "learning_rate": 0.00019768107845966922,
      "loss": 1.672,
      "step": 198
    },
    {
      "epoch": 0.35346358792184723,
      "grad_norm": 0.42945030331611633,
      "learning_rate": 0.00019765708044966007,
      "loss": 1.7243,
      "step": 199
    },
    {
      "epoch": 0.3552397868561279,
      "grad_norm": 0.4440798759460449,
      "learning_rate": 0.00019763296037475172,
      "loss": 1.9277,
      "step": 200
    },
    {
      "epoch": 0.3552397868561279,
      "eval_loss": 1.8932586908340454,
      "eval_runtime": 17.5442,
      "eval_samples_per_second": 57.056,
      "eval_steps_per_second": 28.556,
      "step": 200
    },
    {
      "epoch": 0.35701598579040855,
      "grad_norm": 0.42811062932014465,
      "learning_rate": 0.00019760871826509266,
      "loss": 2.0196,
      "step": 201
    },
    {
      "epoch": 0.35879218472468916,
      "grad_norm": 0.41129323840141296,
      "learning_rate": 0.00019758435415098395,
      "loss": 2.0394,
      "step": 202
    },
    {
      "epoch": 0.3605683836589698,
      "grad_norm": 0.4296959936618805,
      "learning_rate": 0.0001975598680628791,
      "loss": 2.1815,
      "step": 203
    },
    {
      "epoch": 0.3623445825932504,
      "grad_norm": 0.43623343110084534,
      "learning_rate": 0.0001975352600313841,
      "loss": 1.8972,
      "step": 204
    },
    {
      "epoch": 0.3641207815275311,
      "grad_norm": 0.410102903842926,
      "learning_rate": 0.00019751053008725737,
      "loss": 1.9283,
      "step": 205
    },
    {
      "epoch": 0.36589698046181174,
      "grad_norm": 0.4520421624183655,
      "learning_rate": 0.00019748567826140972,
      "loss": 2.0136,
      "step": 206
    },
    {
      "epoch": 0.36767317939609234,
      "grad_norm": 0.4736691117286682,
      "learning_rate": 0.00019746070458490428,
      "loss": 2.4327,
      "step": 207
    },
    {
      "epoch": 0.369449378330373,
      "grad_norm": 0.47168004512786865,
      "learning_rate": 0.00019743560908895647,
      "loss": 1.8474,
      "step": 208
    },
    {
      "epoch": 0.37122557726465366,
      "grad_norm": 0.45908498764038086,
      "learning_rate": 0.00019741039180493403,
      "loss": 1.8709,
      "step": 209
    },
    {
      "epoch": 0.37300177619893427,
      "grad_norm": 0.4423717260360718,
      "learning_rate": 0.0001973850527643569,
      "loss": 1.9015,
      "step": 210
    },
    {
      "epoch": 0.3747779751332149,
      "grad_norm": 0.4620676040649414,
      "learning_rate": 0.00019735959199889723,
      "loss": 1.6488,
      "step": 211
    },
    {
      "epoch": 0.3765541740674956,
      "grad_norm": 0.5108397006988525,
      "learning_rate": 0.00019733400954037922,
      "loss": 2.0828,
      "step": 212
    },
    {
      "epoch": 0.3783303730017762,
      "grad_norm": 0.4272177815437317,
      "learning_rate": 0.00019730830542077934,
      "loss": 1.8063,
      "step": 213
    },
    {
      "epoch": 0.38010657193605685,
      "grad_norm": 0.44216951727867126,
      "learning_rate": 0.00019728247967222594,
      "loss": 2.1608,
      "step": 214
    },
    {
      "epoch": 0.38188277087033745,
      "grad_norm": 0.426279753446579,
      "learning_rate": 0.0001972565323269996,
      "loss": 2.1013,
      "step": 215
    },
    {
      "epoch": 0.3836589698046181,
      "grad_norm": 0.4721355438232422,
      "learning_rate": 0.00019723046341753276,
      "loss": 1.7217,
      "step": 216
    },
    {
      "epoch": 0.38543516873889877,
      "grad_norm": 0.4848463833332062,
      "learning_rate": 0.00019720427297640982,
      "loss": 1.7889,
      "step": 217
    },
    {
      "epoch": 0.3872113676731794,
      "grad_norm": 0.43875014781951904,
      "learning_rate": 0.0001971779610363671,
      "loss": 2.3218,
      "step": 218
    },
    {
      "epoch": 0.38898756660746003,
      "grad_norm": 0.4160006642341614,
      "learning_rate": 0.0001971515276302928,
      "loss": 1.7976,
      "step": 219
    },
    {
      "epoch": 0.3907637655417407,
      "grad_norm": 0.4541258215904236,
      "learning_rate": 0.0001971249727912269,
      "loss": 1.9825,
      "step": 220
    },
    {
      "epoch": 0.3925399644760213,
      "grad_norm": 0.4543601870536804,
      "learning_rate": 0.00019709829655236122,
      "loss": 1.9166,
      "step": 221
    },
    {
      "epoch": 0.39431616341030196,
      "grad_norm": 0.4247014820575714,
      "learning_rate": 0.00019707149894703928,
      "loss": 2.1685,
      "step": 222
    },
    {
      "epoch": 0.3960923623445826,
      "grad_norm": 0.5207393169403076,
      "learning_rate": 0.00019704458000875636,
      "loss": 2.0669,
      "step": 223
    },
    {
      "epoch": 0.3978685612788632,
      "grad_norm": 0.43390846252441406,
      "learning_rate": 0.00019701753977115934,
      "loss": 2.1988,
      "step": 224
    },
    {
      "epoch": 0.3996447602131439,
      "grad_norm": 0.46728917956352234,
      "learning_rate": 0.00019699037826804668,
      "loss": 2.0953,
      "step": 225
    },
    {
      "epoch": 0.40142095914742454,
      "grad_norm": 0.4798896312713623,
      "learning_rate": 0.0001969630955333685,
      "loss": 2.1458,
      "step": 226
    },
    {
      "epoch": 0.40319715808170514,
      "grad_norm": 0.45930740237236023,
      "learning_rate": 0.00019693569160122643,
      "loss": 1.4763,
      "step": 227
    },
    {
      "epoch": 0.4049733570159858,
      "grad_norm": 0.43325430154800415,
      "learning_rate": 0.00019690816650587355,
      "loss": 1.9333,
      "step": 228
    },
    {
      "epoch": 0.4067495559502664,
      "grad_norm": 0.4275812804698944,
      "learning_rate": 0.0001968805202817144,
      "loss": 2.1515,
      "step": 229
    },
    {
      "epoch": 0.40852575488454707,
      "grad_norm": 0.40936627984046936,
      "learning_rate": 0.00019685275296330498,
      "loss": 2.1736,
      "step": 230
    },
    {
      "epoch": 0.4103019538188277,
      "grad_norm": 0.4141199290752411,
      "learning_rate": 0.0001968248645853526,
      "loss": 1.7297,
      "step": 231
    },
    {
      "epoch": 0.41207815275310833,
      "grad_norm": 0.46847742795944214,
      "learning_rate": 0.00019679685518271582,
      "loss": 2.249,
      "step": 232
    },
    {
      "epoch": 0.413854351687389,
      "grad_norm": 0.4531584680080414,
      "learning_rate": 0.0001967687247904046,
      "loss": 1.7629,
      "step": 233
    },
    {
      "epoch": 0.41563055062166965,
      "grad_norm": 0.4545172452926636,
      "learning_rate": 0.0001967404734435801,
      "loss": 2.0266,
      "step": 234
    },
    {
      "epoch": 0.41740674955595025,
      "grad_norm": 0.4343206286430359,
      "learning_rate": 0.00019671210117755457,
      "loss": 2.0015,
      "step": 235
    },
    {
      "epoch": 0.4191829484902309,
      "grad_norm": 0.4606926739215851,
      "learning_rate": 0.00019668360802779154,
      "loss": 1.8731,
      "step": 236
    },
    {
      "epoch": 0.42095914742451157,
      "grad_norm": 0.41662344336509705,
      "learning_rate": 0.00019665499402990547,
      "loss": 1.592,
      "step": 237
    },
    {
      "epoch": 0.4227353463587922,
      "grad_norm": 0.4032103717327118,
      "learning_rate": 0.00019662625921966205,
      "loss": 2.2559,
      "step": 238
    },
    {
      "epoch": 0.42451154529307283,
      "grad_norm": 0.446520060300827,
      "learning_rate": 0.00019659740363297786,
      "loss": 2.1321,
      "step": 239
    },
    {
      "epoch": 0.42628774422735344,
      "grad_norm": 0.44457969069480896,
      "learning_rate": 0.00019656842730592046,
      "loss": 1.8641,
      "step": 240
    },
    {
      "epoch": 0.4280639431616341,
      "grad_norm": 0.48300305008888245,
      "learning_rate": 0.0001965393302747084,
      "loss": 1.4631,
      "step": 241
    },
    {
      "epoch": 0.42984014209591476,
      "grad_norm": 0.44548162817955017,
      "learning_rate": 0.000196510112575711,
      "loss": 1.7498,
      "step": 242
    },
    {
      "epoch": 0.43161634103019536,
      "grad_norm": 0.44994574785232544,
      "learning_rate": 0.00019648077424544845,
      "loss": 1.9095,
      "step": 243
    },
    {
      "epoch": 0.433392539964476,
      "grad_norm": 0.4470200836658478,
      "learning_rate": 0.0001964513153205918,
      "loss": 2.0112,
      "step": 244
    },
    {
      "epoch": 0.4351687388987567,
      "grad_norm": 0.4565306603908539,
      "learning_rate": 0.00019642173583796267,
      "loss": 1.6789,
      "step": 245
    },
    {
      "epoch": 0.4369449378330373,
      "grad_norm": 0.43571972846984863,
      "learning_rate": 0.00019639203583453346,
      "loss": 2.1676,
      "step": 246
    },
    {
      "epoch": 0.43872113676731794,
      "grad_norm": 0.443683922290802,
      "learning_rate": 0.00019636221534742728,
      "loss": 2.0096,
      "step": 247
    },
    {
      "epoch": 0.4404973357015986,
      "grad_norm": 0.4746660888195038,
      "learning_rate": 0.00019633227441391774,
      "loss": 2.027,
      "step": 248
    },
    {
      "epoch": 0.4422735346358792,
      "grad_norm": 0.4404323399066925,
      "learning_rate": 0.00019630221307142897,
      "loss": 1.7873,
      "step": 249
    },
    {
      "epoch": 0.44404973357015987,
      "grad_norm": 0.44215402007102966,
      "learning_rate": 0.00019627203135753578,
      "loss": 1.9319,
      "step": 250
    },
    {
      "epoch": 0.44582593250444047,
      "grad_norm": 0.44816359877586365,
      "learning_rate": 0.00019624172930996322,
      "loss": 1.9671,
      "step": 251
    },
    {
      "epoch": 0.44760213143872113,
      "grad_norm": 0.46310874819755554,
      "learning_rate": 0.00019621130696658687,
      "loss": 2.1617,
      "step": 252
    },
    {
      "epoch": 0.4493783303730018,
      "grad_norm": 0.7113448977470398,
      "learning_rate": 0.00019618076436543267,
      "loss": 1.9432,
      "step": 253
    },
    {
      "epoch": 0.4511545293072824,
      "grad_norm": 0.45633432269096375,
      "learning_rate": 0.00019615010154467685,
      "loss": 2.2835,
      "step": 254
    },
    {
      "epoch": 0.45293072824156305,
      "grad_norm": 0.4767390191555023,
      "learning_rate": 0.0001961193185426459,
      "loss": 2.0263,
      "step": 255
    },
    {
      "epoch": 0.4547069271758437,
      "grad_norm": 0.4538816213607788,
      "learning_rate": 0.00019608841539781654,
      "loss": 1.7641,
      "step": 256
    },
    {
      "epoch": 0.4564831261101243,
      "grad_norm": 0.41003236174583435,
      "learning_rate": 0.00019605739214881567,
      "loss": 2.4097,
      "step": 257
    },
    {
      "epoch": 0.458259325044405,
      "grad_norm": 0.5005156397819519,
      "learning_rate": 0.00019602624883442027,
      "loss": 2.2536,
      "step": 258
    },
    {
      "epoch": 0.46003552397868563,
      "grad_norm": 0.40640825033187866,
      "learning_rate": 0.00019599498549355748,
      "loss": 2.456,
      "step": 259
    },
    {
      "epoch": 0.46181172291296624,
      "grad_norm": 0.4441612958908081,
      "learning_rate": 0.00019596360216530436,
      "loss": 1.927,
      "step": 260
    },
    {
      "epoch": 0.4635879218472469,
      "grad_norm": 0.4262355864048004,
      "learning_rate": 0.00019593209888888803,
      "loss": 2.0603,
      "step": 261
    },
    {
      "epoch": 0.46536412078152756,
      "grad_norm": 0.4317218065261841,
      "learning_rate": 0.00019590047570368547,
      "loss": 2.0728,
      "step": 262
    },
    {
      "epoch": 0.46714031971580816,
      "grad_norm": 0.4529935419559479,
      "learning_rate": 0.0001958687326492236,
      "loss": 2.196,
      "step": 263
    },
    {
      "epoch": 0.4689165186500888,
      "grad_norm": 0.4509224593639374,
      "learning_rate": 0.00019583686976517916,
      "loss": 2.164,
      "step": 264
    },
    {
      "epoch": 0.4706927175843694,
      "grad_norm": 0.4421185255050659,
      "learning_rate": 0.0001958048870913786,
      "loss": 2.1611,
      "step": 265
    },
    {
      "epoch": 0.4724689165186501,
      "grad_norm": 0.46441522240638733,
      "learning_rate": 0.0001957727846677982,
      "loss": 1.978,
      "step": 266
    },
    {
      "epoch": 0.47424511545293074,
      "grad_norm": 0.43932053446769714,
      "learning_rate": 0.00019574056253456382,
      "loss": 2.106,
      "step": 267
    },
    {
      "epoch": 0.47602131438721135,
      "grad_norm": 0.4515652060508728,
      "learning_rate": 0.00019570822073195103,
      "loss": 1.778,
      "step": 268
    },
    {
      "epoch": 0.477797513321492,
      "grad_norm": 0.442630410194397,
      "learning_rate": 0.00019567575930038494,
      "loss": 1.765,
      "step": 269
    },
    {
      "epoch": 0.47957371225577267,
      "grad_norm": 0.47424089908599854,
      "learning_rate": 0.0001956431782804402,
      "loss": 1.7732,
      "step": 270
    },
    {
      "epoch": 0.48134991119005327,
      "grad_norm": 0.4427890181541443,
      "learning_rate": 0.00019561047771284092,
      "loss": 1.6502,
      "step": 271
    },
    {
      "epoch": 0.48312611012433393,
      "grad_norm": 0.454005628824234,
      "learning_rate": 0.00019557765763846064,
      "loss": 2.1827,
      "step": 272
    },
    {
      "epoch": 0.4849023090586146,
      "grad_norm": 0.44144609570503235,
      "learning_rate": 0.0001955447180983223,
      "loss": 1.9762,
      "step": 273
    },
    {
      "epoch": 0.4866785079928952,
      "grad_norm": 0.4218321144580841,
      "learning_rate": 0.0001955116591335982,
      "loss": 1.68,
      "step": 274
    },
    {
      "epoch": 0.48845470692717585,
      "grad_norm": 0.47931987047195435,
      "learning_rate": 0.00019547848078560974,
      "loss": 2.1055,
      "step": 275
    },
    {
      "epoch": 0.49023090586145646,
      "grad_norm": 0.45459476113319397,
      "learning_rate": 0.0001954451830958278,
      "loss": 2.2627,
      "step": 276
    },
    {
      "epoch": 0.4920071047957371,
      "grad_norm": 0.42920202016830444,
      "learning_rate": 0.00019541176610587217,
      "loss": 2.1092,
      "step": 277
    },
    {
      "epoch": 0.4937833037300178,
      "grad_norm": 0.4616375267505646,
      "learning_rate": 0.00019537822985751197,
      "loss": 1.9747,
      "step": 278
    },
    {
      "epoch": 0.4955595026642984,
      "grad_norm": 0.4954018294811249,
      "learning_rate": 0.00019534457439266526,
      "loss": 1.937,
      "step": 279
    },
    {
      "epoch": 0.49733570159857904,
      "grad_norm": 0.4882751405239105,
      "learning_rate": 0.0001953107997533991,
      "loss": 2.0562,
      "step": 280
    },
    {
      "epoch": 0.4991119005328597,
      "grad_norm": 0.4628260135650635,
      "learning_rate": 0.00019527690598192967,
      "loss": 2.2231,
      "step": 281
    },
    {
      "epoch": 0.5008880994671403,
      "grad_norm": 0.4622148275375366,
      "learning_rate": 0.00019524289312062188,
      "loss": 2.0816,
      "step": 282
    },
    {
      "epoch": 0.5026642984014209,
      "grad_norm": 0.44598594307899475,
      "learning_rate": 0.00019520876121198954,
      "loss": 2.0479,
      "step": 283
    },
    {
      "epoch": 0.5044404973357016,
      "grad_norm": 0.42964744567871094,
      "learning_rate": 0.0001951745102986953,
      "loss": 2.0073,
      "step": 284
    },
    {
      "epoch": 0.5062166962699822,
      "grad_norm": 0.42197179794311523,
      "learning_rate": 0.0001951401404235506,
      "loss": 1.8409,
      "step": 285
    },
    {
      "epoch": 0.5079928952042628,
      "grad_norm": 0.439445823431015,
      "learning_rate": 0.00019510565162951537,
      "loss": 2.0988,
      "step": 286
    },
    {
      "epoch": 0.5097690941385435,
      "grad_norm": 0.47230061888694763,
      "learning_rate": 0.00019507104395969843,
      "loss": 2.1105,
      "step": 287
    },
    {
      "epoch": 0.5115452930728241,
      "grad_norm": 0.464979887008667,
      "learning_rate": 0.00019503631745735708,
      "loss": 1.8271,
      "step": 288
    },
    {
      "epoch": 0.5133214920071048,
      "grad_norm": 0.4115940034389496,
      "learning_rate": 0.00019500147216589713,
      "loss": 1.9002,
      "step": 289
    },
    {
      "epoch": 0.5150976909413855,
      "grad_norm": 0.4541196823120117,
      "learning_rate": 0.0001949665081288729,
      "loss": 1.8344,
      "step": 290
    },
    {
      "epoch": 0.5168738898756661,
      "grad_norm": 0.4500337541103363,
      "learning_rate": 0.00019493142538998712,
      "loss": 1.7801,
      "step": 291
    },
    {
      "epoch": 0.5186500888099467,
      "grad_norm": 0.4730067849159241,
      "learning_rate": 0.0001948962239930909,
      "loss": 2.1059,
      "step": 292
    },
    {
      "epoch": 0.5204262877442274,
      "grad_norm": 0.4461643397808075,
      "learning_rate": 0.0001948609039821837,
      "loss": 2.0346,
      "step": 293
    },
    {
      "epoch": 0.522202486678508,
      "grad_norm": 0.37577617168426514,
      "learning_rate": 0.00019482546540141308,
      "loss": 1.8607,
      "step": 294
    },
    {
      "epoch": 0.5239786856127886,
      "grad_norm": 0.4153205156326294,
      "learning_rate": 0.00019478990829507507,
      "loss": 1.767,
      "step": 295
    },
    {
      "epoch": 0.5257548845470693,
      "grad_norm": 0.47432464361190796,
      "learning_rate": 0.00019475423270761362,
      "loss": 1.6188,
      "step": 296
    },
    {
      "epoch": 0.5275310834813499,
      "grad_norm": 0.43071404099464417,
      "learning_rate": 0.00019471843868362084,
      "loss": 1.5767,
      "step": 297
    },
    {
      "epoch": 0.5293072824156305,
      "grad_norm": 0.46165087819099426,
      "learning_rate": 0.00019468252626783698,
      "loss": 1.6544,
      "step": 298
    },
    {
      "epoch": 0.5310834813499112,
      "grad_norm": 0.4708615243434906,
      "learning_rate": 0.00019464649550515013,
      "loss": 2.0475,
      "step": 299
    },
    {
      "epoch": 0.5328596802841918,
      "grad_norm": 0.44285327196121216,
      "learning_rate": 0.00019461034644059638,
      "loss": 1.9856,
      "step": 300
    },
    {
      "epoch": 0.5328596802841918,
      "eval_loss": 1.8899484872817993,
      "eval_runtime": 17.5289,
      "eval_samples_per_second": 57.106,
      "eval_steps_per_second": 28.581,
      "step": 300
    },
    {
      "epoch": 0.5346358792184724,
      "grad_norm": 0.4438420236110687,
      "learning_rate": 0.00019457407911935965,
      "loss": 2.0487,
      "step": 301
    },
    {
      "epoch": 0.5364120781527532,
      "grad_norm": 0.4485795199871063,
      "learning_rate": 0.00019453769358677173,
      "loss": 2.0526,
      "step": 302
    },
    {
      "epoch": 0.5381882770870338,
      "grad_norm": 0.3932655453681946,
      "learning_rate": 0.0001945011898883121,
      "loss": 2.2285,
      "step": 303
    },
    {
      "epoch": 0.5399644760213144,
      "grad_norm": 0.439187616109848,
      "learning_rate": 0.00019446456806960797,
      "loss": 2.0303,
      "step": 304
    },
    {
      "epoch": 0.5417406749555951,
      "grad_norm": 0.4505543112754822,
      "learning_rate": 0.00019442782817643422,
      "loss": 2.0164,
      "step": 305
    },
    {
      "epoch": 0.5435168738898757,
      "grad_norm": 0.4547162652015686,
      "learning_rate": 0.00019439097025471328,
      "loss": 2.2864,
      "step": 306
    },
    {
      "epoch": 0.5452930728241563,
      "grad_norm": 0.49941936135292053,
      "learning_rate": 0.00019435399435051512,
      "loss": 1.6221,
      "step": 307
    },
    {
      "epoch": 0.5470692717584369,
      "grad_norm": 0.4606425166130066,
      "learning_rate": 0.0001943169005100572,
      "loss": 2.0223,
      "step": 308
    },
    {
      "epoch": 0.5488454706927176,
      "grad_norm": 0.4287405014038086,
      "learning_rate": 0.00019427968877970428,
      "loss": 2.1114,
      "step": 309
    },
    {
      "epoch": 0.5506216696269982,
      "grad_norm": 0.45592790842056274,
      "learning_rate": 0.00019424235920596864,
      "loss": 1.8594,
      "step": 310
    },
    {
      "epoch": 0.5523978685612788,
      "grad_norm": 0.46438437700271606,
      "learning_rate": 0.0001942049118355098,
      "loss": 2.0507,
      "step": 311
    },
    {
      "epoch": 0.5541740674955595,
      "grad_norm": 0.4096914529800415,
      "learning_rate": 0.00019416734671513447,
      "loss": 2.3792,
      "step": 312
    },
    {
      "epoch": 0.5559502664298401,
      "grad_norm": 0.44788944721221924,
      "learning_rate": 0.00019412966389179657,
      "loss": 2.0335,
      "step": 313
    },
    {
      "epoch": 0.5577264653641207,
      "grad_norm": 0.4274342954158783,
      "learning_rate": 0.0001940918634125971,
      "loss": 2.0476,
      "step": 314
    },
    {
      "epoch": 0.5595026642984015,
      "grad_norm": 0.4460148215293884,
      "learning_rate": 0.00019405394532478424,
      "loss": 2.0332,
      "step": 315
    },
    {
      "epoch": 0.5612788632326821,
      "grad_norm": 0.44506481289863586,
      "learning_rate": 0.00019401590967575304,
      "loss": 1.856,
      "step": 316
    },
    {
      "epoch": 0.5630550621669627,
      "grad_norm": 0.476184606552124,
      "learning_rate": 0.00019397775651304553,
      "loss": 1.923,
      "step": 317
    },
    {
      "epoch": 0.5648312611012434,
      "grad_norm": 0.5178910493850708,
      "learning_rate": 0.00019393948588435073,
      "loss": 1.9678,
      "step": 318
    },
    {
      "epoch": 0.566607460035524,
      "grad_norm": 0.44286128878593445,
      "learning_rate": 0.0001939010978375043,
      "loss": 1.9217,
      "step": 319
    },
    {
      "epoch": 0.5683836589698046,
      "grad_norm": 0.4596065580844879,
      "learning_rate": 0.0001938625924204888,
      "loss": 2.1248,
      "step": 320
    },
    {
      "epoch": 0.5701598579040853,
      "grad_norm": 0.46016642451286316,
      "learning_rate": 0.00019382396968143351,
      "loss": 2.3317,
      "step": 321
    },
    {
      "epoch": 0.5719360568383659,
      "grad_norm": 0.44263720512390137,
      "learning_rate": 0.0001937852296686142,
      "loss": 2.13,
      "step": 322
    },
    {
      "epoch": 0.5737122557726465,
      "grad_norm": 0.4219183325767517,
      "learning_rate": 0.00019374637243045343,
      "loss": 1.9255,
      "step": 323
    },
    {
      "epoch": 0.5754884547069272,
      "grad_norm": 0.42386704683303833,
      "learning_rate": 0.00019370739801552009,
      "loss": 1.657,
      "step": 324
    },
    {
      "epoch": 0.5772646536412078,
      "grad_norm": 0.4189471900463104,
      "learning_rate": 0.0001936683064725297,
      "loss": 2.1171,
      "step": 325
    },
    {
      "epoch": 0.5790408525754884,
      "grad_norm": 0.4284886419773102,
      "learning_rate": 0.00019362909785034408,
      "loss": 2.0737,
      "step": 326
    },
    {
      "epoch": 0.5808170515097691,
      "grad_norm": 0.43988317251205444,
      "learning_rate": 0.0001935897721979714,
      "loss": 2.4376,
      "step": 327
    },
    {
      "epoch": 0.5825932504440497,
      "grad_norm": 0.44104182720184326,
      "learning_rate": 0.0001935503295645661,
      "loss": 1.8949,
      "step": 328
    },
    {
      "epoch": 0.5843694493783304,
      "grad_norm": 0.477687269449234,
      "learning_rate": 0.00019351076999942897,
      "loss": 2.0444,
      "step": 329
    },
    {
      "epoch": 0.5861456483126111,
      "grad_norm": 0.4487113058567047,
      "learning_rate": 0.0001934710935520067,
      "loss": 1.8628,
      "step": 330
    },
    {
      "epoch": 0.5879218472468917,
      "grad_norm": 0.4188252091407776,
      "learning_rate": 0.0001934313002718924,
      "loss": 1.724,
      "step": 331
    },
    {
      "epoch": 0.5896980461811723,
      "grad_norm": 0.44325894117355347,
      "learning_rate": 0.00019339139020882492,
      "loss": 2.1486,
      "step": 332
    },
    {
      "epoch": 0.5914742451154529,
      "grad_norm": 0.4393855333328247,
      "learning_rate": 0.00019335136341268924,
      "loss": 1.691,
      "step": 333
    },
    {
      "epoch": 0.5932504440497336,
      "grad_norm": 0.44441497325897217,
      "learning_rate": 0.00019331121993351622,
      "loss": 1.7502,
      "step": 334
    },
    {
      "epoch": 0.5950266429840142,
      "grad_norm": 0.41306668519973755,
      "learning_rate": 0.00019327095982148255,
      "loss": 1.902,
      "step": 335
    },
    {
      "epoch": 0.5968028419182948,
      "grad_norm": 0.4508137106895447,
      "learning_rate": 0.00019323058312691068,
      "loss": 2.0255,
      "step": 336
    },
    {
      "epoch": 0.5985790408525755,
      "grad_norm": 0.45917072892189026,
      "learning_rate": 0.00019319008990026888,
      "loss": 2.0826,
      "step": 337
    },
    {
      "epoch": 0.6003552397868561,
      "grad_norm": 0.4464453160762787,
      "learning_rate": 0.00019314948019217093,
      "loss": 1.861,
      "step": 338
    },
    {
      "epoch": 0.6021314387211367,
      "grad_norm": 0.4337208867073059,
      "learning_rate": 0.00019310875405337635,
      "loss": 2.0769,
      "step": 339
    },
    {
      "epoch": 0.6039076376554174,
      "grad_norm": 0.45887869596481323,
      "learning_rate": 0.00019306791153479006,
      "loss": 1.7081,
      "step": 340
    },
    {
      "epoch": 0.605683836589698,
      "grad_norm": 0.49675875902175903,
      "learning_rate": 0.0001930269526874626,
      "loss": 2.0492,
      "step": 341
    },
    {
      "epoch": 0.6074600355239786,
      "grad_norm": 0.46276500821113586,
      "learning_rate": 0.0001929858775625897,
      "loss": 2.0729,
      "step": 342
    },
    {
      "epoch": 0.6092362344582594,
      "grad_norm": 0.41176077723503113,
      "learning_rate": 0.00019294468621151265,
      "loss": 2.1395,
      "step": 343
    },
    {
      "epoch": 0.61101243339254,
      "grad_norm": 0.38920730352401733,
      "learning_rate": 0.00019290337868571787,
      "loss": 2.0022,
      "step": 344
    },
    {
      "epoch": 0.6127886323268206,
      "grad_norm": 0.432917982339859,
      "learning_rate": 0.00019286195503683708,
      "loss": 1.916,
      "step": 345
    },
    {
      "epoch": 0.6145648312611013,
      "grad_norm": 0.43707025051116943,
      "learning_rate": 0.00019282041531664705,
      "loss": 2.0847,
      "step": 346
    },
    {
      "epoch": 0.6163410301953819,
      "grad_norm": 0.5096771717071533,
      "learning_rate": 0.00019277875957706973,
      "loss": 1.8654,
      "step": 347
    },
    {
      "epoch": 0.6181172291296625,
      "grad_norm": 0.4254882037639618,
      "learning_rate": 0.00019273698787017202,
      "loss": 2.2487,
      "step": 348
    },
    {
      "epoch": 0.6198934280639432,
      "grad_norm": 0.4226941466331482,
      "learning_rate": 0.0001926951002481658,
      "loss": 1.9468,
      "step": 349
    },
    {
      "epoch": 0.6216696269982238,
      "grad_norm": 0.426439493894577,
      "learning_rate": 0.00019265309676340785,
      "loss": 2.074,
      "step": 350
    },
    {
      "epoch": 0.6234458259325044,
      "grad_norm": 0.4904014766216278,
      "learning_rate": 0.00019261097746839975,
      "loss": 2.0751,
      "step": 351
    },
    {
      "epoch": 0.6252220248667851,
      "grad_norm": 0.5646877884864807,
      "learning_rate": 0.00019256874241578779,
      "loss": 1.88,
      "step": 352
    },
    {
      "epoch": 0.6269982238010657,
      "grad_norm": 0.44490063190460205,
      "learning_rate": 0.00019252639165836301,
      "loss": 2.113,
      "step": 353
    },
    {
      "epoch": 0.6287744227353463,
      "grad_norm": 0.41131526231765747,
      "learning_rate": 0.0001924839252490611,
      "loss": 2.0679,
      "step": 354
    },
    {
      "epoch": 0.6305506216696269,
      "grad_norm": 0.44147130846977234,
      "learning_rate": 0.00019244134324096225,
      "loss": 1.8631,
      "step": 355
    },
    {
      "epoch": 0.6323268206039077,
      "grad_norm": 0.4396229684352875,
      "learning_rate": 0.0001923986456872911,
      "loss": 2.4356,
      "step": 356
    },
    {
      "epoch": 0.6341030195381883,
      "grad_norm": 0.4997573792934418,
      "learning_rate": 0.00019235583264141684,
      "loss": 2.1304,
      "step": 357
    },
    {
      "epoch": 0.6358792184724689,
      "grad_norm": 0.4418199360370636,
      "learning_rate": 0.00019231290415685294,
      "loss": 2.0235,
      "step": 358
    },
    {
      "epoch": 0.6376554174067496,
      "grad_norm": 0.4212493896484375,
      "learning_rate": 0.0001922698602872572,
      "loss": 1.8047,
      "step": 359
    },
    {
      "epoch": 0.6394316163410302,
      "grad_norm": 0.41120877861976624,
      "learning_rate": 0.00019222670108643152,
      "loss": 1.8025,
      "step": 360
    },
    {
      "epoch": 0.6412078152753108,
      "grad_norm": 0.4344291090965271,
      "learning_rate": 0.00019218342660832209,
      "loss": 1.6285,
      "step": 361
    },
    {
      "epoch": 0.6429840142095915,
      "grad_norm": 0.42076975107192993,
      "learning_rate": 0.00019214003690701918,
      "loss": 1.8317,
      "step": 362
    },
    {
      "epoch": 0.6447602131438721,
      "grad_norm": 0.40705862641334534,
      "learning_rate": 0.00019209653203675704,
      "loss": 2.0669,
      "step": 363
    },
    {
      "epoch": 0.6465364120781527,
      "grad_norm": 0.5094960331916809,
      "learning_rate": 0.00019205291205191385,
      "loss": 2.2456,
      "step": 364
    },
    {
      "epoch": 0.6483126110124334,
      "grad_norm": 0.4229685664176941,
      "learning_rate": 0.00019200917700701176,
      "loss": 1.8792,
      "step": 365
    },
    {
      "epoch": 0.650088809946714,
      "grad_norm": 0.4246689975261688,
      "learning_rate": 0.00019196532695671663,
      "loss": 2.2165,
      "step": 366
    },
    {
      "epoch": 0.6518650088809946,
      "grad_norm": 0.4235001802444458,
      "learning_rate": 0.0001919213619558382,
      "loss": 1.6442,
      "step": 367
    },
    {
      "epoch": 0.6536412078152753,
      "grad_norm": 0.43805190920829773,
      "learning_rate": 0.00019187728205932974,
      "loss": 1.7868,
      "step": 368
    },
    {
      "epoch": 0.655417406749556,
      "grad_norm": 0.4366198778152466,
      "learning_rate": 0.00019183308732228827,
      "loss": 2.0413,
      "step": 369
    },
    {
      "epoch": 0.6571936056838366,
      "grad_norm": 0.43701887130737305,
      "learning_rate": 0.00019178877779995422,
      "loss": 2.1184,
      "step": 370
    },
    {
      "epoch": 0.6589698046181173,
      "grad_norm": 0.5027895569801331,
      "learning_rate": 0.00019174435354771165,
      "loss": 2.0704,
      "step": 371
    },
    {
      "epoch": 0.6607460035523979,
      "grad_norm": 0.4361107051372528,
      "learning_rate": 0.00019169981462108785,
      "loss": 2.2557,
      "step": 372
    },
    {
      "epoch": 0.6625222024866785,
      "grad_norm": 0.4398843050003052,
      "learning_rate": 0.00019165516107575364,
      "loss": 1.8636,
      "step": 373
    },
    {
      "epoch": 0.6642984014209592,
      "grad_norm": 0.4424595236778259,
      "learning_rate": 0.00019161039296752293,
      "loss": 1.9772,
      "step": 374
    },
    {
      "epoch": 0.6660746003552398,
      "grad_norm": 0.4213164150714874,
      "learning_rate": 0.0001915655103523529,
      "loss": 1.7669,
      "step": 375
    },
    {
      "epoch": 0.6678507992895204,
      "grad_norm": 0.4099929928779602,
      "learning_rate": 0.0001915205132863439,
      "loss": 2.2212,
      "step": 376
    },
    {
      "epoch": 0.6696269982238011,
      "grad_norm": 0.42710360884666443,
      "learning_rate": 0.00019147540182573925,
      "loss": 1.6766,
      "step": 377
    },
    {
      "epoch": 0.6714031971580817,
      "grad_norm": 0.42685043811798096,
      "learning_rate": 0.0001914301760269253,
      "loss": 2.3406,
      "step": 378
    },
    {
      "epoch": 0.6731793960923623,
      "grad_norm": 0.44154638051986694,
      "learning_rate": 0.00019138483594643134,
      "loss": 1.8951,
      "step": 379
    },
    {
      "epoch": 0.6749555950266429,
      "grad_norm": 0.4106541872024536,
      "learning_rate": 0.00019133938164092942,
      "loss": 2.271,
      "step": 380
    },
    {
      "epoch": 0.6767317939609236,
      "grad_norm": 0.5013306140899658,
      "learning_rate": 0.00019129381316723443,
      "loss": 1.6584,
      "step": 381
    },
    {
      "epoch": 0.6785079928952042,
      "grad_norm": 0.48590245842933655,
      "learning_rate": 0.00019124813058230399,
      "loss": 1.8066,
      "step": 382
    },
    {
      "epoch": 0.6802841918294849,
      "grad_norm": 0.40615737438201904,
      "learning_rate": 0.0001912023339432383,
      "loss": 1.9287,
      "step": 383
    },
    {
      "epoch": 0.6820603907637656,
      "grad_norm": 0.48425769805908203,
      "learning_rate": 0.00019115642330728018,
      "loss": 2.1331,
      "step": 384
    },
    {
      "epoch": 0.6838365896980462,
      "grad_norm": 0.4437471330165863,
      "learning_rate": 0.00019111039873181477,
      "loss": 1.9904,
      "step": 385
    },
    {
      "epoch": 0.6856127886323268,
      "grad_norm": 0.42253896594047546,
      "learning_rate": 0.00019106426027436986,
      "loss": 1.5577,
      "step": 386
    },
    {
      "epoch": 0.6873889875666075,
      "grad_norm": 0.4388871192932129,
      "learning_rate": 0.00019101800799261543,
      "loss": 2.1807,
      "step": 387
    },
    {
      "epoch": 0.6891651865008881,
      "grad_norm": 0.450830340385437,
      "learning_rate": 0.00019097164194436375,
      "loss": 1.9255,
      "step": 388
    },
    {
      "epoch": 0.6909413854351687,
      "grad_norm": 0.43665239214897156,
      "learning_rate": 0.00019092516218756938,
      "loss": 2.2903,
      "step": 389
    },
    {
      "epoch": 0.6927175843694494,
      "grad_norm": 0.4547036290168762,
      "learning_rate": 0.0001908785687803289,
      "loss": 2.1227,
      "step": 390
    },
    {
      "epoch": 0.69449378330373,
      "grad_norm": 0.4178352952003479,
      "learning_rate": 0.000190831861780881,
      "loss": 2.0461,
      "step": 391
    },
    {
      "epoch": 0.6962699822380106,
      "grad_norm": 0.4663878083229065,
      "learning_rate": 0.0001907850412476064,
      "loss": 2.1472,
      "step": 392
    },
    {
      "epoch": 0.6980461811722913,
      "grad_norm": 0.45797213912010193,
      "learning_rate": 0.00019073810723902756,
      "loss": 1.8605,
      "step": 393
    },
    {
      "epoch": 0.6998223801065719,
      "grad_norm": 0.39486759901046753,
      "learning_rate": 0.00019069105981380896,
      "loss": 1.9724,
      "step": 394
    },
    {
      "epoch": 0.7015985790408525,
      "grad_norm": 0.4473598301410675,
      "learning_rate": 0.00019064389903075677,
      "loss": 1.8968,
      "step": 395
    },
    {
      "epoch": 0.7033747779751333,
      "grad_norm": 0.47510868310928345,
      "learning_rate": 0.00019059662494881887,
      "loss": 2.028,
      "step": 396
    },
    {
      "epoch": 0.7051509769094139,
      "grad_norm": 0.4042087495326996,
      "learning_rate": 0.0001905492376270847,
      "loss": 2.1556,
      "step": 397
    },
    {
      "epoch": 0.7069271758436945,
      "grad_norm": 0.4212908446788788,
      "learning_rate": 0.0001905017371247853,
      "loss": 2.1102,
      "step": 398
    },
    {
      "epoch": 0.7087033747779752,
      "grad_norm": 0.40984147787094116,
      "learning_rate": 0.00019045412350129315,
      "loss": 1.8234,
      "step": 399
    },
    {
      "epoch": 0.7104795737122558,
      "grad_norm": 0.40091556310653687,
      "learning_rate": 0.00019040639681612218,
      "loss": 2.1848,
      "step": 400
    },
    {
      "epoch": 0.7104795737122558,
      "eval_loss": 1.883875846862793,
      "eval_runtime": 17.5317,
      "eval_samples_per_second": 57.097,
      "eval_steps_per_second": 28.577,
      "step": 400
    },
    {
      "epoch": 0.7122557726465364,
      "grad_norm": 0.43238532543182373,
      "learning_rate": 0.00019035855712892753,
      "loss": 1.6121,
      "step": 401
    },
    {
      "epoch": 0.7140319715808171,
      "grad_norm": 0.47652530670166016,
      "learning_rate": 0.0001903106044995057,
      "loss": 1.9938,
      "step": 402
    },
    {
      "epoch": 0.7158081705150977,
      "grad_norm": 0.43661484122276306,
      "learning_rate": 0.00019026253898779424,
      "loss": 1.6268,
      "step": 403
    },
    {
      "epoch": 0.7175843694493783,
      "grad_norm": 0.46455177664756775,
      "learning_rate": 0.00019021436065387194,
      "loss": 1.8151,
      "step": 404
    },
    {
      "epoch": 0.7193605683836589,
      "grad_norm": 0.4447750449180603,
      "learning_rate": 0.00019016606955795848,
      "loss": 2.0248,
      "step": 405
    },
    {
      "epoch": 0.7211367673179396,
      "grad_norm": 0.42699840664863586,
      "learning_rate": 0.00019011766576041458,
      "loss": 1.749,
      "step": 406
    },
    {
      "epoch": 0.7229129662522202,
      "grad_norm": 0.46390849351882935,
      "learning_rate": 0.0001900691493217418,
      "loss": 1.8877,
      "step": 407
    },
    {
      "epoch": 0.7246891651865008,
      "grad_norm": 0.41476351022720337,
      "learning_rate": 0.00019002052030258243,
      "loss": 1.8647,
      "step": 408
    },
    {
      "epoch": 0.7264653641207816,
      "grad_norm": 0.5148999691009521,
      "learning_rate": 0.00018997177876371957,
      "loss": 2.0018,
      "step": 409
    },
    {
      "epoch": 0.7282415630550622,
      "grad_norm": 0.4009697437286377,
      "learning_rate": 0.0001899229247660769,
      "loss": 1.8305,
      "step": 410
    },
    {
      "epoch": 0.7300177619893428,
      "grad_norm": 0.3996514678001404,
      "learning_rate": 0.0001898739583707187,
      "loss": 1.9116,
      "step": 411
    },
    {
      "epoch": 0.7317939609236235,
      "grad_norm": 0.4347704350948334,
      "learning_rate": 0.00018982487963884975,
      "loss": 2.3117,
      "step": 412
    },
    {
      "epoch": 0.7335701598579041,
      "grad_norm": 0.44870176911354065,
      "learning_rate": 0.00018977568863181516,
      "loss": 1.8429,
      "step": 413
    },
    {
      "epoch": 0.7353463587921847,
      "grad_norm": 0.43834176659584045,
      "learning_rate": 0.00018972638541110053,
      "loss": 2.288,
      "step": 414
    },
    {
      "epoch": 0.7371225577264654,
      "grad_norm": 0.3987554609775543,
      "learning_rate": 0.00018967697003833157,
      "loss": 2.2679,
      "step": 415
    },
    {
      "epoch": 0.738898756660746,
      "grad_norm": 0.4387021064758301,
      "learning_rate": 0.00018962744257527422,
      "loss": 1.576,
      "step": 416
    },
    {
      "epoch": 0.7406749555950266,
      "grad_norm": 0.4461187422275543,
      "learning_rate": 0.00018957780308383456,
      "loss": 2.3788,
      "step": 417
    },
    {
      "epoch": 0.7424511545293073,
      "grad_norm": 0.4991357922554016,
      "learning_rate": 0.00018952805162605869,
      "loss": 2.1715,
      "step": 418
    },
    {
      "epoch": 0.7442273534635879,
      "grad_norm": 0.4659833312034607,
      "learning_rate": 0.00018947818826413264,
      "loss": 1.9464,
      "step": 419
    },
    {
      "epoch": 0.7460035523978685,
      "grad_norm": 0.41284704208374023,
      "learning_rate": 0.0001894282130603823,
      "loss": 2.1542,
      "step": 420
    },
    {
      "epoch": 0.7477797513321492,
      "grad_norm": 0.5547340512275696,
      "learning_rate": 0.00018937812607727338,
      "loss": 2.1198,
      "step": 421
    },
    {
      "epoch": 0.7495559502664298,
      "grad_norm": 0.4388543963432312,
      "learning_rate": 0.00018932792737741129,
      "loss": 1.9379,
      "step": 422
    },
    {
      "epoch": 0.7513321492007105,
      "grad_norm": 0.41265663504600525,
      "learning_rate": 0.00018927761702354113,
      "loss": 1.8169,
      "step": 423
    },
    {
      "epoch": 0.7531083481349912,
      "grad_norm": 0.4760828912258148,
      "learning_rate": 0.00018922719507854748,
      "loss": 2.2821,
      "step": 424
    },
    {
      "epoch": 0.7548845470692718,
      "grad_norm": 0.4584570825099945,
      "learning_rate": 0.00018917666160545444,
      "loss": 2.162,
      "step": 425
    },
    {
      "epoch": 0.7566607460035524,
      "grad_norm": 0.44125086069107056,
      "learning_rate": 0.00018912601666742551,
      "loss": 1.7598,
      "step": 426
    },
    {
      "epoch": 0.7584369449378331,
      "grad_norm": 0.4422093331813812,
      "learning_rate": 0.00018907526032776355,
      "loss": 1.6479,
      "step": 427
    },
    {
      "epoch": 0.7602131438721137,
      "grad_norm": 0.43991178274154663,
      "learning_rate": 0.00018902439264991062,
      "loss": 1.6872,
      "step": 428
    },
    {
      "epoch": 0.7619893428063943,
      "grad_norm": 0.48972269892692566,
      "learning_rate": 0.00018897341369744793,
      "loss": 1.878,
      "step": 429
    },
    {
      "epoch": 0.7637655417406749,
      "grad_norm": 0.48215749859809875,
      "learning_rate": 0.0001889223235340958,
      "loss": 1.9473,
      "step": 430
    },
    {
      "epoch": 0.7655417406749556,
      "grad_norm": 0.43077534437179565,
      "learning_rate": 0.00018887112222371361,
      "loss": 1.9293,
      "step": 431
    },
    {
      "epoch": 0.7673179396092362,
      "grad_norm": 0.42315348982810974,
      "learning_rate": 0.0001888198098302996,
      "loss": 1.998,
      "step": 432
    },
    {
      "epoch": 0.7690941385435168,
      "grad_norm": 0.4607672095298767,
      "learning_rate": 0.0001887683864179908,
      "loss": 1.8415,
      "step": 433
    },
    {
      "epoch": 0.7708703374777975,
      "grad_norm": 0.45919305086135864,
      "learning_rate": 0.0001887168520510632,
      "loss": 2.0726,
      "step": 434
    },
    {
      "epoch": 0.7726465364120781,
      "grad_norm": 0.4592558443546295,
      "learning_rate": 0.00018866520679393126,
      "loss": 2.1927,
      "step": 435
    },
    {
      "epoch": 0.7744227353463587,
      "grad_norm": 0.46584615111351013,
      "learning_rate": 0.0001886134507111482,
      "loss": 1.862,
      "step": 436
    },
    {
      "epoch": 0.7761989342806395,
      "grad_norm": 0.4328685998916626,
      "learning_rate": 0.00018856158386740564,
      "loss": 1.8802,
      "step": 437
    },
    {
      "epoch": 0.7779751332149201,
      "grad_norm": 0.40540120005607605,
      "learning_rate": 0.00018850960632753373,
      "loss": 1.8671,
      "step": 438
    },
    {
      "epoch": 0.7797513321492007,
      "grad_norm": 0.4082028567790985,
      "learning_rate": 0.00018845751815650102,
      "loss": 2.0093,
      "step": 439
    },
    {
      "epoch": 0.7815275310834814,
      "grad_norm": 0.410640150308609,
      "learning_rate": 0.0001884053194194142,
      "loss": 1.9845,
      "step": 440
    },
    {
      "epoch": 0.783303730017762,
      "grad_norm": 0.45287570357322693,
      "learning_rate": 0.0001883530101815183,
      "loss": 2.0556,
      "step": 441
    },
    {
      "epoch": 0.7850799289520426,
      "grad_norm": 0.4389626681804657,
      "learning_rate": 0.0001883005905081964,
      "loss": 1.862,
      "step": 442
    },
    {
      "epoch": 0.7868561278863233,
      "grad_norm": 0.4057653844356537,
      "learning_rate": 0.00018824806046496958,
      "loss": 2.137,
      "step": 443
    },
    {
      "epoch": 0.7886323268206039,
      "grad_norm": 0.4277278184890747,
      "learning_rate": 0.000188195420117497,
      "loss": 1.9918,
      "step": 444
    },
    {
      "epoch": 0.7904085257548845,
      "grad_norm": 0.43505293130874634,
      "learning_rate": 0.00018814266953157556,
      "loss": 2.1589,
      "step": 445
    },
    {
      "epoch": 0.7921847246891652,
      "grad_norm": 0.3927178680896759,
      "learning_rate": 0.00018808980877314002,
      "loss": 2.0687,
      "step": 446
    },
    {
      "epoch": 0.7939609236234458,
      "grad_norm": 0.4424169659614563,
      "learning_rate": 0.0001880368379082629,
      "loss": 1.9891,
      "step": 447
    },
    {
      "epoch": 0.7957371225577264,
      "grad_norm": 0.444903165102005,
      "learning_rate": 0.00018798375700315416,
      "loss": 1.9673,
      "step": 448
    },
    {
      "epoch": 0.7975133214920072,
      "grad_norm": 0.5016148090362549,
      "learning_rate": 0.00018793056612416155,
      "loss": 2.0874,
      "step": 449
    },
    {
      "epoch": 0.7992895204262878,
      "grad_norm": 0.5738353729248047,
      "learning_rate": 0.00018787726533777004,
      "loss": 1.806,
      "step": 450
    },
    {
      "epoch": 0.8010657193605684,
      "grad_norm": 0.4226686656475067,
      "learning_rate": 0.00018782385471060215,
      "loss": 1.8987,
      "step": 451
    },
    {
      "epoch": 0.8028419182948491,
      "grad_norm": 0.4466644525527954,
      "learning_rate": 0.00018777033430941766,
      "loss": 2.1248,
      "step": 452
    },
    {
      "epoch": 0.8046181172291297,
      "grad_norm": 0.47510892152786255,
      "learning_rate": 0.0001877167042011135,
      "loss": 1.8949,
      "step": 453
    },
    {
      "epoch": 0.8063943161634103,
      "grad_norm": 0.41336846351623535,
      "learning_rate": 0.00018766296445272377,
      "loss": 2.0414,
      "step": 454
    },
    {
      "epoch": 0.8081705150976909,
      "grad_norm": 0.47358089685440063,
      "learning_rate": 0.0001876091151314196,
      "loss": 1.9172,
      "step": 455
    },
    {
      "epoch": 0.8099467140319716,
      "grad_norm": 0.4210824966430664,
      "learning_rate": 0.00018755515630450913,
      "loss": 1.874,
      "step": 456
    },
    {
      "epoch": 0.8117229129662522,
      "grad_norm": 0.5020896792411804,
      "learning_rate": 0.00018750108803943728,
      "loss": 1.7698,
      "step": 457
    },
    {
      "epoch": 0.8134991119005328,
      "grad_norm": 0.44762885570526123,
      "learning_rate": 0.0001874469104037858,
      "loss": 1.6549,
      "step": 458
    },
    {
      "epoch": 0.8152753108348135,
      "grad_norm": 0.43813836574554443,
      "learning_rate": 0.00018739262346527315,
      "loss": 1.6735,
      "step": 459
    },
    {
      "epoch": 0.8170515097690941,
      "grad_norm": 0.4420367479324341,
      "learning_rate": 0.0001873382272917545,
      "loss": 2.1253,
      "step": 460
    },
    {
      "epoch": 0.8188277087033747,
      "grad_norm": 0.43580523133277893,
      "learning_rate": 0.00018728372195122138,
      "loss": 1.9402,
      "step": 461
    },
    {
      "epoch": 0.8206039076376554,
      "grad_norm": 0.42156457901000977,
      "learning_rate": 0.0001872291075118019,
      "loss": 2.1517,
      "step": 462
    },
    {
      "epoch": 0.822380106571936,
      "grad_norm": 0.5092214345932007,
      "learning_rate": 0.0001871743840417605,
      "loss": 1.7623,
      "step": 463
    },
    {
      "epoch": 0.8241563055062167,
      "grad_norm": 0.5822166800498962,
      "learning_rate": 0.0001871195516094979,
      "loss": 2.0609,
      "step": 464
    },
    {
      "epoch": 0.8259325044404974,
      "grad_norm": 0.4523063898086548,
      "learning_rate": 0.00018706461028355104,
      "loss": 1.9752,
      "step": 465
    },
    {
      "epoch": 0.827708703374778,
      "grad_norm": 0.5429864525794983,
      "learning_rate": 0.00018700956013259292,
      "loss": 1.8646,
      "step": 466
    },
    {
      "epoch": 0.8294849023090586,
      "grad_norm": 0.4763113558292389,
      "learning_rate": 0.00018695440122543262,
      "loss": 1.572,
      "step": 467
    },
    {
      "epoch": 0.8312611012433393,
      "grad_norm": 0.4482514262199402,
      "learning_rate": 0.00018689913363101508,
      "loss": 1.8914,
      "step": 468
    },
    {
      "epoch": 0.8330373001776199,
      "grad_norm": 0.46891582012176514,
      "learning_rate": 0.0001868437574184212,
      "loss": 2.0085,
      "step": 469
    },
    {
      "epoch": 0.8348134991119005,
      "grad_norm": 0.4280640184879303,
      "learning_rate": 0.00018678827265686758,
      "loss": 2.0163,
      "step": 470
    },
    {
      "epoch": 0.8365896980461812,
      "grad_norm": 0.4218597710132599,
      "learning_rate": 0.00018673267941570645,
      "loss": 2.2285,
      "step": 471
    },
    {
      "epoch": 0.8383658969804618,
      "grad_norm": 0.46798062324523926,
      "learning_rate": 0.00018667697776442575,
      "loss": 1.953,
      "step": 472
    },
    {
      "epoch": 0.8401420959147424,
      "grad_norm": 0.4412866532802582,
      "learning_rate": 0.00018662116777264882,
      "loss": 1.9736,
      "step": 473
    },
    {
      "epoch": 0.8419182948490231,
      "grad_norm": 0.43792182207107544,
      "learning_rate": 0.00018656524951013453,
      "loss": 1.9617,
      "step": 474
    },
    {
      "epoch": 0.8436944937833037,
      "grad_norm": 0.447378933429718,
      "learning_rate": 0.00018650922304677692,
      "loss": 1.7922,
      "step": 475
    },
    {
      "epoch": 0.8454706927175843,
      "grad_norm": 0.4354342818260193,
      "learning_rate": 0.0001864530884526054,
      "loss": 2.0694,
      "step": 476
    },
    {
      "epoch": 0.8472468916518651,
      "grad_norm": 0.41499096155166626,
      "learning_rate": 0.0001863968457977846,
      "loss": 1.8601,
      "step": 477
    },
    {
      "epoch": 0.8490230905861457,
      "grad_norm": 0.46143925189971924,
      "learning_rate": 0.000186340495152614,
      "loss": 1.7066,
      "step": 478
    },
    {
      "epoch": 0.8507992895204263,
      "grad_norm": 0.4150255024433136,
      "learning_rate": 0.0001862840365875282,
      "loss": 1.6921,
      "step": 479
    },
    {
      "epoch": 0.8525754884547069,
      "grad_norm": 0.493977427482605,
      "learning_rate": 0.00018622747017309674,
      "loss": 2.3091,
      "step": 480
    },
    {
      "epoch": 0.8543516873889876,
      "grad_norm": 0.4295501708984375,
      "learning_rate": 0.00018617079598002385,
      "loss": 2.0414,
      "step": 481
    },
    {
      "epoch": 0.8561278863232682,
      "grad_norm": 0.4897277057170868,
      "learning_rate": 0.00018611401407914855,
      "loss": 1.8321,
      "step": 482
    },
    {
      "epoch": 0.8579040852575488,
      "grad_norm": 0.4414356052875519,
      "learning_rate": 0.00018605712454144446,
      "loss": 2.3825,
      "step": 483
    },
    {
      "epoch": 0.8596802841918295,
      "grad_norm": 0.42816296219825745,
      "learning_rate": 0.00018600012743801974,
      "loss": 2.2675,
      "step": 484
    },
    {
      "epoch": 0.8614564831261101,
      "grad_norm": 0.4451064467430115,
      "learning_rate": 0.00018594302284011702,
      "loss": 1.8985,
      "step": 485
    },
    {
      "epoch": 0.8632326820603907,
      "grad_norm": 0.48151081800460815,
      "learning_rate": 0.00018588581081911324,
      "loss": 2.0829,
      "step": 486
    },
    {
      "epoch": 0.8650088809946714,
      "grad_norm": 0.4525720477104187,
      "learning_rate": 0.00018582849144651968,
      "loss": 2.102,
      "step": 487
    },
    {
      "epoch": 0.866785079928952,
      "grad_norm": 0.49471238255500793,
      "learning_rate": 0.00018577106479398175,
      "loss": 1.5935,
      "step": 488
    },
    {
      "epoch": 0.8685612788632326,
      "grad_norm": 0.39664557576179504,
      "learning_rate": 0.00018571353093327898,
      "loss": 1.553,
      "step": 489
    },
    {
      "epoch": 0.8703374777975134,
      "grad_norm": 0.4235492944717407,
      "learning_rate": 0.00018565588993632487,
      "loss": 1.3679,
      "step": 490
    },
    {
      "epoch": 0.872113676731794,
      "grad_norm": 0.43580323457717896,
      "learning_rate": 0.00018559814187516692,
      "loss": 1.6947,
      "step": 491
    },
    {
      "epoch": 0.8738898756660746,
      "grad_norm": 0.4421311318874359,
      "learning_rate": 0.00018554028682198634,
      "loss": 1.702,
      "step": 492
    },
    {
      "epoch": 0.8756660746003553,
      "grad_norm": 0.5053011775016785,
      "learning_rate": 0.00018548232484909814,
      "loss": 2.1539,
      "step": 493
    },
    {
      "epoch": 0.8774422735346359,
      "grad_norm": 0.45295193791389465,
      "learning_rate": 0.00018542425602895098,
      "loss": 2.2246,
      "step": 494
    },
    {
      "epoch": 0.8792184724689165,
      "grad_norm": 0.438168466091156,
      "learning_rate": 0.00018536608043412698,
      "loss": 1.898,
      "step": 495
    },
    {
      "epoch": 0.8809946714031972,
      "grad_norm": 0.4346025586128235,
      "learning_rate": 0.00018530779813734184,
      "loss": 1.966,
      "step": 496
    },
    {
      "epoch": 0.8827708703374778,
      "grad_norm": 0.5182878971099854,
      "learning_rate": 0.0001852494092114446,
      "loss": 1.8127,
      "step": 497
    },
    {
      "epoch": 0.8845470692717584,
      "grad_norm": 0.418662428855896,
      "learning_rate": 0.00018519091372941752,
      "loss": 1.8632,
      "step": 498
    },
    {
      "epoch": 0.8863232682060391,
      "grad_norm": 0.43899768590927124,
      "learning_rate": 0.00018513231176437613,
      "loss": 1.9274,
      "step": 499
    },
    {
      "epoch": 0.8880994671403197,
      "grad_norm": 0.4886317849159241,
      "learning_rate": 0.00018507360338956895,
      "loss": 1.9668,
      "step": 500
    },
    {
      "epoch": 0.8880994671403197,
      "eval_loss": 1.8809739351272583,
      "eval_runtime": 17.5309,
      "eval_samples_per_second": 57.099,
      "eval_steps_per_second": 28.578,
      "step": 500
    },
    {
      "epoch": 0.8898756660746003,
      "grad_norm": 0.4358270466327667,
      "learning_rate": 0.00018501478867837765,
      "loss": 1.9539,
      "step": 501
    },
    {
      "epoch": 0.8916518650088809,
      "grad_norm": 0.4731752276420593,
      "learning_rate": 0.00018495586770431665,
      "loss": 1.9241,
      "step": 502
    },
    {
      "epoch": 0.8934280639431617,
      "grad_norm": 0.4150078296661377,
      "learning_rate": 0.00018489684054103335,
      "loss": 1.9992,
      "step": 503
    },
    {
      "epoch": 0.8952042628774423,
      "grad_norm": 0.4190259873867035,
      "learning_rate": 0.00018483770726230775,
      "loss": 1.9245,
      "step": 504
    },
    {
      "epoch": 0.8969804618117229,
      "grad_norm": 0.4663660228252411,
      "learning_rate": 0.0001847784679420526,
      "loss": 2.0205,
      "step": 505
    },
    {
      "epoch": 0.8987566607460036,
      "grad_norm": 0.432867169380188,
      "learning_rate": 0.00018471912265431305,
      "loss": 1.9645,
      "step": 506
    },
    {
      "epoch": 0.9005328596802842,
      "grad_norm": 0.44771745800971985,
      "learning_rate": 0.00018465967147326688,
      "loss": 1.9807,
      "step": 507
    },
    {
      "epoch": 0.9023090586145648,
      "grad_norm": 0.4433210492134094,
      "learning_rate": 0.0001846001144732241,
      "loss": 1.8656,
      "step": 508
    },
    {
      "epoch": 0.9040852575488455,
      "grad_norm": 0.48274320363998413,
      "learning_rate": 0.000184540451728627,
      "loss": 2.1818,
      "step": 509
    },
    {
      "epoch": 0.9058614564831261,
      "grad_norm": 0.5155260562896729,
      "learning_rate": 0.00018448068331405011,
      "loss": 1.7499,
      "step": 510
    },
    {
      "epoch": 0.9076376554174067,
      "grad_norm": 0.5080504417419434,
      "learning_rate": 0.0001844208093042,
      "loss": 1.7187,
      "step": 511
    },
    {
      "epoch": 0.9094138543516874,
      "grad_norm": 0.4338851869106293,
      "learning_rate": 0.0001843608297739152,
      "loss": 2.2401,
      "step": 512
    },
    {
      "epoch": 0.911190053285968,
      "grad_norm": 0.3967326581478119,
      "learning_rate": 0.00018430074479816616,
      "loss": 1.6919,
      "step": 513
    },
    {
      "epoch": 0.9129662522202486,
      "grad_norm": 0.48270365595817566,
      "learning_rate": 0.00018424055445205513,
      "loss": 2.0221,
      "step": 514
    },
    {
      "epoch": 0.9147424511545293,
      "grad_norm": 0.4359724223613739,
      "learning_rate": 0.0001841802588108161,
      "loss": 1.7746,
      "step": 515
    },
    {
      "epoch": 0.91651865008881,
      "grad_norm": 0.4967837929725647,
      "learning_rate": 0.00018411985794981463,
      "loss": 1.7659,
      "step": 516
    },
    {
      "epoch": 0.9182948490230906,
      "grad_norm": 0.4232640266418457,
      "learning_rate": 0.00018405935194454776,
      "loss": 2.0822,
      "step": 517
    },
    {
      "epoch": 0.9200710479573713,
      "grad_norm": 0.5060596466064453,
      "learning_rate": 0.00018399874087064407,
      "loss": 1.7269,
      "step": 518
    },
    {
      "epoch": 0.9218472468916519,
      "grad_norm": 0.4225725531578064,
      "learning_rate": 0.00018393802480386334,
      "loss": 1.9658,
      "step": 519
    },
    {
      "epoch": 0.9236234458259325,
      "grad_norm": 0.4719083309173584,
      "learning_rate": 0.00018387720382009667,
      "loss": 1.8178,
      "step": 520
    },
    {
      "epoch": 0.9253996447602132,
      "grad_norm": 0.4387712776660919,
      "learning_rate": 0.00018381627799536623,
      "loss": 2.1754,
      "step": 521
    },
    {
      "epoch": 0.9271758436944938,
      "grad_norm": 0.4613725244998932,
      "learning_rate": 0.00018375524740582534,
      "loss": 2.1814,
      "step": 522
    },
    {
      "epoch": 0.9289520426287744,
      "grad_norm": 0.4242752492427826,
      "learning_rate": 0.0001836941121277582,
      "loss": 1.8954,
      "step": 523
    },
    {
      "epoch": 0.9307282415630551,
      "grad_norm": 0.44875475764274597,
      "learning_rate": 0.00018363287223757983,
      "loss": 2.2457,
      "step": 524
    },
    {
      "epoch": 0.9325044404973357,
      "grad_norm": 0.4817201793193817,
      "learning_rate": 0.00018357152781183603,
      "loss": 2.21,
      "step": 525
    },
    {
      "epoch": 0.9342806394316163,
      "grad_norm": 0.42431265115737915,
      "learning_rate": 0.00018351007892720335,
      "loss": 1.6701,
      "step": 526
    },
    {
      "epoch": 0.9360568383658969,
      "grad_norm": 0.4761808514595032,
      "learning_rate": 0.0001834485256604888,
      "loss": 1.9825,
      "step": 527
    },
    {
      "epoch": 0.9378330373001776,
      "grad_norm": 0.43151912093162537,
      "learning_rate": 0.00018338686808862988,
      "loss": 1.8466,
      "step": 528
    },
    {
      "epoch": 0.9396092362344582,
      "grad_norm": 0.45268043875694275,
      "learning_rate": 0.00018332510628869448,
      "loss": 2.0086,
      "step": 529
    },
    {
      "epoch": 0.9413854351687388,
      "grad_norm": 0.4743911921977997,
      "learning_rate": 0.0001832632403378808,
      "loss": 2.27,
      "step": 530
    },
    {
      "epoch": 0.9431616341030196,
      "grad_norm": 0.4408976137638092,
      "learning_rate": 0.00018320127031351723,
      "loss": 1.9947,
      "step": 531
    },
    {
      "epoch": 0.9449378330373002,
      "grad_norm": 0.42730963230133057,
      "learning_rate": 0.0001831391962930621,
      "loss": 2.019,
      "step": 532
    },
    {
      "epoch": 0.9467140319715808,
      "grad_norm": 0.4499826729297638,
      "learning_rate": 0.0001830770183541039,
      "loss": 1.979,
      "step": 533
    },
    {
      "epoch": 0.9484902309058615,
      "grad_norm": 0.44856059551239014,
      "learning_rate": 0.00018301473657436095,
      "loss": 2.1086,
      "step": 534
    },
    {
      "epoch": 0.9502664298401421,
      "grad_norm": 0.43914270401000977,
      "learning_rate": 0.00018295235103168132,
      "loss": 1.9852,
      "step": 535
    },
    {
      "epoch": 0.9520426287744227,
      "grad_norm": 0.4576450288295746,
      "learning_rate": 0.00018288986180404283,
      "loss": 2.237,
      "step": 536
    },
    {
      "epoch": 0.9538188277087034,
      "grad_norm": 0.43224361538887024,
      "learning_rate": 0.00018282726896955295,
      "loss": 2.4231,
      "step": 537
    },
    {
      "epoch": 0.955595026642984,
      "grad_norm": 0.41619256138801575,
      "learning_rate": 0.0001827645726064485,
      "loss": 2.1072,
      "step": 538
    },
    {
      "epoch": 0.9573712255772646,
      "grad_norm": 0.4801795184612274,
      "learning_rate": 0.00018270177279309588,
      "loss": 1.7605,
      "step": 539
    },
    {
      "epoch": 0.9591474245115453,
      "grad_norm": 0.44823119044303894,
      "learning_rate": 0.00018263886960799062,
      "loss": 1.8386,
      "step": 540
    },
    {
      "epoch": 0.9609236234458259,
      "grad_norm": 0.4402497410774231,
      "learning_rate": 0.0001825758631297576,
      "loss": 2.0731,
      "step": 541
    },
    {
      "epoch": 0.9626998223801065,
      "grad_norm": 0.43000224232673645,
      "learning_rate": 0.00018251275343715074,
      "loss": 1.9659,
      "step": 542
    },
    {
      "epoch": 0.9644760213143873,
      "grad_norm": 0.41084814071655273,
      "learning_rate": 0.00018244954060905294,
      "loss": 1.7113,
      "step": 543
    },
    {
      "epoch": 0.9662522202486679,
      "grad_norm": 0.44143611192703247,
      "learning_rate": 0.00018238622472447618,
      "loss": 1.7726,
      "step": 544
    },
    {
      "epoch": 0.9680284191829485,
      "grad_norm": 0.45560503005981445,
      "learning_rate": 0.00018232280586256102,
      "loss": 2.1346,
      "step": 545
    },
    {
      "epoch": 0.9698046181172292,
      "grad_norm": 0.434890478849411,
      "learning_rate": 0.00018225928410257688,
      "loss": 2.0363,
      "step": 546
    },
    {
      "epoch": 0.9715808170515098,
      "grad_norm": 0.41692543029785156,
      "learning_rate": 0.00018219565952392176,
      "loss": 1.9604,
      "step": 547
    },
    {
      "epoch": 0.9733570159857904,
      "grad_norm": 0.43887874484062195,
      "learning_rate": 0.00018213193220612223,
      "loss": 2.1172,
      "step": 548
    },
    {
      "epoch": 0.9751332149200711,
      "grad_norm": 0.4600599408149719,
      "learning_rate": 0.00018206810222883317,
      "loss": 2.045,
      "step": 549
    },
    {
      "epoch": 0.9769094138543517,
      "grad_norm": 0.5734915733337402,
      "learning_rate": 0.00018200416967183785,
      "loss": 1.8105,
      "step": 550
    },
    {
      "epoch": 0.9786856127886323,
      "grad_norm": 0.5354852080345154,
      "learning_rate": 0.00018194013461504774,
      "loss": 1.9928,
      "step": 551
    },
    {
      "epoch": 0.9804618117229129,
      "grad_norm": 0.4519789218902588,
      "learning_rate": 0.00018187599713850243,
      "loss": 1.9009,
      "step": 552
    },
    {
      "epoch": 0.9822380106571936,
      "grad_norm": 0.505226731300354,
      "learning_rate": 0.00018181175732236954,
      "loss": 1.7055,
      "step": 553
    },
    {
      "epoch": 0.9840142095914742,
      "grad_norm": 0.45459142327308655,
      "learning_rate": 0.0001817474152469446,
      "loss": 2.2828,
      "step": 554
    },
    {
      "epoch": 0.9857904085257548,
      "grad_norm": 0.4502980709075928,
      "learning_rate": 0.00018168297099265093,
      "loss": 1.7931,
      "step": 555
    },
    {
      "epoch": 0.9875666074600356,
      "grad_norm": 0.4540638029575348,
      "learning_rate": 0.00018161842464003964,
      "loss": 1.8916,
      "step": 556
    },
    {
      "epoch": 0.9893428063943162,
      "grad_norm": 0.5105928778648376,
      "learning_rate": 0.00018155377626978932,
      "loss": 2.2299,
      "step": 557
    },
    {
      "epoch": 0.9911190053285968,
      "grad_norm": 0.4788316786289215,
      "learning_rate": 0.00018148902596270628,
      "loss": 1.9535,
      "step": 558
    },
    {
      "epoch": 0.9928952042628775,
      "grad_norm": 0.4196789562702179,
      "learning_rate": 0.00018142417379972403,
      "loss": 1.9736,
      "step": 559
    },
    {
      "epoch": 0.9946714031971581,
      "grad_norm": 0.4750049412250519,
      "learning_rate": 0.0001813592198619035,
      "loss": 1.7187,
      "step": 560
    },
    {
      "epoch": 0.9964476021314387,
      "grad_norm": 0.4578157961368561,
      "learning_rate": 0.00018129416423043286,
      "loss": 2.3248,
      "step": 561
    },
    {
      "epoch": 0.9982238010657194,
      "grad_norm": 0.4222811460494995,
      "learning_rate": 0.00018122900698662732,
      "loss": 1.2999,
      "step": 562
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.5186070799827576,
      "learning_rate": 0.00018116374821192915,
      "loss": 1.4347,
      "step": 563
    },
    {
      "epoch": 1.0017761989342806,
      "grad_norm": 0.4254154562950134,
      "learning_rate": 0.00018109838798790751,
      "loss": 2.1327,
      "step": 564
    },
    {
      "epoch": 1.0035523978685612,
      "grad_norm": 0.4523231089115143,
      "learning_rate": 0.00018103292639625837,
      "loss": 1.8148,
      "step": 565
    },
    {
      "epoch": 1.0053285968028418,
      "grad_norm": 0.45007315278053284,
      "learning_rate": 0.00018096736351880436,
      "loss": 1.7337,
      "step": 566
    },
    {
      "epoch": 1.0071047957371226,
      "grad_norm": 0.43586209416389465,
      "learning_rate": 0.00018090169943749476,
      "loss": 1.686,
      "step": 567
    },
    {
      "epoch": 1.0088809946714032,
      "grad_norm": 0.396573007106781,
      "learning_rate": 0.00018083593423440534,
      "loss": 1.5284,
      "step": 568
    },
    {
      "epoch": 1.0106571936056838,
      "grad_norm": 0.42471301555633545,
      "learning_rate": 0.00018077006799173826,
      "loss": 1.7742,
      "step": 569
    },
    {
      "epoch": 1.0124333925399644,
      "grad_norm": 0.46667802333831787,
      "learning_rate": 0.00018070410079182197,
      "loss": 1.7827,
      "step": 570
    },
    {
      "epoch": 1.014209591474245,
      "grad_norm": 0.46373942494392395,
      "learning_rate": 0.00018063803271711108,
      "loss": 1.7947,
      "step": 571
    },
    {
      "epoch": 1.0159857904085257,
      "grad_norm": 0.47406166791915894,
      "learning_rate": 0.00018057186385018637,
      "loss": 1.8424,
      "step": 572
    },
    {
      "epoch": 1.0177619893428065,
      "grad_norm": 0.3975731134414673,
      "learning_rate": 0.00018050559427375455,
      "loss": 2.2291,
      "step": 573
    },
    {
      "epoch": 1.019538188277087,
      "grad_norm": 0.4248700737953186,
      "learning_rate": 0.0001804392240706482,
      "loss": 1.8063,
      "step": 574
    },
    {
      "epoch": 1.0213143872113677,
      "grad_norm": 0.42294591665267944,
      "learning_rate": 0.00018037275332382567,
      "loss": 1.7222,
      "step": 575
    },
    {
      "epoch": 1.0230905861456483,
      "grad_norm": 0.4290037751197815,
      "learning_rate": 0.000180306182116371,
      "loss": 1.4998,
      "step": 576
    },
    {
      "epoch": 1.024866785079929,
      "grad_norm": 0.4207656979560852,
      "learning_rate": 0.00018023951053149385,
      "loss": 1.8068,
      "step": 577
    },
    {
      "epoch": 1.0266429840142095,
      "grad_norm": 0.46902996301651,
      "learning_rate": 0.00018017273865252923,
      "loss": 2.062,
      "step": 578
    },
    {
      "epoch": 1.0284191829484903,
      "grad_norm": 0.4815019369125366,
      "learning_rate": 0.00018010586656293763,
      "loss": 1.9637,
      "step": 579
    },
    {
      "epoch": 1.030195381882771,
      "grad_norm": 0.5320613384246826,
      "learning_rate": 0.0001800388943463047,
      "loss": 1.936,
      "step": 580
    },
    {
      "epoch": 1.0319715808170515,
      "grad_norm": 0.5541017651557922,
      "learning_rate": 0.00017997182208634137,
      "loss": 1.8726,
      "step": 581
    },
    {
      "epoch": 1.0337477797513321,
      "grad_norm": 0.4520726501941681,
      "learning_rate": 0.00017990464986688347,
      "loss": 1.5606,
      "step": 582
    },
    {
      "epoch": 1.0355239786856127,
      "grad_norm": 0.45375677943229675,
      "learning_rate": 0.00017983737777189183,
      "loss": 1.6701,
      "step": 583
    },
    {
      "epoch": 1.0373001776198933,
      "grad_norm": 0.49217912554740906,
      "learning_rate": 0.00017977000588545211,
      "loss": 1.8264,
      "step": 584
    },
    {
      "epoch": 1.0390763765541742,
      "grad_norm": 0.48828572034835815,
      "learning_rate": 0.00017970253429177476,
      "loss": 1.9109,
      "step": 585
    },
    {
      "epoch": 1.0408525754884548,
      "grad_norm": 0.476512610912323,
      "learning_rate": 0.00017963496307519484,
      "loss": 1.5214,
      "step": 586
    },
    {
      "epoch": 1.0426287744227354,
      "grad_norm": 0.46450909972190857,
      "learning_rate": 0.00017956729232017184,
      "loss": 1.8614,
      "step": 587
    },
    {
      "epoch": 1.044404973357016,
      "grad_norm": 0.4665273427963257,
      "learning_rate": 0.00017949952211128976,
      "loss": 1.5961,
      "step": 588
    },
    {
      "epoch": 1.0461811722912966,
      "grad_norm": 0.5071829557418823,
      "learning_rate": 0.0001794316525332569,
      "loss": 1.703,
      "step": 589
    },
    {
      "epoch": 1.0479573712255772,
      "grad_norm": 0.4525648355484009,
      "learning_rate": 0.0001793636836709057,
      "loss": 1.6422,
      "step": 590
    },
    {
      "epoch": 1.0497335701598578,
      "grad_norm": 0.5042405128479004,
      "learning_rate": 0.00017929561560919282,
      "loss": 1.6582,
      "step": 591
    },
    {
      "epoch": 1.0515097690941386,
      "grad_norm": 0.48635151982307434,
      "learning_rate": 0.0001792274484331988,
      "loss": 1.7719,
      "step": 592
    },
    {
      "epoch": 1.0532859680284192,
      "grad_norm": 0.4673728048801422,
      "learning_rate": 0.0001791591822281281,
      "loss": 1.8327,
      "step": 593
    },
    {
      "epoch": 1.0550621669626998,
      "grad_norm": 0.4584294259548187,
      "learning_rate": 0.00017909081707930894,
      "loss": 1.9912,
      "step": 594
    },
    {
      "epoch": 1.0568383658969804,
      "grad_norm": 0.4873778820037842,
      "learning_rate": 0.00017902235307219332,
      "loss": 1.4972,
      "step": 595
    },
    {
      "epoch": 1.058614564831261,
      "grad_norm": 0.4925454556941986,
      "learning_rate": 0.00017895379029235668,
      "loss": 1.8964,
      "step": 596
    },
    {
      "epoch": 1.0603907637655416,
      "grad_norm": 0.5414361953735352,
      "learning_rate": 0.00017888512882549798,
      "loss": 1.9351,
      "step": 597
    },
    {
      "epoch": 1.0621669626998225,
      "grad_norm": 0.4924388527870178,
      "learning_rate": 0.00017881636875743948,
      "loss": 1.5637,
      "step": 598
    },
    {
      "epoch": 1.063943161634103,
      "grad_norm": 0.5515813231468201,
      "learning_rate": 0.00017874751017412675,
      "loss": 1.6911,
      "step": 599
    },
    {
      "epoch": 1.0657193605683837,
      "grad_norm": 0.47421300411224365,
      "learning_rate": 0.00017867855316162847,
      "loss": 1.5726,
      "step": 600
    },
    {
      "epoch": 1.0657193605683837,
      "eval_loss": 1.8857200145721436,
      "eval_runtime": 17.5526,
      "eval_samples_per_second": 57.029,
      "eval_steps_per_second": 28.543,
      "step": 600
    },
    {
      "epoch": 1.0674955595026643,
      "grad_norm": 0.5206230878829956,
      "learning_rate": 0.00017860949780613633,
      "loss": 2.2535,
      "step": 601
    },
    {
      "epoch": 1.0692717584369449,
      "grad_norm": 0.4943333864212036,
      "learning_rate": 0.00017854034419396503,
      "loss": 1.9514,
      "step": 602
    },
    {
      "epoch": 1.0710479573712255,
      "grad_norm": 0.5364169478416443,
      "learning_rate": 0.00017847109241155194,
      "loss": 1.9041,
      "step": 603
    },
    {
      "epoch": 1.0728241563055063,
      "grad_norm": 0.4877062737941742,
      "learning_rate": 0.00017840174254545724,
      "loss": 2.233,
      "step": 604
    },
    {
      "epoch": 1.074600355239787,
      "grad_norm": 0.4746587574481964,
      "learning_rate": 0.00017833229468236367,
      "loss": 1.8053,
      "step": 605
    },
    {
      "epoch": 1.0763765541740675,
      "grad_norm": 0.5281076431274414,
      "learning_rate": 0.0001782627489090765,
      "loss": 2.0198,
      "step": 606
    },
    {
      "epoch": 1.0781527531083481,
      "grad_norm": 0.4775375425815582,
      "learning_rate": 0.00017819310531252334,
      "loss": 1.8548,
      "step": 607
    },
    {
      "epoch": 1.0799289520426287,
      "grad_norm": 0.49485769867897034,
      "learning_rate": 0.00017812336397975405,
      "loss": 1.8465,
      "step": 608
    },
    {
      "epoch": 1.0817051509769093,
      "grad_norm": 0.6340792179107666,
      "learning_rate": 0.00017805352499794075,
      "loss": 1.6261,
      "step": 609
    },
    {
      "epoch": 1.0834813499111902,
      "grad_norm": 0.49063217639923096,
      "learning_rate": 0.00017798358845437753,
      "loss": 1.5927,
      "step": 610
    },
    {
      "epoch": 1.0852575488454708,
      "grad_norm": 0.5340775847434998,
      "learning_rate": 0.00017791355443648043,
      "loss": 1.5527,
      "step": 611
    },
    {
      "epoch": 1.0870337477797514,
      "grad_norm": 0.4960753917694092,
      "learning_rate": 0.00017784342303178737,
      "loss": 1.6396,
      "step": 612
    },
    {
      "epoch": 1.088809946714032,
      "grad_norm": 0.5109440088272095,
      "learning_rate": 0.0001777731943279579,
      "loss": 1.9784,
      "step": 613
    },
    {
      "epoch": 1.0905861456483126,
      "grad_norm": 0.5133154988288879,
      "learning_rate": 0.0001777028684127734,
      "loss": 1.7523,
      "step": 614
    },
    {
      "epoch": 1.0923623445825932,
      "grad_norm": 0.4964364469051361,
      "learning_rate": 0.0001776324453741365,
      "loss": 2.0451,
      "step": 615
    },
    {
      "epoch": 1.0941385435168738,
      "grad_norm": 0.4768456518650055,
      "learning_rate": 0.00017756192530007142,
      "loss": 2.1423,
      "step": 616
    },
    {
      "epoch": 1.0959147424511546,
      "grad_norm": 0.5926968455314636,
      "learning_rate": 0.0001774913082787235,
      "loss": 1.8163,
      "step": 617
    },
    {
      "epoch": 1.0976909413854352,
      "grad_norm": 0.5189889073371887,
      "learning_rate": 0.00017742059439835946,
      "loss": 1.7732,
      "step": 618
    },
    {
      "epoch": 1.0994671403197158,
      "grad_norm": 0.516622006893158,
      "learning_rate": 0.0001773497837473669,
      "loss": 1.6427,
      "step": 619
    },
    {
      "epoch": 1.1012433392539964,
      "grad_norm": 0.5253352522850037,
      "learning_rate": 0.00017727887641425452,
      "loss": 1.8217,
      "step": 620
    },
    {
      "epoch": 1.103019538188277,
      "grad_norm": 0.5453703999519348,
      "learning_rate": 0.00017720787248765173,
      "loss": 1.9083,
      "step": 621
    },
    {
      "epoch": 1.1047957371225576,
      "grad_norm": 0.5059117078781128,
      "learning_rate": 0.0001771367720563088,
      "loss": 1.7444,
      "step": 622
    },
    {
      "epoch": 1.1065719360568385,
      "grad_norm": 0.5131975412368774,
      "learning_rate": 0.00017706557520909657,
      "loss": 1.4651,
      "step": 623
    },
    {
      "epoch": 1.108348134991119,
      "grad_norm": 0.5031772255897522,
      "learning_rate": 0.00017699428203500639,
      "loss": 1.8513,
      "step": 624
    },
    {
      "epoch": 1.1101243339253997,
      "grad_norm": 0.5379109978675842,
      "learning_rate": 0.00017692289262315,
      "loss": 1.804,
      "step": 625
    },
    {
      "epoch": 1.1119005328596803,
      "grad_norm": 0.5580901503562927,
      "learning_rate": 0.00017685140706275945,
      "loss": 1.8026,
      "step": 626
    },
    {
      "epoch": 1.1136767317939609,
      "grad_norm": 0.5205426216125488,
      "learning_rate": 0.00017677982544318698,
      "loss": 1.8926,
      "step": 627
    },
    {
      "epoch": 1.1154529307282415,
      "grad_norm": 0.4798078238964081,
      "learning_rate": 0.0001767081478539049,
      "loss": 2.0249,
      "step": 628
    },
    {
      "epoch": 1.1172291296625223,
      "grad_norm": 0.5220930576324463,
      "learning_rate": 0.00017663637438450542,
      "loss": 1.5418,
      "step": 629
    },
    {
      "epoch": 1.119005328596803,
      "grad_norm": 0.5289032459259033,
      "learning_rate": 0.00017656450512470068,
      "loss": 1.7223,
      "step": 630
    },
    {
      "epoch": 1.1207815275310835,
      "grad_norm": 0.5167121291160583,
      "learning_rate": 0.0001764925401643225,
      "loss": 1.9939,
      "step": 631
    },
    {
      "epoch": 1.1225577264653641,
      "grad_norm": 0.5160818099975586,
      "learning_rate": 0.0001764204795933223,
      "loss": 1.6725,
      "step": 632
    },
    {
      "epoch": 1.1243339253996447,
      "grad_norm": 0.46935635805130005,
      "learning_rate": 0.00017634832350177105,
      "loss": 1.9014,
      "step": 633
    },
    {
      "epoch": 1.1261101243339253,
      "grad_norm": 0.5138757228851318,
      "learning_rate": 0.0001762760719798591,
      "loss": 1.8652,
      "step": 634
    },
    {
      "epoch": 1.1278863232682061,
      "grad_norm": 0.5317404270172119,
      "learning_rate": 0.00017620372511789606,
      "loss": 2.1189,
      "step": 635
    },
    {
      "epoch": 1.1296625222024868,
      "grad_norm": 0.49650683999061584,
      "learning_rate": 0.00017613128300631074,
      "loss": 1.5899,
      "step": 636
    },
    {
      "epoch": 1.1314387211367674,
      "grad_norm": 0.5572506785392761,
      "learning_rate": 0.00017605874573565103,
      "loss": 1.6655,
      "step": 637
    },
    {
      "epoch": 1.133214920071048,
      "grad_norm": 0.5323126912117004,
      "learning_rate": 0.00017598611339658367,
      "loss": 1.7051,
      "step": 638
    },
    {
      "epoch": 1.1349911190053286,
      "grad_norm": 0.5285413265228271,
      "learning_rate": 0.0001759133860798943,
      "loss": 1.9598,
      "step": 639
    },
    {
      "epoch": 1.1367673179396092,
      "grad_norm": 0.5421034693717957,
      "learning_rate": 0.00017584056387648727,
      "loss": 1.7226,
      "step": 640
    },
    {
      "epoch": 1.1385435168738898,
      "grad_norm": 0.489807665348053,
      "learning_rate": 0.00017576764687738555,
      "loss": 1.7181,
      "step": 641
    },
    {
      "epoch": 1.1403197158081706,
      "grad_norm": 0.5361825823783875,
      "learning_rate": 0.0001756946351737305,
      "loss": 1.7632,
      "step": 642
    },
    {
      "epoch": 1.1420959147424512,
      "grad_norm": 0.5283854603767395,
      "learning_rate": 0.000175621528856782,
      "loss": 1.9696,
      "step": 643
    },
    {
      "epoch": 1.1438721136767318,
      "grad_norm": 0.5732810497283936,
      "learning_rate": 0.0001755483280179181,
      "loss": 1.9936,
      "step": 644
    },
    {
      "epoch": 1.1456483126110124,
      "grad_norm": 0.5451516509056091,
      "learning_rate": 0.00017547503274863496,
      "loss": 1.8094,
      "step": 645
    },
    {
      "epoch": 1.147424511545293,
      "grad_norm": 0.5306533575057983,
      "learning_rate": 0.00017540164314054692,
      "loss": 2.161,
      "step": 646
    },
    {
      "epoch": 1.1492007104795736,
      "grad_norm": 0.5237314105033875,
      "learning_rate": 0.00017532815928538605,
      "loss": 1.9778,
      "step": 647
    },
    {
      "epoch": 1.1509769094138544,
      "grad_norm": 0.5186898112297058,
      "learning_rate": 0.00017525458127500237,
      "loss": 1.9595,
      "step": 648
    },
    {
      "epoch": 1.152753108348135,
      "grad_norm": 0.4996102750301361,
      "learning_rate": 0.00017518090920136352,
      "loss": 1.8521,
      "step": 649
    },
    {
      "epoch": 1.1545293072824157,
      "grad_norm": 0.5294045209884644,
      "learning_rate": 0.0001751071431565547,
      "loss": 1.5712,
      "step": 650
    },
    {
      "epoch": 1.1563055062166963,
      "grad_norm": 0.5128822922706604,
      "learning_rate": 0.00017503328323277862,
      "loss": 1.497,
      "step": 651
    },
    {
      "epoch": 1.1580817051509769,
      "grad_norm": 0.5400491952896118,
      "learning_rate": 0.00017495932952235531,
      "loss": 2.1218,
      "step": 652
    },
    {
      "epoch": 1.1598579040852575,
      "grad_norm": 0.5141798853874207,
      "learning_rate": 0.00017488528211772198,
      "loss": 1.672,
      "step": 653
    },
    {
      "epoch": 1.161634103019538,
      "grad_norm": 0.5169104337692261,
      "learning_rate": 0.00017481114111143306,
      "loss": 1.7585,
      "step": 654
    },
    {
      "epoch": 1.163410301953819,
      "grad_norm": 0.5610992312431335,
      "learning_rate": 0.00017473690659615987,
      "loss": 1.8248,
      "step": 655
    },
    {
      "epoch": 1.1651865008880995,
      "grad_norm": 0.5439818501472473,
      "learning_rate": 0.00017466257866469064,
      "loss": 1.5611,
      "step": 656
    },
    {
      "epoch": 1.16696269982238,
      "grad_norm": 0.5320035219192505,
      "learning_rate": 0.0001745881574099304,
      "loss": 1.7207,
      "step": 657
    },
    {
      "epoch": 1.1687388987566607,
      "grad_norm": 0.5196565985679626,
      "learning_rate": 0.0001745136429249008,
      "loss": 1.8095,
      "step": 658
    },
    {
      "epoch": 1.1705150976909413,
      "grad_norm": 0.5675838589668274,
      "learning_rate": 0.00017443903530274,
      "loss": 2.022,
      "step": 659
    },
    {
      "epoch": 1.1722912966252221,
      "grad_norm": 0.5221540331840515,
      "learning_rate": 0.0001743643346367026,
      "loss": 2.1644,
      "step": 660
    },
    {
      "epoch": 1.1740674955595027,
      "grad_norm": 0.526856541633606,
      "learning_rate": 0.00017428954102015952,
      "loss": 1.45,
      "step": 661
    },
    {
      "epoch": 1.1758436944937833,
      "grad_norm": 0.5167378783226013,
      "learning_rate": 0.00017421465454659784,
      "loss": 1.6023,
      "step": 662
    },
    {
      "epoch": 1.177619893428064,
      "grad_norm": 0.5605522990226746,
      "learning_rate": 0.0001741396753096207,
      "loss": 1.9801,
      "step": 663
    },
    {
      "epoch": 1.1793960923623446,
      "grad_norm": 0.5621047616004944,
      "learning_rate": 0.00017406460340294716,
      "loss": 1.6569,
      "step": 664
    },
    {
      "epoch": 1.1811722912966252,
      "grad_norm": 0.49053460359573364,
      "learning_rate": 0.0001739894389204122,
      "loss": 1.8767,
      "step": 665
    },
    {
      "epoch": 1.1829484902309058,
      "grad_norm": 0.5308481454849243,
      "learning_rate": 0.00017391418195596644,
      "loss": 2.0269,
      "step": 666
    },
    {
      "epoch": 1.1847246891651866,
      "grad_norm": 0.5386219620704651,
      "learning_rate": 0.00017383883260367608,
      "loss": 2.0544,
      "step": 667
    },
    {
      "epoch": 1.1865008880994672,
      "grad_norm": 0.5527297854423523,
      "learning_rate": 0.00017376339095772288,
      "loss": 1.6785,
      "step": 668
    },
    {
      "epoch": 1.1882770870337478,
      "grad_norm": 0.5182768702507019,
      "learning_rate": 0.0001736878571124039,
      "loss": 1.9236,
      "step": 669
    },
    {
      "epoch": 1.1900532859680284,
      "grad_norm": 0.5472124218940735,
      "learning_rate": 0.00017361223116213142,
      "loss": 2.2347,
      "step": 670
    },
    {
      "epoch": 1.191829484902309,
      "grad_norm": 0.5929980278015137,
      "learning_rate": 0.00017353651320143292,
      "loss": 1.9355,
      "step": 671
    },
    {
      "epoch": 1.1936056838365896,
      "grad_norm": 0.5103628039360046,
      "learning_rate": 0.00017346070332495083,
      "loss": 1.6888,
      "step": 672
    },
    {
      "epoch": 1.1953818827708704,
      "grad_norm": 0.5247202515602112,
      "learning_rate": 0.00017338480162744252,
      "loss": 1.6632,
      "step": 673
    },
    {
      "epoch": 1.197158081705151,
      "grad_norm": 0.546600878238678,
      "learning_rate": 0.00017330880820378005,
      "loss": 1.9117,
      "step": 674
    },
    {
      "epoch": 1.1989342806394316,
      "grad_norm": 0.49607381224632263,
      "learning_rate": 0.0001732327231489502,
      "loss": 1.8096,
      "step": 675
    },
    {
      "epoch": 1.2007104795737122,
      "grad_norm": 0.5528424978256226,
      "learning_rate": 0.0001731565465580543,
      "loss": 1.9315,
      "step": 676
    },
    {
      "epoch": 1.2024866785079928,
      "grad_norm": 0.5358368754386902,
      "learning_rate": 0.000173080278526308,
      "loss": 1.648,
      "step": 677
    },
    {
      "epoch": 1.2042628774422734,
      "grad_norm": 0.5684419274330139,
      "learning_rate": 0.0001730039191490413,
      "loss": 1.6934,
      "step": 678
    },
    {
      "epoch": 1.206039076376554,
      "grad_norm": 0.511469841003418,
      "learning_rate": 0.00017292746852169843,
      "loss": 1.7938,
      "step": 679
    },
    {
      "epoch": 1.2078152753108349,
      "grad_norm": 0.5751768946647644,
      "learning_rate": 0.0001728509267398376,
      "loss": 1.8923,
      "step": 680
    },
    {
      "epoch": 1.2095914742451155,
      "grad_norm": 0.5053960680961609,
      "learning_rate": 0.00017277429389913097,
      "loss": 1.7296,
      "step": 681
    },
    {
      "epoch": 1.211367673179396,
      "grad_norm": 0.5264067649841309,
      "learning_rate": 0.0001726975700953645,
      "loss": 1.7754,
      "step": 682
    },
    {
      "epoch": 1.2131438721136767,
      "grad_norm": 0.5489183664321899,
      "learning_rate": 0.00017262075542443794,
      "loss": 1.6879,
      "step": 683
    },
    {
      "epoch": 1.2149200710479573,
      "grad_norm": 0.6085485816001892,
      "learning_rate": 0.0001725438499823645,
      "loss": 2.1698,
      "step": 684
    },
    {
      "epoch": 1.2166962699822381,
      "grad_norm": 0.5800978541374207,
      "learning_rate": 0.00017246685386527097,
      "loss": 1.8451,
      "step": 685
    },
    {
      "epoch": 1.2184724689165187,
      "grad_norm": 0.5479023456573486,
      "learning_rate": 0.00017238976716939735,
      "loss": 1.8011,
      "step": 686
    },
    {
      "epoch": 1.2202486678507993,
      "grad_norm": 0.5233104825019836,
      "learning_rate": 0.0001723125899910969,
      "loss": 2.0469,
      "step": 687
    },
    {
      "epoch": 1.22202486678508,
      "grad_norm": 0.6056929230690002,
      "learning_rate": 0.00017223532242683604,
      "loss": 1.3309,
      "step": 688
    },
    {
      "epoch": 1.2238010657193605,
      "grad_norm": 0.5565758347511292,
      "learning_rate": 0.00017215796457319414,
      "loss": 1.8377,
      "step": 689
    },
    {
      "epoch": 1.2255772646536411,
      "grad_norm": 0.5194698572158813,
      "learning_rate": 0.00017208051652686335,
      "loss": 2.0637,
      "step": 690
    },
    {
      "epoch": 1.2273534635879217,
      "grad_norm": 0.5993460416793823,
      "learning_rate": 0.00017200297838464862,
      "loss": 1.8372,
      "step": 691
    },
    {
      "epoch": 1.2291296625222026,
      "grad_norm": 0.5329267978668213,
      "learning_rate": 0.00017192535024346757,
      "loss": 1.7417,
      "step": 692
    },
    {
      "epoch": 1.2309058614564832,
      "grad_norm": 0.6450623273849487,
      "learning_rate": 0.00017184763220035016,
      "loss": 1.668,
      "step": 693
    },
    {
      "epoch": 1.2326820603907638,
      "grad_norm": 0.6194930672645569,
      "learning_rate": 0.00017176982435243888,
      "loss": 2.0383,
      "step": 694
    },
    {
      "epoch": 1.2344582593250444,
      "grad_norm": 0.5298301577568054,
      "learning_rate": 0.00017169192679698835,
      "loss": 1.7718,
      "step": 695
    },
    {
      "epoch": 1.236234458259325,
      "grad_norm": 0.5626735687255859,
      "learning_rate": 0.0001716139396313654,
      "loss": 1.8241,
      "step": 696
    },
    {
      "epoch": 1.2380106571936056,
      "grad_norm": 0.5139234066009521,
      "learning_rate": 0.0001715358629530488,
      "loss": 1.8726,
      "step": 697
    },
    {
      "epoch": 1.2397868561278864,
      "grad_norm": 0.5750727653503418,
      "learning_rate": 0.0001714576968596293,
      "loss": 1.6959,
      "step": 698
    },
    {
      "epoch": 1.241563055062167,
      "grad_norm": 0.5066772103309631,
      "learning_rate": 0.0001713794414488093,
      "loss": 2.1267,
      "step": 699
    },
    {
      "epoch": 1.2433392539964476,
      "grad_norm": 0.5709089636802673,
      "learning_rate": 0.00017130109681840288,
      "loss": 1.4244,
      "step": 700
    },
    {
      "epoch": 1.2433392539964476,
      "eval_loss": 1.8886024951934814,
      "eval_runtime": 17.5056,
      "eval_samples_per_second": 57.182,
      "eval_steps_per_second": 28.619,
      "step": 700
    },
    {
      "epoch": 1.2451154529307282,
      "grad_norm": 0.571684718132019,
      "learning_rate": 0.00017122266306633573,
      "loss": 1.7558,
      "step": 701
    },
    {
      "epoch": 1.2468916518650088,
      "grad_norm": 0.5277456641197205,
      "learning_rate": 0.0001711441402906448,
      "loss": 1.7414,
      "step": 702
    },
    {
      "epoch": 1.2486678507992894,
      "grad_norm": 0.5609720945358276,
      "learning_rate": 0.00017106552858947837,
      "loss": 1.8811,
      "step": 703
    },
    {
      "epoch": 1.25044404973357,
      "grad_norm": 0.5095233917236328,
      "learning_rate": 0.00017098682806109588,
      "loss": 1.7182,
      "step": 704
    },
    {
      "epoch": 1.2522202486678509,
      "grad_norm": 0.5532099008560181,
      "learning_rate": 0.00017090803880386783,
      "loss": 2.1275,
      "step": 705
    },
    {
      "epoch": 1.2539964476021315,
      "grad_norm": 0.6051645874977112,
      "learning_rate": 0.00017082916091627553,
      "loss": 1.6856,
      "step": 706
    },
    {
      "epoch": 1.255772646536412,
      "grad_norm": 0.5401607751846313,
      "learning_rate": 0.00017075019449691117,
      "loss": 1.9044,
      "step": 707
    },
    {
      "epoch": 1.2575488454706927,
      "grad_norm": 0.5397858023643494,
      "learning_rate": 0.00017067113964447752,
      "loss": 1.7537,
      "step": 708
    },
    {
      "epoch": 1.2593250444049733,
      "grad_norm": 0.5870073437690735,
      "learning_rate": 0.00017059199645778797,
      "loss": 2.0909,
      "step": 709
    },
    {
      "epoch": 1.261101243339254,
      "grad_norm": 0.5679288506507874,
      "learning_rate": 0.00017051276503576623,
      "loss": 1.5975,
      "step": 710
    },
    {
      "epoch": 1.2628774422735347,
      "grad_norm": 0.5446649789810181,
      "learning_rate": 0.00017043344547744638,
      "loss": 1.6189,
      "step": 711
    },
    {
      "epoch": 1.2646536412078153,
      "grad_norm": 0.5350863933563232,
      "learning_rate": 0.0001703540378819726,
      "loss": 1.6464,
      "step": 712
    },
    {
      "epoch": 1.266429840142096,
      "grad_norm": 0.5401250123977661,
      "learning_rate": 0.00017027454234859915,
      "loss": 1.7201,
      "step": 713
    },
    {
      "epoch": 1.2682060390763765,
      "grad_norm": 0.5266538262367249,
      "learning_rate": 0.0001701949589766902,
      "loss": 1.7038,
      "step": 714
    },
    {
      "epoch": 1.2699822380106571,
      "grad_norm": 0.5663775205612183,
      "learning_rate": 0.00017011528786571969,
      "loss": 1.9402,
      "step": 715
    },
    {
      "epoch": 1.2717584369449377,
      "grad_norm": 0.5794070363044739,
      "learning_rate": 0.00017003552911527129,
      "loss": 1.7568,
      "step": 716
    },
    {
      "epoch": 1.2735346358792183,
      "grad_norm": 0.5962381958961487,
      "learning_rate": 0.00016995568282503808,
      "loss": 1.8345,
      "step": 717
    },
    {
      "epoch": 1.2753108348134992,
      "grad_norm": 0.5339125394821167,
      "learning_rate": 0.00016987574909482273,
      "loss": 2.0343,
      "step": 718
    },
    {
      "epoch": 1.2770870337477798,
      "grad_norm": 0.5415145754814148,
      "learning_rate": 0.00016979572802453712,
      "loss": 1.9005,
      "step": 719
    },
    {
      "epoch": 1.2788632326820604,
      "grad_norm": 0.5795564651489258,
      "learning_rate": 0.00016971561971420223,
      "loss": 1.8352,
      "step": 720
    },
    {
      "epoch": 1.280639431616341,
      "grad_norm": 0.5574411749839783,
      "learning_rate": 0.00016963542426394828,
      "loss": 2.1123,
      "step": 721
    },
    {
      "epoch": 1.2824156305506218,
      "grad_norm": 0.5219211578369141,
      "learning_rate": 0.00016955514177401413,
      "loss": 1.8962,
      "step": 722
    },
    {
      "epoch": 1.2841918294849024,
      "grad_norm": 0.5482720136642456,
      "learning_rate": 0.00016947477234474772,
      "loss": 1.8585,
      "step": 723
    },
    {
      "epoch": 1.285968028419183,
      "grad_norm": 0.5489762425422668,
      "learning_rate": 0.00016939431607660547,
      "loss": 2.2379,
      "step": 724
    },
    {
      "epoch": 1.2877442273534636,
      "grad_norm": 0.5615796446800232,
      "learning_rate": 0.0001693137730701524,
      "loss": 2.0435,
      "step": 725
    },
    {
      "epoch": 1.2895204262877442,
      "grad_norm": 0.5679551362991333,
      "learning_rate": 0.00016923314342606193,
      "loss": 1.9769,
      "step": 726
    },
    {
      "epoch": 1.2912966252220248,
      "grad_norm": 0.5800356864929199,
      "learning_rate": 0.00016915242724511583,
      "loss": 1.3791,
      "step": 727
    },
    {
      "epoch": 1.2930728241563054,
      "grad_norm": 0.5644147396087646,
      "learning_rate": 0.00016907162462820397,
      "loss": 1.819,
      "step": 728
    },
    {
      "epoch": 1.294849023090586,
      "grad_norm": 0.6086805462837219,
      "learning_rate": 0.0001689907356763243,
      "loss": 1.5425,
      "step": 729
    },
    {
      "epoch": 1.2966252220248669,
      "grad_norm": 0.573344886302948,
      "learning_rate": 0.00016890976049058264,
      "loss": 1.7091,
      "step": 730
    },
    {
      "epoch": 1.2984014209591475,
      "grad_norm": 0.5351448655128479,
      "learning_rate": 0.00016882869917219265,
      "loss": 1.7453,
      "step": 731
    },
    {
      "epoch": 1.300177619893428,
      "grad_norm": 0.6074206829071045,
      "learning_rate": 0.00016874755182247562,
      "loss": 1.9393,
      "step": 732
    },
    {
      "epoch": 1.3019538188277087,
      "grad_norm": 0.5404677391052246,
      "learning_rate": 0.00016866631854286034,
      "loss": 1.4523,
      "step": 733
    },
    {
      "epoch": 1.3037300177619893,
      "grad_norm": 0.5446416139602661,
      "learning_rate": 0.0001685849994348831,
      "loss": 1.6481,
      "step": 734
    },
    {
      "epoch": 1.30550621669627,
      "grad_norm": 0.579345166683197,
      "learning_rate": 0.00016850359460018736,
      "loss": 1.6238,
      "step": 735
    },
    {
      "epoch": 1.3072824156305507,
      "grad_norm": 0.6685298085212708,
      "learning_rate": 0.0001684221041405238,
      "loss": 2.3116,
      "step": 736
    },
    {
      "epoch": 1.3090586145648313,
      "grad_norm": 0.6404750347137451,
      "learning_rate": 0.00016834052815775018,
      "loss": 1.7988,
      "step": 737
    },
    {
      "epoch": 1.310834813499112,
      "grad_norm": 0.5730388760566711,
      "learning_rate": 0.00016825886675383097,
      "loss": 2.0111,
      "step": 738
    },
    {
      "epoch": 1.3126110124333925,
      "grad_norm": 0.568732500076294,
      "learning_rate": 0.0001681771200308376,
      "loss": 1.8908,
      "step": 739
    },
    {
      "epoch": 1.3143872113676731,
      "grad_norm": 0.5550668239593506,
      "learning_rate": 0.00016809528809094807,
      "loss": 1.533,
      "step": 740
    },
    {
      "epoch": 1.3161634103019537,
      "grad_norm": 0.5564268827438354,
      "learning_rate": 0.00016801337103644688,
      "loss": 1.9229,
      "step": 741
    },
    {
      "epoch": 1.3179396092362343,
      "grad_norm": 0.5789637565612793,
      "learning_rate": 0.00016793136896972496,
      "loss": 1.7939,
      "step": 742
    },
    {
      "epoch": 1.3197158081705151,
      "grad_norm": 0.6003232598304749,
      "learning_rate": 0.0001678492819932795,
      "loss": 1.6553,
      "step": 743
    },
    {
      "epoch": 1.3214920071047958,
      "grad_norm": 0.5843870639801025,
      "learning_rate": 0.0001677671102097137,
      "loss": 1.7591,
      "step": 744
    },
    {
      "epoch": 1.3232682060390764,
      "grad_norm": 0.5615906119346619,
      "learning_rate": 0.00016768485372173697,
      "loss": 2.0585,
      "step": 745
    },
    {
      "epoch": 1.325044404973357,
      "grad_norm": 0.6087834239006042,
      "learning_rate": 0.00016760251263216443,
      "loss": 1.8748,
      "step": 746
    },
    {
      "epoch": 1.3268206039076378,
      "grad_norm": 0.5473018884658813,
      "learning_rate": 0.000167520087043917,
      "loss": 1.9399,
      "step": 747
    },
    {
      "epoch": 1.3285968028419184,
      "grad_norm": 0.6015052795410156,
      "learning_rate": 0.00016743757706002123,
      "loss": 1.8277,
      "step": 748
    },
    {
      "epoch": 1.330373001776199,
      "grad_norm": 0.5957398414611816,
      "learning_rate": 0.00016735498278360917,
      "loss": 1.5096,
      "step": 749
    },
    {
      "epoch": 1.3321492007104796,
      "grad_norm": 0.5325897932052612,
      "learning_rate": 0.00016727230431791816,
      "loss": 1.9472,
      "step": 750
    },
    {
      "epoch": 1.3339253996447602,
      "grad_norm": 0.5355690717697144,
      "learning_rate": 0.0001671895417662909,
      "loss": 1.9724,
      "step": 751
    },
    {
      "epoch": 1.3357015985790408,
      "grad_norm": 0.5965506434440613,
      "learning_rate": 0.000167106695232175,
      "loss": 1.7442,
      "step": 752
    },
    {
      "epoch": 1.3374777975133214,
      "grad_norm": 0.5852263569831848,
      "learning_rate": 0.00016702376481912327,
      "loss": 1.7285,
      "step": 753
    },
    {
      "epoch": 1.339253996447602,
      "grad_norm": 0.5853632092475891,
      "learning_rate": 0.00016694075063079317,
      "loss": 2.0343,
      "step": 754
    },
    {
      "epoch": 1.3410301953818828,
      "grad_norm": 0.5846993923187256,
      "learning_rate": 0.00016685765277094702,
      "loss": 1.5856,
      "step": 755
    },
    {
      "epoch": 1.3428063943161634,
      "grad_norm": 0.5333015322685242,
      "learning_rate": 0.00016677447134345153,
      "loss": 1.8115,
      "step": 756
    },
    {
      "epoch": 1.344582593250444,
      "grad_norm": 0.5541965961456299,
      "learning_rate": 0.00016669120645227813,
      "loss": 1.9712,
      "step": 757
    },
    {
      "epoch": 1.3463587921847247,
      "grad_norm": 0.591178834438324,
      "learning_rate": 0.0001666078582015024,
      "loss": 1.9811,
      "step": 758
    },
    {
      "epoch": 1.3481349911190053,
      "grad_norm": 0.5791703462600708,
      "learning_rate": 0.00016652442669530407,
      "loss": 1.5328,
      "step": 759
    },
    {
      "epoch": 1.349911190053286,
      "grad_norm": 0.5852805972099304,
      "learning_rate": 0.00016644091203796708,
      "loss": 1.8261,
      "step": 760
    },
    {
      "epoch": 1.3516873889875667,
      "grad_norm": 0.5541784763336182,
      "learning_rate": 0.0001663573143338792,
      "loss": 1.8663,
      "step": 761
    },
    {
      "epoch": 1.3534635879218473,
      "grad_norm": 0.5544590353965759,
      "learning_rate": 0.0001662736336875321,
      "loss": 1.9776,
      "step": 762
    },
    {
      "epoch": 1.355239786856128,
      "grad_norm": 0.6145575642585754,
      "learning_rate": 0.000166189870203521,
      "loss": 2.1513,
      "step": 763
    },
    {
      "epoch": 1.3570159857904085,
      "grad_norm": 0.573694109916687,
      "learning_rate": 0.00016610602398654472,
      "loss": 1.693,
      "step": 764
    },
    {
      "epoch": 1.358792184724689,
      "grad_norm": 0.5703313946723938,
      "learning_rate": 0.0001660220951414055,
      "loss": 1.8145,
      "step": 765
    },
    {
      "epoch": 1.3605683836589697,
      "grad_norm": 0.539929986000061,
      "learning_rate": 0.0001659380837730089,
      "loss": 1.9159,
      "step": 766
    },
    {
      "epoch": 1.3623445825932503,
      "grad_norm": 0.588083803653717,
      "learning_rate": 0.0001658539899863635,
      "loss": 1.5222,
      "step": 767
    },
    {
      "epoch": 1.3641207815275311,
      "grad_norm": 0.5885948538780212,
      "learning_rate": 0.00016576981388658103,
      "loss": 1.9025,
      "step": 768
    },
    {
      "epoch": 1.3658969804618117,
      "grad_norm": 0.5786214470863342,
      "learning_rate": 0.00016568555557887606,
      "loss": 1.9279,
      "step": 769
    },
    {
      "epoch": 1.3676731793960923,
      "grad_norm": 0.546720027923584,
      "learning_rate": 0.00016560121516856588,
      "loss": 1.8584,
      "step": 770
    },
    {
      "epoch": 1.369449378330373,
      "grad_norm": 0.595921516418457,
      "learning_rate": 0.00016551679276107046,
      "loss": 1.7813,
      "step": 771
    },
    {
      "epoch": 1.3712255772646538,
      "grad_norm": 0.5792990326881409,
      "learning_rate": 0.0001654322884619122,
      "loss": 2.1029,
      "step": 772
    },
    {
      "epoch": 1.3730017761989344,
      "grad_norm": 0.6024417877197266,
      "learning_rate": 0.0001653477023767159,
      "loss": 1.8408,
      "step": 773
    },
    {
      "epoch": 1.374777975133215,
      "grad_norm": 0.6299260258674622,
      "learning_rate": 0.0001652630346112086,
      "loss": 1.9579,
      "step": 774
    },
    {
      "epoch": 1.3765541740674956,
      "grad_norm": 0.5758579969406128,
      "learning_rate": 0.00016517828527121942,
      "loss": 1.9215,
      "step": 775
    },
    {
      "epoch": 1.3783303730017762,
      "grad_norm": 0.5230008959770203,
      "learning_rate": 0.00016509345446267938,
      "loss": 2.0963,
      "step": 776
    },
    {
      "epoch": 1.3801065719360568,
      "grad_norm": 0.5634267330169678,
      "learning_rate": 0.00016500854229162143,
      "loss": 1.9633,
      "step": 777
    },
    {
      "epoch": 1.3818827708703374,
      "grad_norm": 0.5762854218482971,
      "learning_rate": 0.00016492354886418019,
      "loss": 1.6588,
      "step": 778
    },
    {
      "epoch": 1.383658969804618,
      "grad_norm": 0.5798690319061279,
      "learning_rate": 0.0001648384742865918,
      "loss": 1.6035,
      "step": 779
    },
    {
      "epoch": 1.3854351687388988,
      "grad_norm": 0.5968732237815857,
      "learning_rate": 0.00016475331866519386,
      "loss": 2.1986,
      "step": 780
    },
    {
      "epoch": 1.3872113676731794,
      "grad_norm": 0.5378758907318115,
      "learning_rate": 0.00016466808210642526,
      "loss": 1.831,
      "step": 781
    },
    {
      "epoch": 1.38898756660746,
      "grad_norm": 0.5597891211509705,
      "learning_rate": 0.0001645827647168261,
      "loss": 1.877,
      "step": 782
    },
    {
      "epoch": 1.3907637655417406,
      "grad_norm": 0.5437365770339966,
      "learning_rate": 0.00016449736660303747,
      "loss": 2.2005,
      "step": 783
    },
    {
      "epoch": 1.3925399644760212,
      "grad_norm": 0.6209936738014221,
      "learning_rate": 0.00016441188787180138,
      "loss": 1.7576,
      "step": 784
    },
    {
      "epoch": 1.394316163410302,
      "grad_norm": 0.5528843402862549,
      "learning_rate": 0.00016432632862996055,
      "loss": 1.9704,
      "step": 785
    },
    {
      "epoch": 1.3960923623445827,
      "grad_norm": 0.5896710753440857,
      "learning_rate": 0.00016424068898445845,
      "loss": 1.9691,
      "step": 786
    },
    {
      "epoch": 1.3978685612788633,
      "grad_norm": 0.5352449417114258,
      "learning_rate": 0.00016415496904233895,
      "loss": 1.8302,
      "step": 787
    },
    {
      "epoch": 1.3996447602131439,
      "grad_norm": 0.5687698721885681,
      "learning_rate": 0.0001640691689107463,
      "loss": 2.2893,
      "step": 788
    },
    {
      "epoch": 1.4014209591474245,
      "grad_norm": 0.6054495573043823,
      "learning_rate": 0.000163983288696925,
      "loss": 1.8518,
      "step": 789
    },
    {
      "epoch": 1.403197158081705,
      "grad_norm": 0.575993001461029,
      "learning_rate": 0.00016389732850821966,
      "loss": 1.7457,
      "step": 790
    },
    {
      "epoch": 1.4049733570159857,
      "grad_norm": 0.674614429473877,
      "learning_rate": 0.00016381128845207481,
      "loss": 1.8681,
      "step": 791
    },
    {
      "epoch": 1.4067495559502663,
      "grad_norm": 0.5626409649848938,
      "learning_rate": 0.00016372516863603486,
      "loss": 1.8548,
      "step": 792
    },
    {
      "epoch": 1.4085257548845471,
      "grad_norm": 0.5684689283370972,
      "learning_rate": 0.00016363896916774388,
      "loss": 1.5165,
      "step": 793
    },
    {
      "epoch": 1.4103019538188277,
      "grad_norm": 0.5735300183296204,
      "learning_rate": 0.00016355269015494553,
      "loss": 1.5926,
      "step": 794
    },
    {
      "epoch": 1.4120781527531083,
      "grad_norm": 0.5922591686248779,
      "learning_rate": 0.0001634663317054829,
      "loss": 1.9652,
      "step": 795
    },
    {
      "epoch": 1.413854351687389,
      "grad_norm": 0.5671470761299133,
      "learning_rate": 0.00016337989392729821,
      "loss": 2.2372,
      "step": 796
    },
    {
      "epoch": 1.4156305506216698,
      "grad_norm": 0.5707034468650818,
      "learning_rate": 0.00016329337692843314,
      "loss": 1.9812,
      "step": 797
    },
    {
      "epoch": 1.4174067495559504,
      "grad_norm": 0.6949366331100464,
      "learning_rate": 0.00016320678081702816,
      "loss": 1.7535,
      "step": 798
    },
    {
      "epoch": 1.419182948490231,
      "grad_norm": 0.5790000557899475,
      "learning_rate": 0.0001631201057013227,
      "loss": 1.8964,
      "step": 799
    },
    {
      "epoch": 1.4209591474245116,
      "grad_norm": 0.6276417970657349,
      "learning_rate": 0.00016303335168965487,
      "loss": 1.5388,
      "step": 800
    },
    {
      "epoch": 1.4209591474245116,
      "eval_loss": 1.8918941020965576,
      "eval_runtime": 17.5523,
      "eval_samples_per_second": 57.03,
      "eval_steps_per_second": 28.543,
      "step": 800
    },
    {
      "epoch": 1.4227353463587922,
      "grad_norm": 0.5881146192550659,
      "learning_rate": 0.0001629465188904615,
      "loss": 1.8195,
      "step": 801
    },
    {
      "epoch": 1.4245115452930728,
      "grad_norm": 0.6196092367172241,
      "learning_rate": 0.00016285960741227784,
      "loss": 1.796,
      "step": 802
    },
    {
      "epoch": 1.4262877442273534,
      "grad_norm": 0.584575355052948,
      "learning_rate": 0.00016277261736373752,
      "loss": 1.917,
      "step": 803
    },
    {
      "epoch": 1.428063943161634,
      "grad_norm": 0.6292086243629456,
      "learning_rate": 0.00016268554885357235,
      "loss": 1.4693,
      "step": 804
    },
    {
      "epoch": 1.4298401420959148,
      "grad_norm": 0.6106860637664795,
      "learning_rate": 0.00016259840199061215,
      "loss": 1.7396,
      "step": 805
    },
    {
      "epoch": 1.4316163410301954,
      "grad_norm": 0.4887128472328186,
      "learning_rate": 0.00016251117688378486,
      "loss": 1.8714,
      "step": 806
    },
    {
      "epoch": 1.433392539964476,
      "grad_norm": 0.5990557074546814,
      "learning_rate": 0.000162423873642116,
      "loss": 1.6416,
      "step": 807
    },
    {
      "epoch": 1.4351687388987566,
      "grad_norm": 0.5942093729972839,
      "learning_rate": 0.00016233649237472895,
      "loss": 1.8325,
      "step": 808
    },
    {
      "epoch": 1.4369449378330372,
      "grad_norm": 0.6086543202400208,
      "learning_rate": 0.00016224903319084438,
      "loss": 2.2132,
      "step": 809
    },
    {
      "epoch": 1.438721136767318,
      "grad_norm": 0.543213963508606,
      "learning_rate": 0.00016216149619978064,
      "loss": 1.8908,
      "step": 810
    },
    {
      "epoch": 1.4404973357015987,
      "grad_norm": 0.5792186260223389,
      "learning_rate": 0.0001620738815109531,
      "loss": 2.1117,
      "step": 811
    },
    {
      "epoch": 1.4422735346358793,
      "grad_norm": 0.6133096814155579,
      "learning_rate": 0.00016198618923387434,
      "loss": 1.8365,
      "step": 812
    },
    {
      "epoch": 1.4440497335701599,
      "grad_norm": 0.6299993991851807,
      "learning_rate": 0.00016189841947815395,
      "loss": 1.9849,
      "step": 813
    },
    {
      "epoch": 1.4458259325044405,
      "grad_norm": 0.636391818523407,
      "learning_rate": 0.0001618105723534983,
      "loss": 2.069,
      "step": 814
    },
    {
      "epoch": 1.447602131438721,
      "grad_norm": 0.6022375822067261,
      "learning_rate": 0.0001617226479697105,
      "loss": 1.9346,
      "step": 815
    },
    {
      "epoch": 1.4493783303730017,
      "grad_norm": 0.6289360523223877,
      "learning_rate": 0.0001616346464366902,
      "loss": 1.9245,
      "step": 816
    },
    {
      "epoch": 1.4511545293072823,
      "grad_norm": 0.5746697187423706,
      "learning_rate": 0.00016154656786443355,
      "loss": 1.896,
      "step": 817
    },
    {
      "epoch": 1.452930728241563,
      "grad_norm": 0.6024563908576965,
      "learning_rate": 0.00016145841236303286,
      "loss": 1.9923,
      "step": 818
    },
    {
      "epoch": 1.4547069271758437,
      "grad_norm": 0.550466775894165,
      "learning_rate": 0.00016137018004267682,
      "loss": 2.1396,
      "step": 819
    },
    {
      "epoch": 1.4564831261101243,
      "grad_norm": 0.6055349707603455,
      "learning_rate": 0.00016128187101364987,
      "loss": 1.8426,
      "step": 820
    },
    {
      "epoch": 1.458259325044405,
      "grad_norm": 0.6064062714576721,
      "learning_rate": 0.0001611934853863325,
      "loss": 1.8068,
      "step": 821
    },
    {
      "epoch": 1.4600355239786857,
      "grad_norm": 0.6272661089897156,
      "learning_rate": 0.00016110502327120098,
      "loss": 1.8223,
      "step": 822
    },
    {
      "epoch": 1.4618117229129663,
      "grad_norm": 0.5644489526748657,
      "learning_rate": 0.00016101648477882702,
      "loss": 1.8539,
      "step": 823
    },
    {
      "epoch": 1.463587921847247,
      "grad_norm": 0.6197564005851746,
      "learning_rate": 0.00016092787001987792,
      "loss": 2.0166,
      "step": 824
    },
    {
      "epoch": 1.4653641207815276,
      "grad_norm": 0.6238218545913696,
      "learning_rate": 0.00016083917910511629,
      "loss": 1.8992,
      "step": 825
    },
    {
      "epoch": 1.4671403197158082,
      "grad_norm": 0.5813512206077576,
      "learning_rate": 0.00016075041214539984,
      "loss": 1.727,
      "step": 826
    },
    {
      "epoch": 1.4689165186500888,
      "grad_norm": 0.5961207747459412,
      "learning_rate": 0.00016066156925168153,
      "loss": 2.0073,
      "step": 827
    },
    {
      "epoch": 1.4706927175843694,
      "grad_norm": 0.5839224457740784,
      "learning_rate": 0.000160572650535009,
      "loss": 1.7999,
      "step": 828
    },
    {
      "epoch": 1.47246891651865,
      "grad_norm": 0.6484678387641907,
      "learning_rate": 0.00016048365610652482,
      "loss": 1.9262,
      "step": 829
    },
    {
      "epoch": 1.4742451154529308,
      "grad_norm": 0.548755943775177,
      "learning_rate": 0.00016039458607746614,
      "loss": 1.6367,
      "step": 830
    },
    {
      "epoch": 1.4760213143872114,
      "grad_norm": 0.5883561372756958,
      "learning_rate": 0.0001603054405591646,
      "loss": 1.8918,
      "step": 831
    },
    {
      "epoch": 1.477797513321492,
      "grad_norm": 0.6327068209648132,
      "learning_rate": 0.00016021621966304624,
      "loss": 1.9331,
      "step": 832
    },
    {
      "epoch": 1.4795737122557726,
      "grad_norm": 0.6582532525062561,
      "learning_rate": 0.00016012692350063126,
      "loss": 1.6824,
      "step": 833
    },
    {
      "epoch": 1.4813499111900532,
      "grad_norm": 0.5976439714431763,
      "learning_rate": 0.00016003755218353395,
      "loss": 1.7983,
      "step": 834
    },
    {
      "epoch": 1.483126110124334,
      "grad_norm": 0.6134210824966431,
      "learning_rate": 0.0001599481058234626,
      "loss": 2.2098,
      "step": 835
    },
    {
      "epoch": 1.4849023090586146,
      "grad_norm": 0.6046844124794006,
      "learning_rate": 0.0001598585845322192,
      "loss": 1.9658,
      "step": 836
    },
    {
      "epoch": 1.4866785079928952,
      "grad_norm": 0.602982223033905,
      "learning_rate": 0.0001597689884216995,
      "loss": 1.7649,
      "step": 837
    },
    {
      "epoch": 1.4884547069271759,
      "grad_norm": 0.605516791343689,
      "learning_rate": 0.00015967931760389265,
      "loss": 2.0109,
      "step": 838
    },
    {
      "epoch": 1.4902309058614565,
      "grad_norm": 0.6028212308883667,
      "learning_rate": 0.00015958957219088134,
      "loss": 1.726,
      "step": 839
    },
    {
      "epoch": 1.492007104795737,
      "grad_norm": 0.5539565682411194,
      "learning_rate": 0.00015949975229484134,
      "loss": 1.5349,
      "step": 840
    },
    {
      "epoch": 1.4937833037300177,
      "grad_norm": 0.6573892831802368,
      "learning_rate": 0.00015940985802804155,
      "loss": 1.7337,
      "step": 841
    },
    {
      "epoch": 1.4955595026642983,
      "grad_norm": 0.6214087605476379,
      "learning_rate": 0.000159319889502844,
      "loss": 1.8312,
      "step": 842
    },
    {
      "epoch": 1.497335701598579,
      "grad_norm": 0.616106390953064,
      "learning_rate": 0.00015922984683170327,
      "loss": 1.5248,
      "step": 843
    },
    {
      "epoch": 1.4991119005328597,
      "grad_norm": 0.5459708571434021,
      "learning_rate": 0.00015913973012716682,
      "loss": 2.0266,
      "step": 844
    },
    {
      "epoch": 1.5008880994671403,
      "grad_norm": 0.5821857452392578,
      "learning_rate": 0.00015904953950187457,
      "loss": 1.9524,
      "step": 845
    },
    {
      "epoch": 1.502664298401421,
      "grad_norm": 0.5687321424484253,
      "learning_rate": 0.00015895927506855883,
      "loss": 1.999,
      "step": 846
    },
    {
      "epoch": 1.5044404973357017,
      "grad_norm": 0.606232762336731,
      "learning_rate": 0.00015886893694004418,
      "loss": 2.1248,
      "step": 847
    },
    {
      "epoch": 1.5062166962699823,
      "grad_norm": 0.6832650899887085,
      "learning_rate": 0.00015877852522924732,
      "loss": 1.9353,
      "step": 848
    },
    {
      "epoch": 1.507992895204263,
      "grad_norm": 0.5841145515441895,
      "learning_rate": 0.0001586880400491769,
      "loss": 1.8427,
      "step": 849
    },
    {
      "epoch": 1.5097690941385435,
      "grad_norm": 0.5645660758018494,
      "learning_rate": 0.00015859748151293346,
      "loss": 2.2209,
      "step": 850
    },
    {
      "epoch": 1.5115452930728241,
      "grad_norm": 0.5970801711082458,
      "learning_rate": 0.00015850684973370914,
      "loss": 2.0469,
      "step": 851
    },
    {
      "epoch": 1.5133214920071048,
      "grad_norm": 0.6085620522499084,
      "learning_rate": 0.00015841614482478767,
      "loss": 1.9884,
      "step": 852
    },
    {
      "epoch": 1.5150976909413854,
      "grad_norm": 0.6346157193183899,
      "learning_rate": 0.00015832536689954423,
      "loss": 1.8245,
      "step": 853
    },
    {
      "epoch": 1.516873889875666,
      "grad_norm": 0.5912746787071228,
      "learning_rate": 0.00015823451607144522,
      "loss": 1.972,
      "step": 854
    },
    {
      "epoch": 1.5186500888099466,
      "grad_norm": 0.5920143723487854,
      "learning_rate": 0.00015814359245404816,
      "loss": 1.7179,
      "step": 855
    },
    {
      "epoch": 1.5204262877442274,
      "grad_norm": 0.5774127244949341,
      "learning_rate": 0.00015805259616100164,
      "loss": 1.797,
      "step": 856
    },
    {
      "epoch": 1.522202486678508,
      "grad_norm": 0.6066740155220032,
      "learning_rate": 0.00015796152730604489,
      "loss": 1.9299,
      "step": 857
    },
    {
      "epoch": 1.5239786856127886,
      "grad_norm": 0.5766725540161133,
      "learning_rate": 0.00015787038600300802,
      "loss": 1.8549,
      "step": 858
    },
    {
      "epoch": 1.5257548845470694,
      "grad_norm": 0.5519466400146484,
      "learning_rate": 0.00015777917236581165,
      "loss": 2.0266,
      "step": 859
    },
    {
      "epoch": 1.52753108348135,
      "grad_norm": 0.5982426404953003,
      "learning_rate": 0.0001576878865084668,
      "loss": 1.6434,
      "step": 860
    },
    {
      "epoch": 1.5293072824156306,
      "grad_norm": 0.5609515905380249,
      "learning_rate": 0.00015759652854507476,
      "loss": 1.7575,
      "step": 861
    },
    {
      "epoch": 1.5310834813499112,
      "grad_norm": 0.5498602390289307,
      "learning_rate": 0.00015750509858982693,
      "loss": 1.6832,
      "step": 862
    },
    {
      "epoch": 1.5328596802841918,
      "grad_norm": 0.5763415098190308,
      "learning_rate": 0.00015741359675700472,
      "loss": 1.9615,
      "step": 863
    },
    {
      "epoch": 1.5346358792184724,
      "grad_norm": 0.5893312096595764,
      "learning_rate": 0.00015732202316097942,
      "loss": 1.5865,
      "step": 864
    },
    {
      "epoch": 1.536412078152753,
      "grad_norm": 0.5605270862579346,
      "learning_rate": 0.00015723037791621193,
      "loss": 2.1259,
      "step": 865
    },
    {
      "epoch": 1.5381882770870337,
      "grad_norm": 0.5532479882240295,
      "learning_rate": 0.00015713866113725272,
      "loss": 1.4857,
      "step": 866
    },
    {
      "epoch": 1.5399644760213143,
      "grad_norm": 0.5923479199409485,
      "learning_rate": 0.00015704687293874176,
      "loss": 1.8116,
      "step": 867
    },
    {
      "epoch": 1.541740674955595,
      "grad_norm": 0.6499646306037903,
      "learning_rate": 0.0001569550134354082,
      "loss": 1.9368,
      "step": 868
    },
    {
      "epoch": 1.5435168738898757,
      "grad_norm": 0.5686277747154236,
      "learning_rate": 0.00015686308274207042,
      "loss": 1.8631,
      "step": 869
    },
    {
      "epoch": 1.5452930728241563,
      "grad_norm": 0.5920370817184448,
      "learning_rate": 0.00015677108097363564,
      "loss": 1.9485,
      "step": 870
    },
    {
      "epoch": 1.547069271758437,
      "grad_norm": 0.623026967048645,
      "learning_rate": 0.00015667900824510003,
      "loss": 2.0042,
      "step": 871
    },
    {
      "epoch": 1.5488454706927177,
      "grad_norm": 0.6088441610336304,
      "learning_rate": 0.0001565868646715484,
      "loss": 1.92,
      "step": 872
    },
    {
      "epoch": 1.5506216696269983,
      "grad_norm": 0.60791015625,
      "learning_rate": 0.0001564946503681541,
      "loss": 1.9655,
      "step": 873
    },
    {
      "epoch": 1.552397868561279,
      "grad_norm": 0.5662127137184143,
      "learning_rate": 0.000156402365450179,
      "loss": 1.9175,
      "step": 874
    },
    {
      "epoch": 1.5541740674955595,
      "grad_norm": 0.690692126750946,
      "learning_rate": 0.00015631001003297307,
      "loss": 1.6981,
      "step": 875
    },
    {
      "epoch": 1.5559502664298401,
      "grad_norm": 0.5585904717445374,
      "learning_rate": 0.0001562175842319745,
      "loss": 1.922,
      "step": 876
    },
    {
      "epoch": 1.5577264653641207,
      "grad_norm": 0.5841543078422546,
      "learning_rate": 0.0001561250881627095,
      "loss": 1.9446,
      "step": 877
    },
    {
      "epoch": 1.5595026642984013,
      "grad_norm": 0.5972234010696411,
      "learning_rate": 0.00015603252194079198,
      "loss": 1.9554,
      "step": 878
    },
    {
      "epoch": 1.561278863232682,
      "grad_norm": 0.5781091451644897,
      "learning_rate": 0.0001559398856819236,
      "loss": 1.8569,
      "step": 879
    },
    {
      "epoch": 1.5630550621669625,
      "grad_norm": 0.545781135559082,
      "learning_rate": 0.00015584717950189357,
      "loss": 1.8867,
      "step": 880
    },
    {
      "epoch": 1.5648312611012434,
      "grad_norm": 0.5672938823699951,
      "learning_rate": 0.00015575440351657852,
      "loss": 2.0413,
      "step": 881
    },
    {
      "epoch": 1.566607460035524,
      "grad_norm": 0.6107877492904663,
      "learning_rate": 0.0001556615578419423,
      "loss": 1.7856,
      "step": 882
    },
    {
      "epoch": 1.5683836589698046,
      "grad_norm": 0.6958935856819153,
      "learning_rate": 0.00015556864259403584,
      "loss": 2.0391,
      "step": 883
    },
    {
      "epoch": 1.5701598579040854,
      "grad_norm": 0.6300920248031616,
      "learning_rate": 0.00015547565788899708,
      "loss": 1.8645,
      "step": 884
    },
    {
      "epoch": 1.571936056838366,
      "grad_norm": 0.5741441249847412,
      "learning_rate": 0.00015538260384305075,
      "loss": 1.8297,
      "step": 885
    },
    {
      "epoch": 1.5737122557726466,
      "grad_norm": 0.6208085417747498,
      "learning_rate": 0.00015528948057250824,
      "loss": 2.0644,
      "step": 886
    },
    {
      "epoch": 1.5754884547069272,
      "grad_norm": 0.577325165271759,
      "learning_rate": 0.00015519628819376754,
      "loss": 1.7559,
      "step": 887
    },
    {
      "epoch": 1.5772646536412078,
      "grad_norm": 0.600643515586853,
      "learning_rate": 0.00015510302682331293,
      "loss": 1.6666,
      "step": 888
    },
    {
      "epoch": 1.5790408525754884,
      "grad_norm": 0.5971135497093201,
      "learning_rate": 0.000155009696577715,
      "loss": 1.565,
      "step": 889
    },
    {
      "epoch": 1.580817051509769,
      "grad_norm": 0.5686447024345398,
      "learning_rate": 0.00015491629757363032,
      "loss": 2.0014,
      "step": 890
    },
    {
      "epoch": 1.5825932504440496,
      "grad_norm": 0.6513876914978027,
      "learning_rate": 0.00015482282992780155,
      "loss": 1.8678,
      "step": 891
    },
    {
      "epoch": 1.5843694493783302,
      "grad_norm": 0.6242618560791016,
      "learning_rate": 0.00015472929375705703,
      "loss": 2.0024,
      "step": 892
    },
    {
      "epoch": 1.586145648312611,
      "grad_norm": 0.6046873927116394,
      "learning_rate": 0.00015463568917831083,
      "loss": 1.8123,
      "step": 893
    },
    {
      "epoch": 1.5879218472468917,
      "grad_norm": 0.616163969039917,
      "learning_rate": 0.0001545420163085624,
      "loss": 1.8212,
      "step": 894
    },
    {
      "epoch": 1.5896980461811723,
      "grad_norm": 0.5721304416656494,
      "learning_rate": 0.00015444827526489677,
      "loss": 1.9365,
      "step": 895
    },
    {
      "epoch": 1.5914742451154529,
      "grad_norm": 0.5937680006027222,
      "learning_rate": 0.00015435446616448393,
      "loss": 1.9086,
      "step": 896
    },
    {
      "epoch": 1.5932504440497337,
      "grad_norm": 0.6117508411407471,
      "learning_rate": 0.0001542605891245791,
      "loss": 1.8982,
      "step": 897
    },
    {
      "epoch": 1.5950266429840143,
      "grad_norm": 0.5448206067085266,
      "learning_rate": 0.00015416664426252247,
      "loss": 1.5134,
      "step": 898
    },
    {
      "epoch": 1.596802841918295,
      "grad_norm": 0.6197383403778076,
      "learning_rate": 0.00015407263169573877,
      "loss": 1.7243,
      "step": 899
    },
    {
      "epoch": 1.5985790408525755,
      "grad_norm": 0.5766022205352783,
      "learning_rate": 0.00015397855154173758,
      "loss": 1.6751,
      "step": 900
    },
    {
      "epoch": 1.5985790408525755,
      "eval_loss": 1.8915456533432007,
      "eval_runtime": 17.5455,
      "eval_samples_per_second": 57.052,
      "eval_steps_per_second": 28.554,
      "step": 900
    },
    {
      "epoch": 1.6003552397868561,
      "grad_norm": 0.6614560484886169,
      "learning_rate": 0.00015388440391811287,
      "loss": 1.8998,
      "step": 901
    },
    {
      "epoch": 1.6021314387211367,
      "grad_norm": 0.5949293971061707,
      "learning_rate": 0.0001537901889425429,
      "loss": 1.7872,
      "step": 902
    },
    {
      "epoch": 1.6039076376554173,
      "grad_norm": 0.6061775088310242,
      "learning_rate": 0.00015369590673279025,
      "loss": 2.0065,
      "step": 903
    },
    {
      "epoch": 1.605683836589698,
      "grad_norm": 0.6063766479492188,
      "learning_rate": 0.0001536015574067014,
      "loss": 1.6268,
      "step": 904
    },
    {
      "epoch": 1.6074600355239785,
      "grad_norm": 0.6193698048591614,
      "learning_rate": 0.00015350714108220673,
      "loss": 1.5191,
      "step": 905
    },
    {
      "epoch": 1.6092362344582594,
      "grad_norm": 0.6285121440887451,
      "learning_rate": 0.00015341265787732053,
      "loss": 1.7842,
      "step": 906
    },
    {
      "epoch": 1.61101243339254,
      "grad_norm": 0.6320367455482483,
      "learning_rate": 0.00015331810791014046,
      "loss": 2.0282,
      "step": 907
    },
    {
      "epoch": 1.6127886323268206,
      "grad_norm": 0.5884556770324707,
      "learning_rate": 0.00015322349129884776,
      "loss": 1.6152,
      "step": 908
    },
    {
      "epoch": 1.6145648312611014,
      "grad_norm": 0.5928732752799988,
      "learning_rate": 0.00015312880816170695,
      "loss": 1.9621,
      "step": 909
    },
    {
      "epoch": 1.616341030195382,
      "grad_norm": 0.5892992615699768,
      "learning_rate": 0.00015303405861706574,
      "loss": 1.9998,
      "step": 910
    },
    {
      "epoch": 1.6181172291296626,
      "grad_norm": 0.5816720724105835,
      "learning_rate": 0.00015293924278335475,
      "loss": 1.9498,
      "step": 911
    },
    {
      "epoch": 1.6198934280639432,
      "grad_norm": 0.6086511015892029,
      "learning_rate": 0.00015284436077908747,
      "loss": 1.6833,
      "step": 912
    },
    {
      "epoch": 1.6216696269982238,
      "grad_norm": 0.6222444772720337,
      "learning_rate": 0.00015274941272286024,
      "loss": 1.8225,
      "step": 913
    },
    {
      "epoch": 1.6234458259325044,
      "grad_norm": 0.6078671216964722,
      "learning_rate": 0.00015265439873335176,
      "loss": 1.4886,
      "step": 914
    },
    {
      "epoch": 1.625222024866785,
      "grad_norm": 0.5981762409210205,
      "learning_rate": 0.00015255931892932333,
      "loss": 1.7602,
      "step": 915
    },
    {
      "epoch": 1.6269982238010656,
      "grad_norm": 0.5541541576385498,
      "learning_rate": 0.00015246417342961835,
      "loss": 1.8843,
      "step": 916
    },
    {
      "epoch": 1.6287744227353462,
      "grad_norm": 0.670077383518219,
      "learning_rate": 0.0001523689623531624,
      "loss": 1.6899,
      "step": 917
    },
    {
      "epoch": 1.6305506216696268,
      "grad_norm": 0.6286355257034302,
      "learning_rate": 0.00015227368581896314,
      "loss": 1.7776,
      "step": 918
    },
    {
      "epoch": 1.6323268206039077,
      "grad_norm": 0.5522047281265259,
      "learning_rate": 0.00015217834394610984,
      "loss": 1.3546,
      "step": 919
    },
    {
      "epoch": 1.6341030195381883,
      "grad_norm": 0.5497022271156311,
      "learning_rate": 0.00015208293685377357,
      "loss": 1.7369,
      "step": 920
    },
    {
      "epoch": 1.6358792184724689,
      "grad_norm": 0.6160714626312256,
      "learning_rate": 0.0001519874646612069,
      "loss": 1.8082,
      "step": 921
    },
    {
      "epoch": 1.6376554174067497,
      "grad_norm": 0.5778859257698059,
      "learning_rate": 0.0001518919274877438,
      "loss": 1.8691,
      "step": 922
    },
    {
      "epoch": 1.6394316163410303,
      "grad_norm": 0.6012445092201233,
      "learning_rate": 0.0001517963254527994,
      "loss": 2.0584,
      "step": 923
    },
    {
      "epoch": 1.641207815275311,
      "grad_norm": 0.6099020838737488,
      "learning_rate": 0.00015170065867586987,
      "loss": 1.8027,
      "step": 924
    },
    {
      "epoch": 1.6429840142095915,
      "grad_norm": 0.5920459628105164,
      "learning_rate": 0.00015160492727653242,
      "loss": 1.7474,
      "step": 925
    },
    {
      "epoch": 1.644760213143872,
      "grad_norm": 0.5783458948135376,
      "learning_rate": 0.000151509131374445,
      "loss": 1.6262,
      "step": 926
    },
    {
      "epoch": 1.6465364120781527,
      "grad_norm": 0.6065337061882019,
      "learning_rate": 0.00015141327108934608,
      "loss": 1.6377,
      "step": 927
    },
    {
      "epoch": 1.6483126110124333,
      "grad_norm": 0.582298994064331,
      "learning_rate": 0.00015131734654105476,
      "loss": 1.5462,
      "step": 928
    },
    {
      "epoch": 1.650088809946714,
      "grad_norm": 0.580778181552887,
      "learning_rate": 0.00015122135784947035,
      "loss": 1.6615,
      "step": 929
    },
    {
      "epoch": 1.6518650088809945,
      "grad_norm": 0.63937908411026,
      "learning_rate": 0.00015112530513457234,
      "loss": 1.8125,
      "step": 930
    },
    {
      "epoch": 1.6536412078152753,
      "grad_norm": 0.5972400307655334,
      "learning_rate": 0.00015102918851642034,
      "loss": 2.0891,
      "step": 931
    },
    {
      "epoch": 1.655417406749556,
      "grad_norm": 0.6231987476348877,
      "learning_rate": 0.00015093300811515377,
      "loss": 1.8175,
      "step": 932
    },
    {
      "epoch": 1.6571936056838366,
      "grad_norm": 0.6009554862976074,
      "learning_rate": 0.00015083676405099178,
      "loss": 1.9073,
      "step": 933
    },
    {
      "epoch": 1.6589698046181174,
      "grad_norm": 0.5681602358818054,
      "learning_rate": 0.000150740456444233,
      "loss": 1.6192,
      "step": 934
    },
    {
      "epoch": 1.660746003552398,
      "grad_norm": 0.6406825184822083,
      "learning_rate": 0.00015064408541525572,
      "loss": 1.9155,
      "step": 935
    },
    {
      "epoch": 1.6625222024866786,
      "grad_norm": 0.5754558444023132,
      "learning_rate": 0.00015054765108451723,
      "loss": 1.8701,
      "step": 936
    },
    {
      "epoch": 1.6642984014209592,
      "grad_norm": 0.571086049079895,
      "learning_rate": 0.00015045115357255418,
      "loss": 1.8877,
      "step": 937
    },
    {
      "epoch": 1.6660746003552398,
      "grad_norm": 0.6124176383018494,
      "learning_rate": 0.000150354592999982,
      "loss": 1.9684,
      "step": 938
    },
    {
      "epoch": 1.6678507992895204,
      "grad_norm": 0.5903929471969604,
      "learning_rate": 0.00015025796948749505,
      "loss": 1.6661,
      "step": 939
    },
    {
      "epoch": 1.669626998223801,
      "grad_norm": 0.641107439994812,
      "learning_rate": 0.0001501612831558664,
      "loss": 1.8729,
      "step": 940
    },
    {
      "epoch": 1.6714031971580816,
      "grad_norm": 0.592301070690155,
      "learning_rate": 0.0001500645341259475,
      "loss": 1.5741,
      "step": 941
    },
    {
      "epoch": 1.6731793960923622,
      "grad_norm": 0.6322725415229797,
      "learning_rate": 0.00014996772251866825,
      "loss": 1.9312,
      "step": 942
    },
    {
      "epoch": 1.6749555950266428,
      "grad_norm": 0.6590679883956909,
      "learning_rate": 0.00014987084845503683,
      "loss": 1.6868,
      "step": 943
    },
    {
      "epoch": 1.6767317939609236,
      "grad_norm": 0.6164213418960571,
      "learning_rate": 0.00014977391205613934,
      "loss": 1.9881,
      "step": 944
    },
    {
      "epoch": 1.6785079928952042,
      "grad_norm": 0.5949009656906128,
      "learning_rate": 0.00014967691344313996,
      "loss": 2.1492,
      "step": 945
    },
    {
      "epoch": 1.6802841918294849,
      "grad_norm": 0.5915289521217346,
      "learning_rate": 0.00014957985273728044,
      "loss": 1.9454,
      "step": 946
    },
    {
      "epoch": 1.6820603907637657,
      "grad_norm": 0.6073183417320251,
      "learning_rate": 0.00014948273005988032,
      "loss": 1.942,
      "step": 947
    },
    {
      "epoch": 1.6838365896980463,
      "grad_norm": 0.5700584053993225,
      "learning_rate": 0.00014938554553233652,
      "loss": 1.9137,
      "step": 948
    },
    {
      "epoch": 1.6856127886323269,
      "grad_norm": 0.608840823173523,
      "learning_rate": 0.0001492882992761233,
      "loss": 1.9929,
      "step": 949
    },
    {
      "epoch": 1.6873889875666075,
      "grad_norm": 0.5968925952911377,
      "learning_rate": 0.00014919099141279203,
      "loss": 1.962,
      "step": 950
    },
    {
      "epoch": 1.689165186500888,
      "grad_norm": 0.5561500191688538,
      "learning_rate": 0.00014909362206397113,
      "loss": 1.8595,
      "step": 951
    },
    {
      "epoch": 1.6909413854351687,
      "grad_norm": 0.6271839141845703,
      "learning_rate": 0.00014899619135136583,
      "loss": 1.8679,
      "step": 952
    },
    {
      "epoch": 1.6927175843694493,
      "grad_norm": 0.6014618277549744,
      "learning_rate": 0.0001488986993967581,
      "loss": 1.8732,
      "step": 953
    },
    {
      "epoch": 1.69449378330373,
      "grad_norm": 0.6322990655899048,
      "learning_rate": 0.00014880114632200643,
      "loss": 1.7905,
      "step": 954
    },
    {
      "epoch": 1.6962699822380105,
      "grad_norm": 0.6107459664344788,
      "learning_rate": 0.00014870353224904573,
      "loss": 1.9705,
      "step": 955
    },
    {
      "epoch": 1.6980461811722913,
      "grad_norm": 0.6043215394020081,
      "learning_rate": 0.00014860585729988712,
      "loss": 1.5994,
      "step": 956
    },
    {
      "epoch": 1.699822380106572,
      "grad_norm": 0.5697582364082336,
      "learning_rate": 0.00014850812159661786,
      "loss": 1.8344,
      "step": 957
    },
    {
      "epoch": 1.7015985790408525,
      "grad_norm": 0.6905009746551514,
      "learning_rate": 0.00014841032526140112,
      "loss": 1.7942,
      "step": 958
    },
    {
      "epoch": 1.7033747779751334,
      "grad_norm": 0.5880087614059448,
      "learning_rate": 0.00014831246841647585,
      "loss": 1.9501,
      "step": 959
    },
    {
      "epoch": 1.705150976909414,
      "grad_norm": 0.6571997404098511,
      "learning_rate": 0.00014821455118415667,
      "loss": 1.7883,
      "step": 960
    },
    {
      "epoch": 1.7069271758436946,
      "grad_norm": 0.5602095127105713,
      "learning_rate": 0.00014811657368683364,
      "loss": 1.7951,
      "step": 961
    },
    {
      "epoch": 1.7087033747779752,
      "grad_norm": 0.5878534317016602,
      "learning_rate": 0.0001480185360469721,
      "loss": 1.6267,
      "step": 962
    },
    {
      "epoch": 1.7104795737122558,
      "grad_norm": 0.5539603233337402,
      "learning_rate": 0.00014792043838711267,
      "loss": 1.7441,
      "step": 963
    },
    {
      "epoch": 1.7122557726465364,
      "grad_norm": 0.6059386134147644,
      "learning_rate": 0.00014782228082987098,
      "loss": 1.8214,
      "step": 964
    },
    {
      "epoch": 1.714031971580817,
      "grad_norm": 0.6452008485794067,
      "learning_rate": 0.00014772406349793744,
      "loss": 2.0814,
      "step": 965
    },
    {
      "epoch": 1.7158081705150976,
      "grad_norm": 0.5751988887786865,
      "learning_rate": 0.00014762578651407725,
      "loss": 1.8385,
      "step": 966
    },
    {
      "epoch": 1.7175843694493782,
      "grad_norm": 0.6812849640846252,
      "learning_rate": 0.00014752745000113013,
      "loss": 2.0168,
      "step": 967
    },
    {
      "epoch": 1.7193605683836588,
      "grad_norm": 0.5656623244285583,
      "learning_rate": 0.00014742905408201022,
      "loss": 1.9843,
      "step": 968
    },
    {
      "epoch": 1.7211367673179396,
      "grad_norm": 0.6325672268867493,
      "learning_rate": 0.00014733059887970596,
      "loss": 1.9563,
      "step": 969
    },
    {
      "epoch": 1.7229129662522202,
      "grad_norm": 0.6405397653579712,
      "learning_rate": 0.00014723208451727983,
      "loss": 1.8898,
      "step": 970
    },
    {
      "epoch": 1.7246891651865008,
      "grad_norm": 0.6451442241668701,
      "learning_rate": 0.00014713351111786825,
      "loss": 1.7929,
      "step": 971
    },
    {
      "epoch": 1.7264653641207817,
      "grad_norm": 0.6197517514228821,
      "learning_rate": 0.00014703487880468147,
      "loss": 1.8935,
      "step": 972
    },
    {
      "epoch": 1.7282415630550623,
      "grad_norm": 0.660396933555603,
      "learning_rate": 0.00014693618770100339,
      "loss": 1.8245,
      "step": 973
    },
    {
      "epoch": 1.7300177619893429,
      "grad_norm": 0.606372058391571,
      "learning_rate": 0.00014683743793019133,
      "loss": 1.901,
      "step": 974
    },
    {
      "epoch": 1.7317939609236235,
      "grad_norm": 0.6290836334228516,
      "learning_rate": 0.00014673862961567602,
      "loss": 1.8943,
      "step": 975
    },
    {
      "epoch": 1.733570159857904,
      "grad_norm": 0.597625195980072,
      "learning_rate": 0.00014663976288096131,
      "loss": 1.8687,
      "step": 976
    },
    {
      "epoch": 1.7353463587921847,
      "grad_norm": 0.603987991809845,
      "learning_rate": 0.00014654083784962406,
      "loss": 2.0977,
      "step": 977
    },
    {
      "epoch": 1.7371225577264653,
      "grad_norm": 0.5582409501075745,
      "learning_rate": 0.00014644185464531408,
      "loss": 1.7397,
      "step": 978
    },
    {
      "epoch": 1.738898756660746,
      "grad_norm": 0.604293942451477,
      "learning_rate": 0.0001463428133917538,
      "loss": 1.8035,
      "step": 979
    },
    {
      "epoch": 1.7406749555950265,
      "grad_norm": 0.5962516069412231,
      "learning_rate": 0.00014624371421273822,
      "loss": 1.8757,
      "step": 980
    },
    {
      "epoch": 1.7424511545293073,
      "grad_norm": 0.5834837555885315,
      "learning_rate": 0.0001461445572321348,
      "loss": 2.0608,
      "step": 981
    },
    {
      "epoch": 1.744227353463588,
      "grad_norm": 0.5867877006530762,
      "learning_rate": 0.00014604534257388324,
      "loss": 1.4758,
      "step": 982
    },
    {
      "epoch": 1.7460035523978685,
      "grad_norm": 0.6065515279769897,
      "learning_rate": 0.00014594607036199524,
      "loss": 1.8267,
      "step": 983
    },
    {
      "epoch": 1.7477797513321494,
      "grad_norm": 0.6027518510818481,
      "learning_rate": 0.00014584674072055457,
      "loss": 1.7938,
      "step": 984
    },
    {
      "epoch": 1.74955595026643,
      "grad_norm": 0.563589870929718,
      "learning_rate": 0.0001457473537737167,
      "loss": 2.0535,
      "step": 985
    },
    {
      "epoch": 1.7513321492007106,
      "grad_norm": 0.6111871004104614,
      "learning_rate": 0.00014564790964570874,
      "loss": 1.9552,
      "step": 986
    },
    {
      "epoch": 1.7531083481349912,
      "grad_norm": 0.5682632923126221,
      "learning_rate": 0.00014554840846082932,
      "loss": 2.2138,
      "step": 987
    },
    {
      "epoch": 1.7548845470692718,
      "grad_norm": 0.5960236191749573,
      "learning_rate": 0.00014544885034344825,
      "loss": 2.0065,
      "step": 988
    },
    {
      "epoch": 1.7566607460035524,
      "grad_norm": 0.582837700843811,
      "learning_rate": 0.0001453492354180067,
      "loss": 1.8247,
      "step": 989
    },
    {
      "epoch": 1.758436944937833,
      "grad_norm": 0.6251599192619324,
      "learning_rate": 0.0001452495638090167,
      "loss": 2.0568,
      "step": 990
    },
    {
      "epoch": 1.7602131438721136,
      "grad_norm": 0.5865461230278015,
      "learning_rate": 0.00014514983564106116,
      "loss": 2.1419,
      "step": 991
    },
    {
      "epoch": 1.7619893428063942,
      "grad_norm": 0.6522809267044067,
      "learning_rate": 0.00014505005103879377,
      "loss": 2.1461,
      "step": 992
    },
    {
      "epoch": 1.7637655417406748,
      "grad_norm": 0.5658120512962341,
      "learning_rate": 0.00014495021012693864,
      "loss": 1.6075,
      "step": 993
    },
    {
      "epoch": 1.7655417406749556,
      "grad_norm": 0.6404721140861511,
      "learning_rate": 0.0001448503130302903,
      "loss": 1.6805,
      "step": 994
    },
    {
      "epoch": 1.7673179396092362,
      "grad_norm": 0.5948987603187561,
      "learning_rate": 0.00014475035987371356,
      "loss": 1.944,
      "step": 995
    },
    {
      "epoch": 1.7690941385435168,
      "grad_norm": 0.5645561218261719,
      "learning_rate": 0.0001446503507821432,
      "loss": 1.8587,
      "step": 996
    },
    {
      "epoch": 1.7708703374777977,
      "grad_norm": 0.5787933468818665,
      "learning_rate": 0.00014455028588058412,
      "loss": 1.813,
      "step": 997
    },
    {
      "epoch": 1.7726465364120783,
      "grad_norm": 0.6523125171661377,
      "learning_rate": 0.0001444501652941107,
      "loss": 1.6031,
      "step": 998
    },
    {
      "epoch": 1.7744227353463589,
      "grad_norm": 0.5875586867332458,
      "learning_rate": 0.00014434998914786707,
      "loss": 1.9484,
      "step": 999
    },
    {
      "epoch": 1.7761989342806395,
      "grad_norm": 0.6185705065727234,
      "learning_rate": 0.0001442497575670668,
      "loss": 1.8289,
      "step": 1000
    },
    {
      "epoch": 1.7761989342806395,
      "eval_loss": 1.8878443241119385,
      "eval_runtime": 17.52,
      "eval_samples_per_second": 57.135,
      "eval_steps_per_second": 28.596,
      "step": 1000
    },
    {
      "epoch": 1.77797513321492,
      "grad_norm": 0.6019285321235657,
      "learning_rate": 0.00014414947067699284,
      "loss": 1.9775,
      "step": 1001
    },
    {
      "epoch": 1.7797513321492007,
      "grad_norm": 0.570448637008667,
      "learning_rate": 0.00014404912860299706,
      "loss": 1.8857,
      "step": 1002
    },
    {
      "epoch": 1.7815275310834813,
      "grad_norm": 0.5655049681663513,
      "learning_rate": 0.0001439487314705005,
      "loss": 1.9518,
      "step": 1003
    },
    {
      "epoch": 1.7833037300177619,
      "grad_norm": 0.5784058570861816,
      "learning_rate": 0.0001438482794049929,
      "loss": 1.7965,
      "step": 1004
    },
    {
      "epoch": 1.7850799289520425,
      "grad_norm": 0.5957422852516174,
      "learning_rate": 0.00014374777253203273,
      "loss": 1.9917,
      "step": 1005
    },
    {
      "epoch": 1.7868561278863233,
      "grad_norm": 0.6360458135604858,
      "learning_rate": 0.0001436472109772469,
      "loss": 1.842,
      "step": 1006
    },
    {
      "epoch": 1.788632326820604,
      "grad_norm": 0.6583022475242615,
      "learning_rate": 0.00014354659486633077,
      "loss": 1.9972,
      "step": 1007
    },
    {
      "epoch": 1.7904085257548845,
      "grad_norm": 0.6241896748542786,
      "learning_rate": 0.0001434459243250478,
      "loss": 1.6293,
      "step": 1008
    },
    {
      "epoch": 1.7921847246891653,
      "grad_norm": 0.621865451335907,
      "learning_rate": 0.00014334519947922957,
      "loss": 1.957,
      "step": 1009
    },
    {
      "epoch": 1.793960923623446,
      "grad_norm": 0.7115092873573303,
      "learning_rate": 0.00014324442045477535,
      "loss": 1.8509,
      "step": 1010
    },
    {
      "epoch": 1.7957371225577266,
      "grad_norm": 0.6425333023071289,
      "learning_rate": 0.00014314358737765238,
      "loss": 1.8612,
      "step": 1011
    },
    {
      "epoch": 1.7975133214920072,
      "grad_norm": 0.5908367037773132,
      "learning_rate": 0.00014304270037389532,
      "loss": 1.6177,
      "step": 1012
    },
    {
      "epoch": 1.7992895204262878,
      "grad_norm": 0.5993726849555969,
      "learning_rate": 0.0001429417595696062,
      "loss": 2.0474,
      "step": 1013
    },
    {
      "epoch": 1.8010657193605684,
      "grad_norm": 0.603723406791687,
      "learning_rate": 0.0001428407650909545,
      "loss": 2.1231,
      "step": 1014
    },
    {
      "epoch": 1.802841918294849,
      "grad_norm": 0.567091703414917,
      "learning_rate": 0.00014273971706417647,
      "loss": 1.4802,
      "step": 1015
    },
    {
      "epoch": 1.8046181172291296,
      "grad_norm": 0.5592240691184998,
      "learning_rate": 0.00014263861561557558,
      "loss": 1.6959,
      "step": 1016
    },
    {
      "epoch": 1.8063943161634102,
      "grad_norm": 0.5930811762809753,
      "learning_rate": 0.00014253746087152194,
      "loss": 2.0685,
      "step": 1017
    },
    {
      "epoch": 1.8081705150976908,
      "grad_norm": 0.5956545472145081,
      "learning_rate": 0.0001424362529584523,
      "loss": 1.7144,
      "step": 1018
    },
    {
      "epoch": 1.8099467140319716,
      "grad_norm": 0.6552716493606567,
      "learning_rate": 0.00014233499200286986,
      "loss": 2.1013,
      "step": 1019
    },
    {
      "epoch": 1.8117229129662522,
      "grad_norm": 0.6194227337837219,
      "learning_rate": 0.00014223367813134412,
      "loss": 1.8448,
      "step": 1020
    },
    {
      "epoch": 1.8134991119005328,
      "grad_norm": 0.5854374766349792,
      "learning_rate": 0.00014213231147051068,
      "loss": 1.9303,
      "step": 1021
    },
    {
      "epoch": 1.8152753108348136,
      "grad_norm": 0.5748059749603271,
      "learning_rate": 0.0001420308921470713,
      "loss": 1.819,
      "step": 1022
    },
    {
      "epoch": 1.8170515097690942,
      "grad_norm": 0.5598680377006531,
      "learning_rate": 0.0001419294202877933,
      "loss": 1.2145,
      "step": 1023
    },
    {
      "epoch": 1.8188277087033748,
      "grad_norm": 0.6596031785011292,
      "learning_rate": 0.00014182789601950989,
      "loss": 2.307,
      "step": 1024
    },
    {
      "epoch": 1.8206039076376554,
      "grad_norm": 0.6182551383972168,
      "learning_rate": 0.00014172631946911962,
      "loss": 1.7271,
      "step": 1025
    },
    {
      "epoch": 1.822380106571936,
      "grad_norm": 0.5600947141647339,
      "learning_rate": 0.00014162469076358652,
      "loss": 1.946,
      "step": 1026
    },
    {
      "epoch": 1.8241563055062167,
      "grad_norm": 0.5816718339920044,
      "learning_rate": 0.00014152301002993978,
      "loss": 1.7972,
      "step": 1027
    },
    {
      "epoch": 1.8259325044404973,
      "grad_norm": 0.6150637269020081,
      "learning_rate": 0.00014142127739527353,
      "loss": 2.0241,
      "step": 1028
    },
    {
      "epoch": 1.8277087033747779,
      "grad_norm": 0.6355162262916565,
      "learning_rate": 0.00014131949298674688,
      "loss": 1.9661,
      "step": 1029
    },
    {
      "epoch": 1.8294849023090585,
      "grad_norm": 0.6061913967132568,
      "learning_rate": 0.00014121765693158363,
      "loss": 1.6641,
      "step": 1030
    },
    {
      "epoch": 1.8312611012433393,
      "grad_norm": 0.6190813779830933,
      "learning_rate": 0.00014111576935707214,
      "loss": 2.0545,
      "step": 1031
    },
    {
      "epoch": 1.83303730017762,
      "grad_norm": 0.6106885075569153,
      "learning_rate": 0.0001410138303905651,
      "loss": 2.0097,
      "step": 1032
    },
    {
      "epoch": 1.8348134991119005,
      "grad_norm": 0.6571269035339355,
      "learning_rate": 0.00014091184015947947,
      "loss": 2.133,
      "step": 1033
    },
    {
      "epoch": 1.8365896980461813,
      "grad_norm": 0.5942704081535339,
      "learning_rate": 0.00014080979879129636,
      "loss": 1.4696,
      "step": 1034
    },
    {
      "epoch": 1.838365896980462,
      "grad_norm": 0.6407346129417419,
      "learning_rate": 0.0001407077064135607,
      "loss": 2.1213,
      "step": 1035
    },
    {
      "epoch": 1.8401420959147425,
      "grad_norm": 0.6532360911369324,
      "learning_rate": 0.00014060556315388125,
      "loss": 1.7397,
      "step": 1036
    },
    {
      "epoch": 1.8419182948490231,
      "grad_norm": 0.6597435474395752,
      "learning_rate": 0.00014050336913993029,
      "loss": 1.8437,
      "step": 1037
    },
    {
      "epoch": 1.8436944937833037,
      "grad_norm": 0.6146280765533447,
      "learning_rate": 0.00014040112449944362,
      "loss": 1.5152,
      "step": 1038
    },
    {
      "epoch": 1.8454706927175843,
      "grad_norm": 0.6747410893440247,
      "learning_rate": 0.00014029882936022027,
      "loss": 1.9554,
      "step": 1039
    },
    {
      "epoch": 1.847246891651865,
      "grad_norm": 0.5976418852806091,
      "learning_rate": 0.00014019648385012244,
      "loss": 1.7472,
      "step": 1040
    },
    {
      "epoch": 1.8490230905861456,
      "grad_norm": 0.6336698532104492,
      "learning_rate": 0.0001400940880970752,
      "loss": 1.8079,
      "step": 1041
    },
    {
      "epoch": 1.8507992895204262,
      "grad_norm": 0.6174579858779907,
      "learning_rate": 0.0001399916422290665,
      "loss": 1.8871,
      "step": 1042
    },
    {
      "epoch": 1.8525754884547068,
      "grad_norm": 0.6794602870941162,
      "learning_rate": 0.00013988914637414696,
      "loss": 1.9351,
      "step": 1043
    },
    {
      "epoch": 1.8543516873889876,
      "grad_norm": 0.6272727251052856,
      "learning_rate": 0.00013978660066042955,
      "loss": 1.8945,
      "step": 1044
    },
    {
      "epoch": 1.8561278863232682,
      "grad_norm": 0.6417560577392578,
      "learning_rate": 0.00013968400521608969,
      "loss": 2.1021,
      "step": 1045
    },
    {
      "epoch": 1.8579040852575488,
      "grad_norm": 0.6674197912216187,
      "learning_rate": 0.00013958136016936488,
      "loss": 2.1302,
      "step": 1046
    },
    {
      "epoch": 1.8596802841918296,
      "grad_norm": 0.6634453535079956,
      "learning_rate": 0.00013947866564855465,
      "loss": 1.8232,
      "step": 1047
    },
    {
      "epoch": 1.8614564831261102,
      "grad_norm": 0.6008985042572021,
      "learning_rate": 0.00013937592178202038,
      "loss": 1.909,
      "step": 1048
    },
    {
      "epoch": 1.8632326820603908,
      "grad_norm": 0.6436120271682739,
      "learning_rate": 0.0001392731286981851,
      "loss": 1.8741,
      "step": 1049
    },
    {
      "epoch": 1.8650088809946714,
      "grad_norm": 0.5877550840377808,
      "learning_rate": 0.00013917028652553337,
      "loss": 1.9203,
      "step": 1050
    },
    {
      "epoch": 1.866785079928952,
      "grad_norm": 0.6221259832382202,
      "learning_rate": 0.00013906739539261116,
      "loss": 2.0107,
      "step": 1051
    },
    {
      "epoch": 1.8685612788632326,
      "grad_norm": 0.6047084331512451,
      "learning_rate": 0.00013896445542802555,
      "loss": 1.8863,
      "step": 1052
    },
    {
      "epoch": 1.8703374777975132,
      "grad_norm": 0.6116951704025269,
      "learning_rate": 0.0001388614667604447,
      "loss": 2.0519,
      "step": 1053
    },
    {
      "epoch": 1.8721136767317939,
      "grad_norm": 0.6000748872756958,
      "learning_rate": 0.0001387584295185976,
      "loss": 1.8725,
      "step": 1054
    },
    {
      "epoch": 1.8738898756660745,
      "grad_norm": 0.5813182592391968,
      "learning_rate": 0.00013865534383127406,
      "loss": 1.9681,
      "step": 1055
    },
    {
      "epoch": 1.8756660746003553,
      "grad_norm": 0.6076121926307678,
      "learning_rate": 0.00013855220982732436,
      "loss": 2.1051,
      "step": 1056
    },
    {
      "epoch": 1.8774422735346359,
      "grad_norm": 0.5534208416938782,
      "learning_rate": 0.00013844902763565916,
      "loss": 2.1804,
      "step": 1057
    },
    {
      "epoch": 1.8792184724689165,
      "grad_norm": 0.6875385642051697,
      "learning_rate": 0.00013834579738524944,
      "loss": 1.7007,
      "step": 1058
    },
    {
      "epoch": 1.8809946714031973,
      "grad_norm": 0.6850054860115051,
      "learning_rate": 0.00013824251920512617,
      "loss": 2.0897,
      "step": 1059
    },
    {
      "epoch": 1.882770870337478,
      "grad_norm": 0.6359685659408569,
      "learning_rate": 0.0001381391932243802,
      "loss": 2.2506,
      "step": 1060
    },
    {
      "epoch": 1.8845470692717585,
      "grad_norm": 0.5898910164833069,
      "learning_rate": 0.00013803581957216228,
      "loss": 1.8715,
      "step": 1061
    },
    {
      "epoch": 1.8863232682060391,
      "grad_norm": 0.6087694764137268,
      "learning_rate": 0.00013793239837768255,
      "loss": 1.7582,
      "step": 1062
    },
    {
      "epoch": 1.8880994671403197,
      "grad_norm": 0.6221541166305542,
      "learning_rate": 0.00013782892977021077,
      "loss": 1.9156,
      "step": 1063
    },
    {
      "epoch": 1.8898756660746003,
      "grad_norm": 0.5853484869003296,
      "learning_rate": 0.00013772541387907573,
      "loss": 1.7208,
      "step": 1064
    },
    {
      "epoch": 1.891651865008881,
      "grad_norm": 0.5786738991737366,
      "learning_rate": 0.00013762185083366556,
      "loss": 1.8159,
      "step": 1065
    },
    {
      "epoch": 1.8934280639431615,
      "grad_norm": 0.6187761425971985,
      "learning_rate": 0.00013751824076342723,
      "loss": 1.7872,
      "step": 1066
    },
    {
      "epoch": 1.8952042628774421,
      "grad_norm": 0.6213208436965942,
      "learning_rate": 0.00013741458379786642,
      "loss": 1.7129,
      "step": 1067
    },
    {
      "epoch": 1.8969804618117228,
      "grad_norm": 0.5573623776435852,
      "learning_rate": 0.00013731088006654752,
      "loss": 1.934,
      "step": 1068
    },
    {
      "epoch": 1.8987566607460036,
      "grad_norm": 0.6067723631858826,
      "learning_rate": 0.00013720712969909337,
      "loss": 1.8422,
      "step": 1069
    },
    {
      "epoch": 1.9005328596802842,
      "grad_norm": 0.6053537130355835,
      "learning_rate": 0.00013710333282518505,
      "loss": 2.1016,
      "step": 1070
    },
    {
      "epoch": 1.9023090586145648,
      "grad_norm": 0.583219051361084,
      "learning_rate": 0.00013699948957456175,
      "loss": 2.0068,
      "step": 1071
    },
    {
      "epoch": 1.9040852575488456,
      "grad_norm": 0.6202930212020874,
      "learning_rate": 0.00013689560007702076,
      "loss": 1.6899,
      "step": 1072
    },
    {
      "epoch": 1.9058614564831262,
      "grad_norm": 0.5542877912521362,
      "learning_rate": 0.000136791664462417,
      "loss": 1.4806,
      "step": 1073
    },
    {
      "epoch": 1.9076376554174068,
      "grad_norm": 0.6103153824806213,
      "learning_rate": 0.00013668768286066323,
      "loss": 1.6,
      "step": 1074
    },
    {
      "epoch": 1.9094138543516874,
      "grad_norm": 0.6441481709480286,
      "learning_rate": 0.0001365836554017295,
      "loss": 1.957,
      "step": 1075
    },
    {
      "epoch": 1.911190053285968,
      "grad_norm": 0.7117332220077515,
      "learning_rate": 0.00013647958221564324,
      "loss": 1.7959,
      "step": 1076
    },
    {
      "epoch": 1.9129662522202486,
      "grad_norm": 0.6025160551071167,
      "learning_rate": 0.00013637546343248916,
      "loss": 1.7099,
      "step": 1077
    },
    {
      "epoch": 1.9147424511545292,
      "grad_norm": 0.6101287007331848,
      "learning_rate": 0.00013627129918240876,
      "loss": 1.9272,
      "step": 1078
    },
    {
      "epoch": 1.9165186500888098,
      "grad_norm": 0.6287856101989746,
      "learning_rate": 0.00013616708959560055,
      "loss": 1.7974,
      "step": 1079
    },
    {
      "epoch": 1.9182948490230904,
      "grad_norm": 0.6103188991546631,
      "learning_rate": 0.00013606283480231956,
      "loss": 1.8804,
      "step": 1080
    },
    {
      "epoch": 1.9200710479573713,
      "grad_norm": 0.6145761013031006,
      "learning_rate": 0.00013595853493287741,
      "loss": 1.8188,
      "step": 1081
    },
    {
      "epoch": 1.9218472468916519,
      "grad_norm": 0.6743513941764832,
      "learning_rate": 0.00013585419011764212,
      "loss": 2.0924,
      "step": 1082
    },
    {
      "epoch": 1.9236234458259325,
      "grad_norm": 0.6146329641342163,
      "learning_rate": 0.00013574980048703772,
      "loss": 1.8063,
      "step": 1083
    },
    {
      "epoch": 1.9253996447602133,
      "grad_norm": 0.6395987272262573,
      "learning_rate": 0.00013564536617154439,
      "loss": 1.8649,
      "step": 1084
    },
    {
      "epoch": 1.927175843694494,
      "grad_norm": 0.6315710544586182,
      "learning_rate": 0.00013554088730169815,
      "loss": 2.0352,
      "step": 1085
    },
    {
      "epoch": 1.9289520426287745,
      "grad_norm": 0.5919815301895142,
      "learning_rate": 0.00013543636400809063,
      "loss": 2.0221,
      "step": 1086
    },
    {
      "epoch": 1.9307282415630551,
      "grad_norm": 0.6239882707595825,
      "learning_rate": 0.00013533179642136908,
      "loss": 1.9451,
      "step": 1087
    },
    {
      "epoch": 1.9325044404973357,
      "grad_norm": 0.6459618210792542,
      "learning_rate": 0.0001352271846722361,
      "loss": 2.001,
      "step": 1088
    },
    {
      "epoch": 1.9342806394316163,
      "grad_norm": 0.610252857208252,
      "learning_rate": 0.0001351225288914494,
      "loss": 2.1027,
      "step": 1089
    },
    {
      "epoch": 1.936056838365897,
      "grad_norm": 0.6391324996948242,
      "learning_rate": 0.00013501782920982184,
      "loss": 1.9833,
      "step": 1090
    },
    {
      "epoch": 1.9378330373001775,
      "grad_norm": 0.6304078698158264,
      "learning_rate": 0.0001349130857582211,
      "loss": 1.5757,
      "step": 1091
    },
    {
      "epoch": 1.9396092362344581,
      "grad_norm": 0.614848256111145,
      "learning_rate": 0.00013480829866756956,
      "loss": 1.8984,
      "step": 1092
    },
    {
      "epoch": 1.9413854351687387,
      "grad_norm": 0.6130755543708801,
      "learning_rate": 0.00013470346806884417,
      "loss": 1.9316,
      "step": 1093
    },
    {
      "epoch": 1.9431616341030196,
      "grad_norm": 0.5939034819602966,
      "learning_rate": 0.00013459859409307624,
      "loss": 1.9638,
      "step": 1094
    },
    {
      "epoch": 1.9449378330373002,
      "grad_norm": 0.6035938858985901,
      "learning_rate": 0.00013449367687135136,
      "loss": 1.8586,
      "step": 1095
    },
    {
      "epoch": 1.9467140319715808,
      "grad_norm": 0.5947110056877136,
      "learning_rate": 0.00013438871653480904,
      "loss": 1.8311,
      "step": 1096
    },
    {
      "epoch": 1.9484902309058616,
      "grad_norm": 0.5570203065872192,
      "learning_rate": 0.00013428371321464285,
      "loss": 2.085,
      "step": 1097
    },
    {
      "epoch": 1.9502664298401422,
      "grad_norm": 0.561173677444458,
      "learning_rate": 0.00013417866704209996,
      "loss": 2.0329,
      "step": 1098
    },
    {
      "epoch": 1.9520426287744228,
      "grad_norm": 0.5849804282188416,
      "learning_rate": 0.00013407357814848115,
      "loss": 1.9762,
      "step": 1099
    },
    {
      "epoch": 1.9538188277087034,
      "grad_norm": 0.5862393975257874,
      "learning_rate": 0.0001339684466651406,
      "loss": 1.5131,
      "step": 1100
    },
    {
      "epoch": 1.9538188277087034,
      "eval_loss": 1.885851502418518,
      "eval_runtime": 17.5646,
      "eval_samples_per_second": 56.99,
      "eval_steps_per_second": 28.523,
      "step": 1100
    },
    {
      "epoch": 1.955595026642984,
      "grad_norm": 0.5955780744552612,
      "learning_rate": 0.00013386327272348574,
      "loss": 1.8479,
      "step": 1101
    },
    {
      "epoch": 1.9573712255772646,
      "grad_norm": 0.6153426170349121,
      "learning_rate": 0.00013375805645497702,
      "loss": 1.576,
      "step": 1102
    },
    {
      "epoch": 1.9591474245115452,
      "grad_norm": 0.6437920928001404,
      "learning_rate": 0.00013365279799112782,
      "loss": 2.0317,
      "step": 1103
    },
    {
      "epoch": 1.9609236234458258,
      "grad_norm": 0.6182078719139099,
      "learning_rate": 0.00013354749746350428,
      "loss": 1.9914,
      "step": 1104
    },
    {
      "epoch": 1.9626998223801064,
      "grad_norm": 0.635287344455719,
      "learning_rate": 0.0001334421550037251,
      "loss": 2.1377,
      "step": 1105
    },
    {
      "epoch": 1.9644760213143873,
      "grad_norm": 0.6066455245018005,
      "learning_rate": 0.00013333677074346142,
      "loss": 2.1579,
      "step": 1106
    },
    {
      "epoch": 1.9662522202486679,
      "grad_norm": 0.6348589658737183,
      "learning_rate": 0.00013323134481443656,
      "loss": 2.0059,
      "step": 1107
    },
    {
      "epoch": 1.9680284191829485,
      "grad_norm": 0.6161692142486572,
      "learning_rate": 0.00013312587734842598,
      "loss": 2.3329,
      "step": 1108
    },
    {
      "epoch": 1.9698046181172293,
      "grad_norm": 0.6208921074867249,
      "learning_rate": 0.0001330203684772571,
      "loss": 1.8404,
      "step": 1109
    },
    {
      "epoch": 1.97158081705151,
      "grad_norm": 0.6304396986961365,
      "learning_rate": 0.00013291481833280897,
      "loss": 1.817,
      "step": 1110
    },
    {
      "epoch": 1.9733570159857905,
      "grad_norm": 0.6797948479652405,
      "learning_rate": 0.0001328092270470123,
      "loss": 1.9542,
      "step": 1111
    },
    {
      "epoch": 1.975133214920071,
      "grad_norm": 0.6342067718505859,
      "learning_rate": 0.00013270359475184927,
      "loss": 1.9759,
      "step": 1112
    },
    {
      "epoch": 1.9769094138543517,
      "grad_norm": 0.6213245987892151,
      "learning_rate": 0.00013259792157935325,
      "loss": 1.9476,
      "step": 1113
    },
    {
      "epoch": 1.9786856127886323,
      "grad_norm": 0.6586663722991943,
      "learning_rate": 0.0001324922076616087,
      "loss": 2.0224,
      "step": 1114
    },
    {
      "epoch": 1.980461811722913,
      "grad_norm": 0.5851541757583618,
      "learning_rate": 0.00013238645313075104,
      "loss": 1.9954,
      "step": 1115
    },
    {
      "epoch": 1.9822380106571935,
      "grad_norm": 0.6199893951416016,
      "learning_rate": 0.0001322806581189665,
      "loss": 1.899,
      "step": 1116
    },
    {
      "epoch": 1.9840142095914741,
      "grad_norm": 0.5442419052124023,
      "learning_rate": 0.00013217482275849177,
      "loss": 1.6778,
      "step": 1117
    },
    {
      "epoch": 1.9857904085257547,
      "grad_norm": 0.5974409580230713,
      "learning_rate": 0.00013206894718161414,
      "loss": 2.1242,
      "step": 1118
    },
    {
      "epoch": 1.9875666074600356,
      "grad_norm": 0.6146173477172852,
      "learning_rate": 0.00013196303152067105,
      "loss": 1.7568,
      "step": 1119
    },
    {
      "epoch": 1.9893428063943162,
      "grad_norm": 0.6045464277267456,
      "learning_rate": 0.00013185707590805004,
      "loss": 1.8755,
      "step": 1120
    },
    {
      "epoch": 1.9911190053285968,
      "grad_norm": 0.6209976673126221,
      "learning_rate": 0.00013175108047618867,
      "loss": 1.7619,
      "step": 1121
    },
    {
      "epoch": 1.9928952042628776,
      "grad_norm": 0.6564018130302429,
      "learning_rate": 0.00013164504535757425,
      "loss": 1.8957,
      "step": 1122
    },
    {
      "epoch": 1.9946714031971582,
      "grad_norm": 0.5871119499206543,
      "learning_rate": 0.00013153897068474366,
      "loss": 1.9503,
      "step": 1123
    },
    {
      "epoch": 1.9964476021314388,
      "grad_norm": 0.6419007778167725,
      "learning_rate": 0.0001314328565902832,
      "loss": 1.5003,
      "step": 1124
    },
    {
      "epoch": 1.9982238010657194,
      "grad_norm": 0.568334698677063,
      "learning_rate": 0.00013132670320682848,
      "loss": 1.6625,
      "step": 1125
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.7225291132926941,
      "learning_rate": 0.00013122051066706428,
      "loss": 1.7709,
      "step": 1126
    },
    {
      "epoch": 2.0017761989342806,
      "grad_norm": 0.5354976058006287,
      "learning_rate": 0.00013111427910372417,
      "loss": 1.7248,
      "step": 1127
    },
    {
      "epoch": 2.003552397868561,
      "grad_norm": 0.582019031047821,
      "learning_rate": 0.00013100800864959065,
      "loss": 1.6086,
      "step": 1128
    },
    {
      "epoch": 2.005328596802842,
      "grad_norm": 0.5633111000061035,
      "learning_rate": 0.00013090169943749476,
      "loss": 1.5729,
      "step": 1129
    },
    {
      "epoch": 2.0071047957371224,
      "grad_norm": 0.580510139465332,
      "learning_rate": 0.00013079535160031598,
      "loss": 1.9094,
      "step": 1130
    },
    {
      "epoch": 2.008880994671403,
      "grad_norm": 0.5503290295600891,
      "learning_rate": 0.00013068896527098205,
      "loss": 1.5194,
      "step": 1131
    },
    {
      "epoch": 2.0106571936056836,
      "grad_norm": 0.5953585505485535,
      "learning_rate": 0.00013058254058246892,
      "loss": 1.5126,
      "step": 1132
    },
    {
      "epoch": 2.0124333925399647,
      "grad_norm": 0.5854758024215698,
      "learning_rate": 0.00013047607766780036,
      "loss": 1.6809,
      "step": 1133
    },
    {
      "epoch": 2.0142095914742453,
      "grad_norm": 0.5521567463874817,
      "learning_rate": 0.00013036957666004803,
      "loss": 1.7349,
      "step": 1134
    },
    {
      "epoch": 2.015985790408526,
      "grad_norm": 0.6249696016311646,
      "learning_rate": 0.00013026303769233112,
      "loss": 1.7462,
      "step": 1135
    },
    {
      "epoch": 2.0177619893428065,
      "grad_norm": 0.6075407862663269,
      "learning_rate": 0.0001301564608978163,
      "loss": 1.9102,
      "step": 1136
    },
    {
      "epoch": 2.019538188277087,
      "grad_norm": 0.6001859903335571,
      "learning_rate": 0.00013004984640971754,
      "loss": 1.7502,
      "step": 1137
    },
    {
      "epoch": 2.0213143872113677,
      "grad_norm": 0.6225956678390503,
      "learning_rate": 0.00012994319436129592,
      "loss": 1.7275,
      "step": 1138
    },
    {
      "epoch": 2.0230905861456483,
      "grad_norm": 0.6426296234130859,
      "learning_rate": 0.00012983650488585942,
      "loss": 1.4179,
      "step": 1139
    },
    {
      "epoch": 2.024866785079929,
      "grad_norm": 0.6163917183876038,
      "learning_rate": 0.00012972977811676287,
      "loss": 1.7927,
      "step": 1140
    },
    {
      "epoch": 2.0266429840142095,
      "grad_norm": 0.6175844073295593,
      "learning_rate": 0.00012962301418740764,
      "loss": 1.8346,
      "step": 1141
    },
    {
      "epoch": 2.02841918294849,
      "grad_norm": 0.6014488339424133,
      "learning_rate": 0.0001295162132312416,
      "loss": 1.9409,
      "step": 1142
    },
    {
      "epoch": 2.0301953818827707,
      "grad_norm": 0.6333334445953369,
      "learning_rate": 0.00012940937538175891,
      "loss": 2.1461,
      "step": 1143
    },
    {
      "epoch": 2.0319715808170513,
      "grad_norm": 0.649249792098999,
      "learning_rate": 0.0001293025007724998,
      "loss": 1.7036,
      "step": 1144
    },
    {
      "epoch": 2.0337477797513324,
      "grad_norm": 0.7296144962310791,
      "learning_rate": 0.00012919558953705055,
      "loss": 1.7213,
      "step": 1145
    },
    {
      "epoch": 2.035523978685613,
      "grad_norm": 0.6469215750694275,
      "learning_rate": 0.00012908864180904305,
      "loss": 1.6099,
      "step": 1146
    },
    {
      "epoch": 2.0373001776198936,
      "grad_norm": 0.753025233745575,
      "learning_rate": 0.0001289816577221549,
      "loss": 1.6613,
      "step": 1147
    },
    {
      "epoch": 2.039076376554174,
      "grad_norm": 0.7281018495559692,
      "learning_rate": 0.0001288746374101092,
      "loss": 1.8854,
      "step": 1148
    },
    {
      "epoch": 2.040852575488455,
      "grad_norm": 0.675270676612854,
      "learning_rate": 0.00012876758100667418,
      "loss": 1.4848,
      "step": 1149
    },
    {
      "epoch": 2.0426287744227354,
      "grad_norm": 0.7759418487548828,
      "learning_rate": 0.00012866048864566338,
      "loss": 1.4683,
      "step": 1150
    },
    {
      "epoch": 2.044404973357016,
      "grad_norm": 0.6943955421447754,
      "learning_rate": 0.0001285533604609351,
      "loss": 1.8875,
      "step": 1151
    },
    {
      "epoch": 2.0461811722912966,
      "grad_norm": 0.7268499732017517,
      "learning_rate": 0.00012844619658639253,
      "loss": 1.5791,
      "step": 1152
    },
    {
      "epoch": 2.047957371225577,
      "grad_norm": 0.6871978044509888,
      "learning_rate": 0.0001283389971559834,
      "loss": 1.5478,
      "step": 1153
    },
    {
      "epoch": 2.049733570159858,
      "grad_norm": 0.7628158926963806,
      "learning_rate": 0.00012823176230369993,
      "loss": 1.5716,
      "step": 1154
    },
    {
      "epoch": 2.0515097690941384,
      "grad_norm": 0.7313343286514282,
      "learning_rate": 0.00012812449216357863,
      "loss": 1.6798,
      "step": 1155
    },
    {
      "epoch": 2.053285968028419,
      "grad_norm": 0.7426896691322327,
      "learning_rate": 0.00012801718686970004,
      "loss": 1.6222,
      "step": 1156
    },
    {
      "epoch": 2.0550621669626996,
      "grad_norm": 0.7563676834106445,
      "learning_rate": 0.00012790984655618875,
      "loss": 1.5591,
      "step": 1157
    },
    {
      "epoch": 2.0568383658969807,
      "grad_norm": 0.6889647245407104,
      "learning_rate": 0.00012780247135721302,
      "loss": 1.8123,
      "step": 1158
    },
    {
      "epoch": 2.0586145648312613,
      "grad_norm": 0.720762312412262,
      "learning_rate": 0.00012769506140698477,
      "loss": 1.6953,
      "step": 1159
    },
    {
      "epoch": 2.060390763765542,
      "grad_norm": 0.7329158782958984,
      "learning_rate": 0.0001275876168397593,
      "loss": 1.3897,
      "step": 1160
    },
    {
      "epoch": 2.0621669626998225,
      "grad_norm": 0.6889504194259644,
      "learning_rate": 0.0001274801377898353,
      "loss": 1.3298,
      "step": 1161
    },
    {
      "epoch": 2.063943161634103,
      "grad_norm": 0.7645026445388794,
      "learning_rate": 0.0001273726243915544,
      "loss": 1.6392,
      "step": 1162
    },
    {
      "epoch": 2.0657193605683837,
      "grad_norm": 0.6947687268257141,
      "learning_rate": 0.0001272650767793013,
      "loss": 1.489,
      "step": 1163
    },
    {
      "epoch": 2.0674955595026643,
      "grad_norm": 0.6823424696922302,
      "learning_rate": 0.0001271574950875034,
      "loss": 1.5683,
      "step": 1164
    },
    {
      "epoch": 2.069271758436945,
      "grad_norm": 0.7123419046401978,
      "learning_rate": 0.00012704987945063068,
      "loss": 1.7455,
      "step": 1165
    },
    {
      "epoch": 2.0710479573712255,
      "grad_norm": 0.7617823481559753,
      "learning_rate": 0.0001269422300031956,
      "loss": 1.6955,
      "step": 1166
    },
    {
      "epoch": 2.072824156305506,
      "grad_norm": 0.7396826148033142,
      "learning_rate": 0.0001268345468797529,
      "loss": 1.9988,
      "step": 1167
    },
    {
      "epoch": 2.0746003552397867,
      "grad_norm": 0.7417138814926147,
      "learning_rate": 0.00012672683021489926,
      "loss": 1.7869,
      "step": 1168
    },
    {
      "epoch": 2.0763765541740673,
      "grad_norm": 0.719014048576355,
      "learning_rate": 0.00012661908014327352,
      "loss": 1.7327,
      "step": 1169
    },
    {
      "epoch": 2.0781527531083483,
      "grad_norm": 0.7568150758743286,
      "learning_rate": 0.00012651129679955604,
      "loss": 1.4469,
      "step": 1170
    },
    {
      "epoch": 2.079928952042629,
      "grad_norm": 0.6859951615333557,
      "learning_rate": 0.00012640348031846895,
      "loss": 1.6044,
      "step": 1171
    },
    {
      "epoch": 2.0817051509769096,
      "grad_norm": 0.7894623875617981,
      "learning_rate": 0.00012629563083477573,
      "loss": 1.6988,
      "step": 1172
    },
    {
      "epoch": 2.08348134991119,
      "grad_norm": 0.77374267578125,
      "learning_rate": 0.0001261877484832811,
      "loss": 1.5911,
      "step": 1173
    },
    {
      "epoch": 2.0852575488454708,
      "grad_norm": 0.7674537301063538,
      "learning_rate": 0.00012607983339883082,
      "loss": 1.683,
      "step": 1174
    },
    {
      "epoch": 2.0870337477797514,
      "grad_norm": 0.7546994090080261,
      "learning_rate": 0.00012597188571631169,
      "loss": 1.5838,
      "step": 1175
    },
    {
      "epoch": 2.088809946714032,
      "grad_norm": 0.8087135553359985,
      "learning_rate": 0.00012586390557065117,
      "loss": 1.8536,
      "step": 1176
    },
    {
      "epoch": 2.0905861456483126,
      "grad_norm": 0.7492823600769043,
      "learning_rate": 0.00012575589309681725,
      "loss": 1.5822,
      "step": 1177
    },
    {
      "epoch": 2.092362344582593,
      "grad_norm": 0.7981756329536438,
      "learning_rate": 0.00012564784842981845,
      "loss": 1.6597,
      "step": 1178
    },
    {
      "epoch": 2.094138543516874,
      "grad_norm": 0.7556410431861877,
      "learning_rate": 0.00012553977170470343,
      "loss": 1.9397,
      "step": 1179
    },
    {
      "epoch": 2.0959147424511544,
      "grad_norm": 0.7218524217605591,
      "learning_rate": 0.00012543166305656098,
      "loss": 1.5698,
      "step": 1180
    },
    {
      "epoch": 2.097690941385435,
      "grad_norm": 0.8229969143867493,
      "learning_rate": 0.00012532352262051977,
      "loss": 1.5326,
      "step": 1181
    },
    {
      "epoch": 2.0994671403197156,
      "grad_norm": 0.7564206123352051,
      "learning_rate": 0.00012521535053174819,
      "loss": 1.5513,
      "step": 1182
    },
    {
      "epoch": 2.1012433392539966,
      "grad_norm": 0.7433948516845703,
      "learning_rate": 0.00012510714692545415,
      "loss": 1.9217,
      "step": 1183
    },
    {
      "epoch": 2.1030195381882772,
      "grad_norm": 0.7356088757514954,
      "learning_rate": 0.00012499891193688514,
      "loss": 1.785,
      "step": 1184
    },
    {
      "epoch": 2.104795737122558,
      "grad_norm": 0.690923273563385,
      "learning_rate": 0.00012489064570132764,
      "loss": 1.2985,
      "step": 1185
    },
    {
      "epoch": 2.1065719360568385,
      "grad_norm": 0.7302056550979614,
      "learning_rate": 0.00012478234835410732,
      "loss": 1.9067,
      "step": 1186
    },
    {
      "epoch": 2.108348134991119,
      "grad_norm": 0.7207238674163818,
      "learning_rate": 0.0001246740200305887,
      "loss": 1.7022,
      "step": 1187
    },
    {
      "epoch": 2.1101243339253997,
      "grad_norm": 0.7525375485420227,
      "learning_rate": 0.00012456566086617508,
      "loss": 1.7447,
      "step": 1188
    },
    {
      "epoch": 2.1119005328596803,
      "grad_norm": 0.7725025415420532,
      "learning_rate": 0.0001244572709963082,
      "loss": 1.5764,
      "step": 1189
    },
    {
      "epoch": 2.113676731793961,
      "grad_norm": 0.7478821873664856,
      "learning_rate": 0.00012434885055646823,
      "loss": 1.7709,
      "step": 1190
    },
    {
      "epoch": 2.1154529307282415,
      "grad_norm": 0.7622014880180359,
      "learning_rate": 0.0001242403996821736,
      "loss": 1.6323,
      "step": 1191
    },
    {
      "epoch": 2.117229129662522,
      "grad_norm": 0.7546221017837524,
      "learning_rate": 0.00012413191850898071,
      "loss": 1.749,
      "step": 1192
    },
    {
      "epoch": 2.1190053285968027,
      "grad_norm": 0.7640188932418823,
      "learning_rate": 0.0001240234071724839,
      "loss": 1.7235,
      "step": 1193
    },
    {
      "epoch": 2.1207815275310833,
      "grad_norm": 0.8108890652656555,
      "learning_rate": 0.00012391486580831513,
      "loss": 1.4344,
      "step": 1194
    },
    {
      "epoch": 2.122557726465364,
      "grad_norm": 0.7706875205039978,
      "learning_rate": 0.00012380629455214392,
      "loss": 1.6239,
      "step": 1195
    },
    {
      "epoch": 2.124333925399645,
      "grad_norm": 0.7562199234962463,
      "learning_rate": 0.00012369769353967722,
      "loss": 1.7659,
      "step": 1196
    },
    {
      "epoch": 2.1261101243339255,
      "grad_norm": 0.7015137076377869,
      "learning_rate": 0.0001235890629066591,
      "loss": 1.2511,
      "step": 1197
    },
    {
      "epoch": 2.127886323268206,
      "grad_norm": 0.8751410841941833,
      "learning_rate": 0.00012348040278887065,
      "loss": 1.5989,
      "step": 1198
    },
    {
      "epoch": 2.1296625222024868,
      "grad_norm": 0.8563438653945923,
      "learning_rate": 0.0001233717133221299,
      "loss": 1.5288,
      "step": 1199
    },
    {
      "epoch": 2.1314387211367674,
      "grad_norm": 0.7732565402984619,
      "learning_rate": 0.00012326299464229142,
      "loss": 1.3711,
      "step": 1200
    },
    {
      "epoch": 2.1314387211367674,
      "eval_loss": 1.9370704889297485,
      "eval_runtime": 17.5241,
      "eval_samples_per_second": 57.121,
      "eval_steps_per_second": 28.589,
      "step": 1200
    },
    {
      "epoch": 2.133214920071048,
      "grad_norm": 0.7605934143066406,
      "learning_rate": 0.00012315424688524645,
      "loss": 1.5829,
      "step": 1201
    },
    {
      "epoch": 2.1349911190053286,
      "grad_norm": 0.6945953369140625,
      "learning_rate": 0.00012304547018692245,
      "loss": 1.5677,
      "step": 1202
    },
    {
      "epoch": 2.136767317939609,
      "grad_norm": 0.7769915461540222,
      "learning_rate": 0.00012293666468328316,
      "loss": 1.5359,
      "step": 1203
    },
    {
      "epoch": 2.1385435168738898,
      "grad_norm": 0.8047837615013123,
      "learning_rate": 0.00012282783051032823,
      "loss": 1.8348,
      "step": 1204
    },
    {
      "epoch": 2.1403197158081704,
      "grad_norm": 0.7637043595314026,
      "learning_rate": 0.0001227189678040932,
      "loss": 1.7193,
      "step": 1205
    },
    {
      "epoch": 2.142095914742451,
      "grad_norm": 0.768004834651947,
      "learning_rate": 0.00012261007670064927,
      "loss": 1.668,
      "step": 1206
    },
    {
      "epoch": 2.143872113676732,
      "grad_norm": 0.7972174286842346,
      "learning_rate": 0.0001225011573361031,
      "loss": 1.9134,
      "step": 1207
    },
    {
      "epoch": 2.1456483126110126,
      "grad_norm": 0.8336536884307861,
      "learning_rate": 0.0001223922098465967,
      "loss": 1.3297,
      "step": 1208
    },
    {
      "epoch": 2.1474245115452932,
      "grad_norm": 0.8196168541908264,
      "learning_rate": 0.00012228323436830728,
      "loss": 1.6577,
      "step": 1209
    },
    {
      "epoch": 2.149200710479574,
      "grad_norm": 0.7482585310935974,
      "learning_rate": 0.00012217423103744695,
      "loss": 1.4855,
      "step": 1210
    },
    {
      "epoch": 2.1509769094138544,
      "grad_norm": 0.7958678603172302,
      "learning_rate": 0.00012206519999026269,
      "loss": 1.6544,
      "step": 1211
    },
    {
      "epoch": 2.152753108348135,
      "grad_norm": 0.7676479816436768,
      "learning_rate": 0.0001219561413630361,
      "loss": 1.7593,
      "step": 1212
    },
    {
      "epoch": 2.1545293072824157,
      "grad_norm": 0.7480295896530151,
      "learning_rate": 0.00012184705529208325,
      "loss": 1.6949,
      "step": 1213
    },
    {
      "epoch": 2.1563055062166963,
      "grad_norm": 0.7722238302230835,
      "learning_rate": 0.00012173794191375454,
      "loss": 1.6406,
      "step": 1214
    },
    {
      "epoch": 2.158081705150977,
      "grad_norm": 0.7668100595474243,
      "learning_rate": 0.00012162880136443447,
      "loss": 1.687,
      "step": 1215
    },
    {
      "epoch": 2.1598579040852575,
      "grad_norm": 0.7511127591133118,
      "learning_rate": 0.00012151963378054152,
      "loss": 1.6312,
      "step": 1216
    },
    {
      "epoch": 2.161634103019538,
      "grad_norm": 0.7683548331260681,
      "learning_rate": 0.00012141043929852798,
      "loss": 1.6433,
      "step": 1217
    },
    {
      "epoch": 2.1634103019538187,
      "grad_norm": 0.6840116381645203,
      "learning_rate": 0.00012130121805487968,
      "loss": 1.38,
      "step": 1218
    },
    {
      "epoch": 2.1651865008880993,
      "grad_norm": 0.7530163526535034,
      "learning_rate": 0.00012119197018611603,
      "loss": 1.4895,
      "step": 1219
    },
    {
      "epoch": 2.1669626998223803,
      "grad_norm": 0.7700425982475281,
      "learning_rate": 0.00012108269582878957,
      "loss": 1.2939,
      "step": 1220
    },
    {
      "epoch": 2.168738898756661,
      "grad_norm": 0.7536581158638,
      "learning_rate": 0.00012097339511948612,
      "loss": 1.3609,
      "step": 1221
    },
    {
      "epoch": 2.1705150976909415,
      "grad_norm": 0.7880892157554626,
      "learning_rate": 0.00012086406819482433,
      "loss": 1.6302,
      "step": 1222
    },
    {
      "epoch": 2.172291296625222,
      "grad_norm": 0.8589406609535217,
      "learning_rate": 0.00012075471519145554,
      "loss": 1.7408,
      "step": 1223
    },
    {
      "epoch": 2.1740674955595027,
      "grad_norm": 0.8552658557891846,
      "learning_rate": 0.00012064533624606386,
      "loss": 1.5689,
      "step": 1224
    },
    {
      "epoch": 2.1758436944937833,
      "grad_norm": 0.7969607710838318,
      "learning_rate": 0.00012053593149536576,
      "loss": 1.8182,
      "step": 1225
    },
    {
      "epoch": 2.177619893428064,
      "grad_norm": 0.8433353304862976,
      "learning_rate": 0.00012042650107610989,
      "loss": 1.7191,
      "step": 1226
    },
    {
      "epoch": 2.1793960923623446,
      "grad_norm": 0.8146222233772278,
      "learning_rate": 0.00012031704512507709,
      "loss": 1.7532,
      "step": 1227
    },
    {
      "epoch": 2.181172291296625,
      "grad_norm": 0.8028756380081177,
      "learning_rate": 0.00012020756377908007,
      "loss": 1.6404,
      "step": 1228
    },
    {
      "epoch": 2.1829484902309058,
      "grad_norm": 0.781990110874176,
      "learning_rate": 0.00012009805717496322,
      "loss": 1.4821,
      "step": 1229
    },
    {
      "epoch": 2.1847246891651864,
      "grad_norm": 0.7805576920509338,
      "learning_rate": 0.00011998852544960266,
      "loss": 1.6486,
      "step": 1230
    },
    {
      "epoch": 2.186500888099467,
      "grad_norm": 0.7514439225196838,
      "learning_rate": 0.00011987896873990572,
      "loss": 1.3031,
      "step": 1231
    },
    {
      "epoch": 2.1882770870337476,
      "grad_norm": 0.8330938220024109,
      "learning_rate": 0.00011976938718281112,
      "loss": 1.6154,
      "step": 1232
    },
    {
      "epoch": 2.1900532859680286,
      "grad_norm": 0.8010828495025635,
      "learning_rate": 0.00011965978091528852,
      "loss": 1.8354,
      "step": 1233
    },
    {
      "epoch": 2.191829484902309,
      "grad_norm": 0.8373475074768066,
      "learning_rate": 0.00011955015007433851,
      "loss": 1.8034,
      "step": 1234
    },
    {
      "epoch": 2.19360568383659,
      "grad_norm": 0.8154692053794861,
      "learning_rate": 0.00011944049479699244,
      "loss": 1.5838,
      "step": 1235
    },
    {
      "epoch": 2.1953818827708704,
      "grad_norm": 0.7838035225868225,
      "learning_rate": 0.00011933081522031211,
      "loss": 1.4599,
      "step": 1236
    },
    {
      "epoch": 2.197158081705151,
      "grad_norm": 0.8371543884277344,
      "learning_rate": 0.00011922111148138977,
      "loss": 1.7636,
      "step": 1237
    },
    {
      "epoch": 2.1989342806394316,
      "grad_norm": 0.8869801163673401,
      "learning_rate": 0.00011911138371734789,
      "loss": 1.5873,
      "step": 1238
    },
    {
      "epoch": 2.2007104795737122,
      "grad_norm": 0.7572010159492493,
      "learning_rate": 0.00011900163206533888,
      "loss": 1.8158,
      "step": 1239
    },
    {
      "epoch": 2.202486678507993,
      "grad_norm": 0.727900505065918,
      "learning_rate": 0.00011889185666254506,
      "loss": 1.5541,
      "step": 1240
    },
    {
      "epoch": 2.2042628774422734,
      "grad_norm": 0.7705880999565125,
      "learning_rate": 0.0001187820576461784,
      "loss": 1.6435,
      "step": 1241
    },
    {
      "epoch": 2.206039076376554,
      "grad_norm": 0.7625465989112854,
      "learning_rate": 0.00011867223515348047,
      "loss": 1.5397,
      "step": 1242
    },
    {
      "epoch": 2.2078152753108347,
      "grad_norm": 0.853602409362793,
      "learning_rate": 0.00011856238932172213,
      "loss": 1.4874,
      "step": 1243
    },
    {
      "epoch": 2.2095914742451153,
      "grad_norm": 0.7511172890663147,
      "learning_rate": 0.00011845252028820337,
      "loss": 1.5489,
      "step": 1244
    },
    {
      "epoch": 2.211367673179396,
      "grad_norm": 0.74135822057724,
      "learning_rate": 0.00011834262819025326,
      "loss": 1.4701,
      "step": 1245
    },
    {
      "epoch": 2.213143872113677,
      "grad_norm": 0.7808818221092224,
      "learning_rate": 0.00011823271316522963,
      "loss": 1.8203,
      "step": 1246
    },
    {
      "epoch": 2.2149200710479575,
      "grad_norm": 0.7631354331970215,
      "learning_rate": 0.00011812277535051904,
      "loss": 1.4122,
      "step": 1247
    },
    {
      "epoch": 2.216696269982238,
      "grad_norm": 0.804875910282135,
      "learning_rate": 0.00011801281488353648,
      "loss": 1.6195,
      "step": 1248
    },
    {
      "epoch": 2.2184724689165187,
      "grad_norm": 0.829811692237854,
      "learning_rate": 0.00011790283190172527,
      "loss": 1.5464,
      "step": 1249
    },
    {
      "epoch": 2.2202486678507993,
      "grad_norm": 0.7565212249755859,
      "learning_rate": 0.00011779282654255685,
      "loss": 1.4534,
      "step": 1250
    },
    {
      "epoch": 2.22202486678508,
      "grad_norm": 0.8034217953681946,
      "learning_rate": 0.0001176827989435307,
      "loss": 1.7174,
      "step": 1251
    },
    {
      "epoch": 2.2238010657193605,
      "grad_norm": 0.8247466087341309,
      "learning_rate": 0.00011757274924217396,
      "loss": 1.8313,
      "step": 1252
    },
    {
      "epoch": 2.225577264653641,
      "grad_norm": 0.7813190221786499,
      "learning_rate": 0.00011746267757604159,
      "loss": 1.6897,
      "step": 1253
    },
    {
      "epoch": 2.2273534635879217,
      "grad_norm": 0.8119688034057617,
      "learning_rate": 0.00011735258408271585,
      "loss": 1.6695,
      "step": 1254
    },
    {
      "epoch": 2.2291296625222023,
      "grad_norm": 0.8573070168495178,
      "learning_rate": 0.00011724246889980637,
      "loss": 1.6279,
      "step": 1255
    },
    {
      "epoch": 2.230905861456483,
      "grad_norm": 0.8034690022468567,
      "learning_rate": 0.0001171323321649498,
      "loss": 1.7327,
      "step": 1256
    },
    {
      "epoch": 2.232682060390764,
      "grad_norm": 0.8863791823387146,
      "learning_rate": 0.0001170221740158098,
      "loss": 1.7336,
      "step": 1257
    },
    {
      "epoch": 2.2344582593250446,
      "grad_norm": 0.8299232721328735,
      "learning_rate": 0.00011691199459007678,
      "loss": 1.83,
      "step": 1258
    },
    {
      "epoch": 2.236234458259325,
      "grad_norm": 0.787301778793335,
      "learning_rate": 0.00011680179402546778,
      "loss": 1.5376,
      "step": 1259
    },
    {
      "epoch": 2.238010657193606,
      "grad_norm": 0.8315306901931763,
      "learning_rate": 0.0001166915724597262,
      "loss": 1.6815,
      "step": 1260
    },
    {
      "epoch": 2.2397868561278864,
      "grad_norm": 0.771234929561615,
      "learning_rate": 0.00011658133003062169,
      "loss": 1.6578,
      "step": 1261
    },
    {
      "epoch": 2.241563055062167,
      "grad_norm": 0.7992792129516602,
      "learning_rate": 0.00011647106687595004,
      "loss": 1.9378,
      "step": 1262
    },
    {
      "epoch": 2.2433392539964476,
      "grad_norm": 0.794563889503479,
      "learning_rate": 0.00011636078313353287,
      "loss": 1.7122,
      "step": 1263
    },
    {
      "epoch": 2.2451154529307282,
      "grad_norm": 0.7849447727203369,
      "learning_rate": 0.00011625047894121764,
      "loss": 1.9766,
      "step": 1264
    },
    {
      "epoch": 2.246891651865009,
      "grad_norm": 0.8288662433624268,
      "learning_rate": 0.00011614015443687722,
      "loss": 1.5337,
      "step": 1265
    },
    {
      "epoch": 2.2486678507992894,
      "grad_norm": 0.7722797393798828,
      "learning_rate": 0.00011602980975841002,
      "loss": 1.6303,
      "step": 1266
    },
    {
      "epoch": 2.25044404973357,
      "grad_norm": 0.7601892948150635,
      "learning_rate": 0.00011591944504373957,
      "loss": 1.7407,
      "step": 1267
    },
    {
      "epoch": 2.2522202486678506,
      "grad_norm": 0.8319695591926575,
      "learning_rate": 0.00011580906043081448,
      "loss": 1.6532,
      "step": 1268
    },
    {
      "epoch": 2.2539964476021312,
      "grad_norm": 0.7581084370613098,
      "learning_rate": 0.00011569865605760821,
      "loss": 1.6467,
      "step": 1269
    },
    {
      "epoch": 2.2557726465364123,
      "grad_norm": 0.7831776142120361,
      "learning_rate": 0.00011558823206211893,
      "loss": 1.8949,
      "step": 1270
    },
    {
      "epoch": 2.257548845470693,
      "grad_norm": 0.8035985827445984,
      "learning_rate": 0.00011547778858236937,
      "loss": 1.653,
      "step": 1271
    },
    {
      "epoch": 2.2593250444049735,
      "grad_norm": 0.7621026039123535,
      "learning_rate": 0.00011536732575640656,
      "loss": 1.3236,
      "step": 1272
    },
    {
      "epoch": 2.261101243339254,
      "grad_norm": 0.8022913336753845,
      "learning_rate": 0.00011525684372230175,
      "loss": 1.7235,
      "step": 1273
    },
    {
      "epoch": 2.2628774422735347,
      "grad_norm": 0.7684356570243835,
      "learning_rate": 0.00011514634261815014,
      "loss": 1.842,
      "step": 1274
    },
    {
      "epoch": 2.2646536412078153,
      "grad_norm": 0.8213761448860168,
      "learning_rate": 0.00011503582258207088,
      "loss": 1.632,
      "step": 1275
    },
    {
      "epoch": 2.266429840142096,
      "grad_norm": 0.7956170439720154,
      "learning_rate": 0.0001149252837522067,
      "loss": 1.801,
      "step": 1276
    },
    {
      "epoch": 2.2682060390763765,
      "grad_norm": 0.7959960699081421,
      "learning_rate": 0.00011481472626672378,
      "loss": 1.6015,
      "step": 1277
    },
    {
      "epoch": 2.269982238010657,
      "grad_norm": 0.7202561497688293,
      "learning_rate": 0.00011470415026381172,
      "loss": 1.4704,
      "step": 1278
    },
    {
      "epoch": 2.2717584369449377,
      "grad_norm": 0.8171687722206116,
      "learning_rate": 0.00011459355588168316,
      "loss": 1.3599,
      "step": 1279
    },
    {
      "epoch": 2.2735346358792183,
      "grad_norm": 0.7952979803085327,
      "learning_rate": 0.00011448294325857388,
      "loss": 1.7426,
      "step": 1280
    },
    {
      "epoch": 2.275310834813499,
      "grad_norm": 0.8114399313926697,
      "learning_rate": 0.00011437231253274225,
      "loss": 1.7437,
      "step": 1281
    },
    {
      "epoch": 2.2770870337477795,
      "grad_norm": 0.784125030040741,
      "learning_rate": 0.00011426166384246943,
      "loss": 1.5616,
      "step": 1282
    },
    {
      "epoch": 2.2788632326820606,
      "grad_norm": 0.7795009016990662,
      "learning_rate": 0.00011415099732605896,
      "loss": 1.7683,
      "step": 1283
    },
    {
      "epoch": 2.280639431616341,
      "grad_norm": 0.8391236662864685,
      "learning_rate": 0.00011404031312183663,
      "loss": 1.7944,
      "step": 1284
    },
    {
      "epoch": 2.282415630550622,
      "grad_norm": 0.8133367300033569,
      "learning_rate": 0.00011392961136815045,
      "loss": 1.8428,
      "step": 1285
    },
    {
      "epoch": 2.2841918294849024,
      "grad_norm": 0.8442601561546326,
      "learning_rate": 0.00011381889220337025,
      "loss": 1.5586,
      "step": 1286
    },
    {
      "epoch": 2.285968028419183,
      "grad_norm": 0.7565037608146667,
      "learning_rate": 0.00011370815576588771,
      "loss": 1.8618,
      "step": 1287
    },
    {
      "epoch": 2.2877442273534636,
      "grad_norm": 0.7751880288124084,
      "learning_rate": 0.00011359740219411603,
      "loss": 1.5324,
      "step": 1288
    },
    {
      "epoch": 2.289520426287744,
      "grad_norm": 0.8930859565734863,
      "learning_rate": 0.00011348663162648987,
      "loss": 1.6955,
      "step": 1289
    },
    {
      "epoch": 2.291296625222025,
      "grad_norm": 0.8195274472236633,
      "learning_rate": 0.0001133758442014651,
      "loss": 1.634,
      "step": 1290
    },
    {
      "epoch": 2.2930728241563054,
      "grad_norm": 0.8087868690490723,
      "learning_rate": 0.00011326504005751875,
      "loss": 1.5987,
      "step": 1291
    },
    {
      "epoch": 2.294849023090586,
      "grad_norm": 0.8303581476211548,
      "learning_rate": 0.0001131542193331486,
      "loss": 1.7612,
      "step": 1292
    },
    {
      "epoch": 2.2966252220248666,
      "grad_norm": 0.7955389022827148,
      "learning_rate": 0.00011304338216687331,
      "loss": 1.8225,
      "step": 1293
    },
    {
      "epoch": 2.2984014209591472,
      "grad_norm": 0.7876299619674683,
      "learning_rate": 0.00011293252869723194,
      "loss": 1.8141,
      "step": 1294
    },
    {
      "epoch": 2.300177619893428,
      "grad_norm": 0.7613785266876221,
      "learning_rate": 0.00011282165906278401,
      "loss": 1.5903,
      "step": 1295
    },
    {
      "epoch": 2.301953818827709,
      "grad_norm": 0.78912752866745,
      "learning_rate": 0.00011271077340210932,
      "loss": 1.7253,
      "step": 1296
    },
    {
      "epoch": 2.3037300177619895,
      "grad_norm": 0.7801281213760376,
      "learning_rate": 0.00011259987185380754,
      "loss": 1.6017,
      "step": 1297
    },
    {
      "epoch": 2.30550621669627,
      "grad_norm": 0.8713527917861938,
      "learning_rate": 0.00011248895455649832,
      "loss": 1.6557,
      "step": 1298
    },
    {
      "epoch": 2.3072824156305507,
      "grad_norm": 0.8049923777580261,
      "learning_rate": 0.00011237802164882096,
      "loss": 1.8596,
      "step": 1299
    },
    {
      "epoch": 2.3090586145648313,
      "grad_norm": 0.8098387718200684,
      "learning_rate": 0.0001122670732694342,
      "loss": 1.7324,
      "step": 1300
    },
    {
      "epoch": 2.3090586145648313,
      "eval_loss": 1.9445518255233765,
      "eval_runtime": 17.3293,
      "eval_samples_per_second": 57.763,
      "eval_steps_per_second": 28.911,
      "step": 1300
    },
    {
      "epoch": 2.310834813499112,
      "grad_norm": 0.8369006514549255,
      "learning_rate": 0.00011215610955701628,
      "loss": 1.7308,
      "step": 1301
    },
    {
      "epoch": 2.3126110124333925,
      "grad_norm": 0.8661454319953918,
      "learning_rate": 0.00011204513065026443,
      "loss": 1.5638,
      "step": 1302
    },
    {
      "epoch": 2.314387211367673,
      "grad_norm": 0.8115090131759644,
      "learning_rate": 0.00011193413668789501,
      "loss": 1.7015,
      "step": 1303
    },
    {
      "epoch": 2.3161634103019537,
      "grad_norm": 0.8000757694244385,
      "learning_rate": 0.00011182312780864311,
      "loss": 1.4766,
      "step": 1304
    },
    {
      "epoch": 2.3179396092362343,
      "grad_norm": 0.8436541557312012,
      "learning_rate": 0.00011171210415126247,
      "loss": 1.96,
      "step": 1305
    },
    {
      "epoch": 2.319715808170515,
      "grad_norm": 0.7757478356361389,
      "learning_rate": 0.00011160106585452538,
      "loss": 1.6267,
      "step": 1306
    },
    {
      "epoch": 2.321492007104796,
      "grad_norm": 0.8250178694725037,
      "learning_rate": 0.00011149001305722235,
      "loss": 1.528,
      "step": 1307
    },
    {
      "epoch": 2.323268206039076,
      "grad_norm": 0.8125702738761902,
      "learning_rate": 0.00011137894589816205,
      "loss": 1.7022,
      "step": 1308
    },
    {
      "epoch": 2.325044404973357,
      "grad_norm": 0.7810602188110352,
      "learning_rate": 0.00011126786451617108,
      "loss": 1.8183,
      "step": 1309
    },
    {
      "epoch": 2.326820603907638,
      "grad_norm": 0.7047934532165527,
      "learning_rate": 0.00011115676905009385,
      "loss": 1.2712,
      "step": 1310
    },
    {
      "epoch": 2.3285968028419184,
      "grad_norm": 0.8456729650497437,
      "learning_rate": 0.00011104565963879232,
      "loss": 1.6075,
      "step": 1311
    },
    {
      "epoch": 2.330373001776199,
      "grad_norm": 0.7984141707420349,
      "learning_rate": 0.00011093453642114595,
      "loss": 1.7102,
      "step": 1312
    },
    {
      "epoch": 2.3321492007104796,
      "grad_norm": 0.8199589252471924,
      "learning_rate": 0.00011082339953605137,
      "loss": 1.5799,
      "step": 1313
    },
    {
      "epoch": 2.33392539964476,
      "grad_norm": 0.7996301054954529,
      "learning_rate": 0.00011071224912242242,
      "loss": 1.5979,
      "step": 1314
    },
    {
      "epoch": 2.335701598579041,
      "grad_norm": 0.8563810586929321,
      "learning_rate": 0.00011060108531918971,
      "loss": 1.5904,
      "step": 1315
    },
    {
      "epoch": 2.3374777975133214,
      "grad_norm": 0.8053058385848999,
      "learning_rate": 0.00011048990826530068,
      "loss": 1.4942,
      "step": 1316
    },
    {
      "epoch": 2.339253996447602,
      "grad_norm": 0.8730599284172058,
      "learning_rate": 0.00011037871809971929,
      "loss": 1.6085,
      "step": 1317
    },
    {
      "epoch": 2.3410301953818826,
      "grad_norm": 0.7958142757415771,
      "learning_rate": 0.00011026751496142588,
      "loss": 1.3208,
      "step": 1318
    },
    {
      "epoch": 2.342806394316163,
      "grad_norm": 0.7661446928977966,
      "learning_rate": 0.00011015629898941708,
      "loss": 1.4287,
      "step": 1319
    },
    {
      "epoch": 2.3445825932504443,
      "grad_norm": 0.8351251482963562,
      "learning_rate": 0.00011004507032270553,
      "loss": 1.6892,
      "step": 1320
    },
    {
      "epoch": 2.346358792184725,
      "grad_norm": 0.8389258980751038,
      "learning_rate": 0.00010993382910031961,
      "loss": 1.6776,
      "step": 1321
    },
    {
      "epoch": 2.3481349911190055,
      "grad_norm": 0.8180783987045288,
      "learning_rate": 0.00010982257546130359,
      "loss": 1.7593,
      "step": 1322
    },
    {
      "epoch": 2.349911190053286,
      "grad_norm": 0.7853378653526306,
      "learning_rate": 0.0001097113095447171,
      "loss": 1.4079,
      "step": 1323
    },
    {
      "epoch": 2.3516873889875667,
      "grad_norm": 0.8520334362983704,
      "learning_rate": 0.00010960003148963522,
      "loss": 1.788,
      "step": 1324
    },
    {
      "epoch": 2.3534635879218473,
      "grad_norm": 0.8755066394805908,
      "learning_rate": 0.0001094887414351482,
      "loss": 1.7135,
      "step": 1325
    },
    {
      "epoch": 2.355239786856128,
      "grad_norm": 0.802966296672821,
      "learning_rate": 0.00010937743952036121,
      "loss": 1.6846,
      "step": 1326
    },
    {
      "epoch": 2.3570159857904085,
      "grad_norm": 0.7838170528411865,
      "learning_rate": 0.0001092661258843943,
      "loss": 1.7831,
      "step": 1327
    },
    {
      "epoch": 2.358792184724689,
      "grad_norm": 0.8095408082008362,
      "learning_rate": 0.00010915480066638217,
      "loss": 1.6565,
      "step": 1328
    },
    {
      "epoch": 2.3605683836589697,
      "grad_norm": 0.8091537356376648,
      "learning_rate": 0.00010904346400547398,
      "loss": 1.4894,
      "step": 1329
    },
    {
      "epoch": 2.3623445825932503,
      "grad_norm": 0.7872226238250732,
      "learning_rate": 0.00010893211604083324,
      "loss": 1.5195,
      "step": 1330
    },
    {
      "epoch": 2.364120781527531,
      "grad_norm": 0.7547116279602051,
      "learning_rate": 0.00010882075691163748,
      "loss": 1.5955,
      "step": 1331
    },
    {
      "epoch": 2.3658969804618115,
      "grad_norm": 0.8004018664360046,
      "learning_rate": 0.00010870938675707831,
      "loss": 1.8478,
      "step": 1332
    },
    {
      "epoch": 2.3676731793960926,
      "grad_norm": 0.8023124933242798,
      "learning_rate": 0.00010859800571636105,
      "loss": 1.7772,
      "step": 1333
    },
    {
      "epoch": 2.369449378330373,
      "grad_norm": 0.7977077960968018,
      "learning_rate": 0.00010848661392870463,
      "loss": 1.6249,
      "step": 1334
    },
    {
      "epoch": 2.3712255772646538,
      "grad_norm": 0.8681474328041077,
      "learning_rate": 0.00010837521153334143,
      "loss": 1.5157,
      "step": 1335
    },
    {
      "epoch": 2.3730017761989344,
      "grad_norm": 0.7852376103401184,
      "learning_rate": 0.00010826379866951708,
      "loss": 1.7975,
      "step": 1336
    },
    {
      "epoch": 2.374777975133215,
      "grad_norm": 0.8330531120300293,
      "learning_rate": 0.00010815237547649036,
      "loss": 1.7491,
      "step": 1337
    },
    {
      "epoch": 2.3765541740674956,
      "grad_norm": 0.7955513596534729,
      "learning_rate": 0.00010804094209353281,
      "loss": 1.7705,
      "step": 1338
    },
    {
      "epoch": 2.378330373001776,
      "grad_norm": 0.803220272064209,
      "learning_rate": 0.00010792949865992885,
      "loss": 1.5816,
      "step": 1339
    },
    {
      "epoch": 2.380106571936057,
      "grad_norm": 0.8280730247497559,
      "learning_rate": 0.0001078180453149754,
      "loss": 1.6638,
      "step": 1340
    },
    {
      "epoch": 2.3818827708703374,
      "grad_norm": 0.8504760265350342,
      "learning_rate": 0.00010770658219798178,
      "loss": 1.6655,
      "step": 1341
    },
    {
      "epoch": 2.383658969804618,
      "grad_norm": 0.8529512882232666,
      "learning_rate": 0.00010759510944826951,
      "loss": 1.6086,
      "step": 1342
    },
    {
      "epoch": 2.3854351687388986,
      "grad_norm": 0.8196860551834106,
      "learning_rate": 0.00010748362720517217,
      "loss": 1.8856,
      "step": 1343
    },
    {
      "epoch": 2.387211367673179,
      "grad_norm": 0.8136743903160095,
      "learning_rate": 0.0001073721356080352,
      "loss": 1.5054,
      "step": 1344
    },
    {
      "epoch": 2.38898756660746,
      "grad_norm": 0.8062311410903931,
      "learning_rate": 0.00010726063479621573,
      "loss": 1.6915,
      "step": 1345
    },
    {
      "epoch": 2.390763765541741,
      "grad_norm": 0.787847101688385,
      "learning_rate": 0.00010714912490908244,
      "loss": 1.6641,
      "step": 1346
    },
    {
      "epoch": 2.3925399644760215,
      "grad_norm": 0.8227165341377258,
      "learning_rate": 0.00010703760608601528,
      "loss": 1.8333,
      "step": 1347
    },
    {
      "epoch": 2.394316163410302,
      "grad_norm": 0.8105635046958923,
      "learning_rate": 0.00010692607846640544,
      "loss": 1.531,
      "step": 1348
    },
    {
      "epoch": 2.3960923623445827,
      "grad_norm": 0.8227869868278503,
      "learning_rate": 0.00010681454218965505,
      "loss": 1.8755,
      "step": 1349
    },
    {
      "epoch": 2.3978685612788633,
      "grad_norm": 0.8108421564102173,
      "learning_rate": 0.00010670299739517708,
      "loss": 1.5623,
      "step": 1350
    },
    {
      "epoch": 2.399644760213144,
      "grad_norm": 0.8452847003936768,
      "learning_rate": 0.00010659144422239519,
      "loss": 1.4753,
      "step": 1351
    },
    {
      "epoch": 2.4014209591474245,
      "grad_norm": 0.8384227752685547,
      "learning_rate": 0.00010647988281074346,
      "loss": 1.3671,
      "step": 1352
    },
    {
      "epoch": 2.403197158081705,
      "grad_norm": 0.848961353302002,
      "learning_rate": 0.0001063683132996663,
      "loss": 1.795,
      "step": 1353
    },
    {
      "epoch": 2.4049733570159857,
      "grad_norm": 0.8396725654602051,
      "learning_rate": 0.00010625673582861822,
      "loss": 1.5249,
      "step": 1354
    },
    {
      "epoch": 2.4067495559502663,
      "grad_norm": 0.7942584156990051,
      "learning_rate": 0.00010614515053706367,
      "loss": 1.8206,
      "step": 1355
    },
    {
      "epoch": 2.408525754884547,
      "grad_norm": 0.803523063659668,
      "learning_rate": 0.00010603355756447691,
      "loss": 1.6405,
      "step": 1356
    },
    {
      "epoch": 2.410301953818828,
      "grad_norm": 0.8785330057144165,
      "learning_rate": 0.00010592195705034176,
      "loss": 1.7895,
      "step": 1357
    },
    {
      "epoch": 2.412078152753108,
      "grad_norm": 0.8523733019828796,
      "learning_rate": 0.00010581034913415153,
      "loss": 1.8603,
      "step": 1358
    },
    {
      "epoch": 2.413854351687389,
      "grad_norm": 0.8712688088417053,
      "learning_rate": 0.00010569873395540873,
      "loss": 1.6722,
      "step": 1359
    },
    {
      "epoch": 2.4156305506216698,
      "grad_norm": 0.8848590850830078,
      "learning_rate": 0.00010558711165362492,
      "loss": 1.7657,
      "step": 1360
    },
    {
      "epoch": 2.4174067495559504,
      "grad_norm": 0.862893283367157,
      "learning_rate": 0.00010547548236832064,
      "loss": 1.711,
      "step": 1361
    },
    {
      "epoch": 2.419182948490231,
      "grad_norm": 0.8605526685714722,
      "learning_rate": 0.00010536384623902513,
      "loss": 1.6951,
      "step": 1362
    },
    {
      "epoch": 2.4209591474245116,
      "grad_norm": 0.8458767533302307,
      "learning_rate": 0.00010525220340527614,
      "loss": 1.8492,
      "step": 1363
    },
    {
      "epoch": 2.422735346358792,
      "grad_norm": 0.8029478192329407,
      "learning_rate": 0.0001051405540066199,
      "loss": 1.4878,
      "step": 1364
    },
    {
      "epoch": 2.424511545293073,
      "grad_norm": 0.7924934029579163,
      "learning_rate": 0.00010502889818261075,
      "loss": 1.6432,
      "step": 1365
    },
    {
      "epoch": 2.4262877442273534,
      "grad_norm": 0.7824540138244629,
      "learning_rate": 0.00010491723607281104,
      "loss": 1.608,
      "step": 1366
    },
    {
      "epoch": 2.428063943161634,
      "grad_norm": 0.8258391618728638,
      "learning_rate": 0.00010480556781679111,
      "loss": 1.6398,
      "step": 1367
    },
    {
      "epoch": 2.4298401420959146,
      "grad_norm": 0.7917469143867493,
      "learning_rate": 0.00010469389355412886,
      "loss": 1.9512,
      "step": 1368
    },
    {
      "epoch": 2.431616341030195,
      "grad_norm": 0.8897488713264465,
      "learning_rate": 0.00010458221342440977,
      "loss": 1.3558,
      "step": 1369
    },
    {
      "epoch": 2.4333925399644762,
      "grad_norm": 0.7898529171943665,
      "learning_rate": 0.0001044705275672266,
      "loss": 1.6248,
      "step": 1370
    },
    {
      "epoch": 2.435168738898757,
      "grad_norm": 0.8160329461097717,
      "learning_rate": 0.00010435883612217927,
      "loss": 1.5773,
      "step": 1371
    },
    {
      "epoch": 2.4369449378330375,
      "grad_norm": 0.8926382064819336,
      "learning_rate": 0.00010424713922887477,
      "loss": 1.8302,
      "step": 1372
    },
    {
      "epoch": 2.438721136767318,
      "grad_norm": 0.7882575392723083,
      "learning_rate": 0.00010413543702692679,
      "loss": 1.2886,
      "step": 1373
    },
    {
      "epoch": 2.4404973357015987,
      "grad_norm": 0.8510245680809021,
      "learning_rate": 0.00010402372965595571,
      "loss": 1.6854,
      "step": 1374
    },
    {
      "epoch": 2.4422735346358793,
      "grad_norm": 0.7967774868011475,
      "learning_rate": 0.00010391201725558841,
      "loss": 1.6837,
      "step": 1375
    },
    {
      "epoch": 2.44404973357016,
      "grad_norm": 0.7589784264564514,
      "learning_rate": 0.00010380029996545795,
      "loss": 1.7333,
      "step": 1376
    },
    {
      "epoch": 2.4458259325044405,
      "grad_norm": 0.7285872101783752,
      "learning_rate": 0.00010368857792520357,
      "loss": 1.3355,
      "step": 1377
    },
    {
      "epoch": 2.447602131438721,
      "grad_norm": 0.7838412523269653,
      "learning_rate": 0.0001035768512744705,
      "loss": 1.5571,
      "step": 1378
    },
    {
      "epoch": 2.4493783303730017,
      "grad_norm": 0.8520485162734985,
      "learning_rate": 0.0001034651201529096,
      "loss": 1.8353,
      "step": 1379
    },
    {
      "epoch": 2.4511545293072823,
      "grad_norm": 0.8350341320037842,
      "learning_rate": 0.00010335338470017742,
      "loss": 1.6029,
      "step": 1380
    },
    {
      "epoch": 2.452930728241563,
      "grad_norm": 0.8160874843597412,
      "learning_rate": 0.0001032416450559359,
      "loss": 1.8337,
      "step": 1381
    },
    {
      "epoch": 2.4547069271758435,
      "grad_norm": 0.8271399140357971,
      "learning_rate": 0.00010312990135985219,
      "loss": 1.3031,
      "step": 1382
    },
    {
      "epoch": 2.4564831261101245,
      "grad_norm": 0.805717945098877,
      "learning_rate": 0.00010301815375159855,
      "loss": 1.8369,
      "step": 1383
    },
    {
      "epoch": 2.458259325044405,
      "grad_norm": 0.7918758988380432,
      "learning_rate": 0.00010290640237085211,
      "loss": 1.8112,
      "step": 1384
    },
    {
      "epoch": 2.4600355239786857,
      "grad_norm": 0.828177809715271,
      "learning_rate": 0.00010279464735729471,
      "loss": 1.4525,
      "step": 1385
    },
    {
      "epoch": 2.4618117229129663,
      "grad_norm": 0.7829956412315369,
      "learning_rate": 0.00010268288885061275,
      "loss": 1.3354,
      "step": 1386
    },
    {
      "epoch": 2.463587921847247,
      "grad_norm": 0.7542052865028381,
      "learning_rate": 0.00010257112699049694,
      "loss": 1.6118,
      "step": 1387
    },
    {
      "epoch": 2.4653641207815276,
      "grad_norm": 0.848281741142273,
      "learning_rate": 0.00010245936191664225,
      "loss": 1.8757,
      "step": 1388
    },
    {
      "epoch": 2.467140319715808,
      "grad_norm": 0.8370591402053833,
      "learning_rate": 0.00010234759376874763,
      "loss": 1.4374,
      "step": 1389
    },
    {
      "epoch": 2.4689165186500888,
      "grad_norm": 0.8233585357666016,
      "learning_rate": 0.00010223582268651586,
      "loss": 1.8281,
      "step": 1390
    },
    {
      "epoch": 2.4706927175843694,
      "grad_norm": 0.7809579372406006,
      "learning_rate": 0.00010212404880965348,
      "loss": 1.342,
      "step": 1391
    },
    {
      "epoch": 2.47246891651865,
      "grad_norm": 0.829315721988678,
      "learning_rate": 0.00010201227227787037,
      "loss": 1.5784,
      "step": 1392
    },
    {
      "epoch": 2.4742451154529306,
      "grad_norm": 0.8515288829803467,
      "learning_rate": 0.0001019004932308799,
      "loss": 1.59,
      "step": 1393
    },
    {
      "epoch": 2.476021314387211,
      "grad_norm": 0.8028129935264587,
      "learning_rate": 0.00010178871180839838,
      "loss": 1.31,
      "step": 1394
    },
    {
      "epoch": 2.477797513321492,
      "grad_norm": 0.8146170377731323,
      "learning_rate": 0.00010167692815014526,
      "loss": 1.5392,
      "step": 1395
    },
    {
      "epoch": 2.479573712255773,
      "grad_norm": 0.8916728496551514,
      "learning_rate": 0.00010156514239584276,
      "loss": 1.6096,
      "step": 1396
    },
    {
      "epoch": 2.4813499111900534,
      "grad_norm": 0.8370887041091919,
      "learning_rate": 0.00010145335468521568,
      "loss": 1.8438,
      "step": 1397
    },
    {
      "epoch": 2.483126110124334,
      "grad_norm": 0.7930785417556763,
      "learning_rate": 0.0001013415651579912,
      "loss": 1.4978,
      "step": 1398
    },
    {
      "epoch": 2.4849023090586146,
      "grad_norm": 0.9280103445053101,
      "learning_rate": 0.00010122977395389894,
      "loss": 1.6172,
      "step": 1399
    },
    {
      "epoch": 2.4866785079928952,
      "grad_norm": 0.8435252904891968,
      "learning_rate": 0.00010111798121267047,
      "loss": 1.504,
      "step": 1400
    },
    {
      "epoch": 2.4866785079928952,
      "eval_loss": 1.9492541551589966,
      "eval_runtime": 17.6038,
      "eval_samples_per_second": 56.863,
      "eval_steps_per_second": 28.46,
      "step": 1400
    },
    {
      "epoch": 2.488454706927176,
      "grad_norm": 0.8478225469589233,
      "learning_rate": 0.00010100618707403935,
      "loss": 1.6767,
      "step": 1401
    },
    {
      "epoch": 2.4902309058614565,
      "grad_norm": 0.8476981520652771,
      "learning_rate": 0.00010089439167774085,
      "loss": 1.7257,
      "step": 1402
    },
    {
      "epoch": 2.492007104795737,
      "grad_norm": 0.8842042684555054,
      "learning_rate": 0.00010078259516351183,
      "loss": 1.9027,
      "step": 1403
    },
    {
      "epoch": 2.4937833037300177,
      "grad_norm": 0.8460977673530579,
      "learning_rate": 0.00010067079767109055,
      "loss": 1.6405,
      "step": 1404
    },
    {
      "epoch": 2.4955595026642983,
      "grad_norm": 0.9021136164665222,
      "learning_rate": 0.00010055899934021649,
      "loss": 1.8614,
      "step": 1405
    },
    {
      "epoch": 2.497335701598579,
      "grad_norm": 0.757568895816803,
      "learning_rate": 0.0001004472003106302,
      "loss": 1.5899,
      "step": 1406
    },
    {
      "epoch": 2.49911190053286,
      "grad_norm": 0.8455844521522522,
      "learning_rate": 0.00010033540072207306,
      "loss": 1.7307,
      "step": 1407
    },
    {
      "epoch": 2.50088809946714,
      "grad_norm": 0.8450827598571777,
      "learning_rate": 0.00010022360071428718,
      "loss": 1.8223,
      "step": 1408
    },
    {
      "epoch": 2.502664298401421,
      "grad_norm": 0.8712546825408936,
      "learning_rate": 0.00010011180042701517,
      "loss": 1.8187,
      "step": 1409
    },
    {
      "epoch": 2.5044404973357017,
      "grad_norm": 0.8496801853179932,
      "learning_rate": 0.0001,
      "loss": 1.8313,
      "step": 1410
    },
    {
      "epoch": 2.5062166962699823,
      "grad_norm": 0.7993433475494385,
      "learning_rate": 9.988819957298486e-05,
      "loss": 1.6998,
      "step": 1411
    },
    {
      "epoch": 2.507992895204263,
      "grad_norm": 0.806938886642456,
      "learning_rate": 9.977639928571284e-05,
      "loss": 1.6937,
      "step": 1412
    },
    {
      "epoch": 2.5097690941385435,
      "grad_norm": 0.8632665276527405,
      "learning_rate": 9.966459927792697e-05,
      "loss": 1.5775,
      "step": 1413
    },
    {
      "epoch": 2.511545293072824,
      "grad_norm": 0.8931413292884827,
      "learning_rate": 9.955279968936983e-05,
      "loss": 1.6798,
      "step": 1414
    },
    {
      "epoch": 2.5133214920071048,
      "grad_norm": 0.8298978805541992,
      "learning_rate": 9.94410006597835e-05,
      "loss": 1.5709,
      "step": 1415
    },
    {
      "epoch": 2.5150976909413854,
      "grad_norm": 0.888143002986908,
      "learning_rate": 9.932920232890947e-05,
      "loss": 1.8853,
      "step": 1416
    },
    {
      "epoch": 2.516873889875666,
      "grad_norm": 0.7951931357383728,
      "learning_rate": 9.921740483648821e-05,
      "loss": 1.5548,
      "step": 1417
    },
    {
      "epoch": 2.5186500888099466,
      "grad_norm": 0.8722448348999023,
      "learning_rate": 9.910560832225919e-05,
      "loss": 1.4573,
      "step": 1418
    },
    {
      "epoch": 2.520426287744227,
      "grad_norm": 0.8844214677810669,
      "learning_rate": 9.899381292596068e-05,
      "loss": 1.727,
      "step": 1419
    },
    {
      "epoch": 2.522202486678508,
      "grad_norm": 0.818045973777771,
      "learning_rate": 9.888201878732955e-05,
      "loss": 1.6497,
      "step": 1420
    },
    {
      "epoch": 2.5239786856127884,
      "grad_norm": 0.8209692239761353,
      "learning_rate": 9.877022604610109e-05,
      "loss": 1.5025,
      "step": 1421
    },
    {
      "epoch": 2.5257548845470694,
      "grad_norm": 0.8494873642921448,
      "learning_rate": 9.865843484200879e-05,
      "loss": 1.8212,
      "step": 1422
    },
    {
      "epoch": 2.52753108348135,
      "grad_norm": 0.8561432957649231,
      "learning_rate": 9.854664531478434e-05,
      "loss": 1.6825,
      "step": 1423
    },
    {
      "epoch": 2.5293072824156306,
      "grad_norm": 0.7965608835220337,
      "learning_rate": 9.843485760415724e-05,
      "loss": 1.7907,
      "step": 1424
    },
    {
      "epoch": 2.5310834813499112,
      "grad_norm": 0.8450865149497986,
      "learning_rate": 9.832307184985475e-05,
      "loss": 1.7295,
      "step": 1425
    },
    {
      "epoch": 2.532859680284192,
      "grad_norm": 0.8460333943367004,
      "learning_rate": 9.821128819160164e-05,
      "loss": 1.3938,
      "step": 1426
    },
    {
      "epoch": 2.5346358792184724,
      "grad_norm": 0.8162291646003723,
      "learning_rate": 9.809950676912017e-05,
      "loss": 1.8413,
      "step": 1427
    },
    {
      "epoch": 2.536412078152753,
      "grad_norm": 0.7576467394828796,
      "learning_rate": 9.798772772212962e-05,
      "loss": 1.6116,
      "step": 1428
    },
    {
      "epoch": 2.5381882770870337,
      "grad_norm": 0.860646665096283,
      "learning_rate": 9.787595119034655e-05,
      "loss": 1.7151,
      "step": 1429
    },
    {
      "epoch": 2.5399644760213143,
      "grad_norm": 0.8199255466461182,
      "learning_rate": 9.776417731348416e-05,
      "loss": 1.795,
      "step": 1430
    },
    {
      "epoch": 2.5417406749555953,
      "grad_norm": 0.8628001809120178,
      "learning_rate": 9.765240623125238e-05,
      "loss": 1.5947,
      "step": 1431
    },
    {
      "epoch": 2.5435168738898755,
      "grad_norm": 0.8064737319946289,
      "learning_rate": 9.754063808335778e-05,
      "loss": 1.5675,
      "step": 1432
    },
    {
      "epoch": 2.5452930728241565,
      "grad_norm": 0.8522127270698547,
      "learning_rate": 9.742887300950308e-05,
      "loss": 1.6925,
      "step": 1433
    },
    {
      "epoch": 2.5470692717584367,
      "grad_norm": 0.7681319117546082,
      "learning_rate": 9.731711114938729e-05,
      "loss": 1.5796,
      "step": 1434
    },
    {
      "epoch": 2.5488454706927177,
      "grad_norm": 0.8568435907363892,
      "learning_rate": 9.72053526427053e-05,
      "loss": 1.7825,
      "step": 1435
    },
    {
      "epoch": 2.5506216696269983,
      "grad_norm": 0.8437057733535767,
      "learning_rate": 9.70935976291479e-05,
      "loss": 1.4315,
      "step": 1436
    },
    {
      "epoch": 2.552397868561279,
      "grad_norm": 0.7958186864852905,
      "learning_rate": 9.698184624840147e-05,
      "loss": 1.9508,
      "step": 1437
    },
    {
      "epoch": 2.5541740674955595,
      "grad_norm": 0.9085808992385864,
      "learning_rate": 9.687009864014781e-05,
      "loss": 1.5374,
      "step": 1438
    },
    {
      "epoch": 2.55595026642984,
      "grad_norm": 0.7832688093185425,
      "learning_rate": 9.675835494406413e-05,
      "loss": 1.7321,
      "step": 1439
    },
    {
      "epoch": 2.5577264653641207,
      "grad_norm": 0.8394296169281006,
      "learning_rate": 9.66466152998226e-05,
      "loss": 1.8453,
      "step": 1440
    },
    {
      "epoch": 2.5595026642984013,
      "grad_norm": 0.8344399333000183,
      "learning_rate": 9.653487984709042e-05,
      "loss": 1.9307,
      "step": 1441
    },
    {
      "epoch": 2.561278863232682,
      "grad_norm": 0.8759585022926331,
      "learning_rate": 9.642314872552953e-05,
      "loss": 1.6754,
      "step": 1442
    },
    {
      "epoch": 2.5630550621669625,
      "grad_norm": 0.8311995267868042,
      "learning_rate": 9.631142207479644e-05,
      "loss": 1.315,
      "step": 1443
    },
    {
      "epoch": 2.5648312611012436,
      "grad_norm": 0.7764325737953186,
      "learning_rate": 9.619970003454204e-05,
      "loss": 1.6901,
      "step": 1444
    },
    {
      "epoch": 2.5666074600355238,
      "grad_norm": 0.8932809829711914,
      "learning_rate": 9.60879827444116e-05,
      "loss": 1.6091,
      "step": 1445
    },
    {
      "epoch": 2.568383658969805,
      "grad_norm": 0.8677467107772827,
      "learning_rate": 9.59762703440443e-05,
      "loss": 1.8764,
      "step": 1446
    },
    {
      "epoch": 2.5701598579040854,
      "grad_norm": 0.7743967175483704,
      "learning_rate": 9.586456297307326e-05,
      "loss": 1.588,
      "step": 1447
    },
    {
      "epoch": 2.571936056838366,
      "grad_norm": 0.8801282644271851,
      "learning_rate": 9.575286077112525e-05,
      "loss": 1.7104,
      "step": 1448
    },
    {
      "epoch": 2.5737122557726466,
      "grad_norm": 0.830073893070221,
      "learning_rate": 9.564116387782075e-05,
      "loss": 1.7018,
      "step": 1449
    },
    {
      "epoch": 2.575488454706927,
      "grad_norm": 0.8685994744300842,
      "learning_rate": 9.552947243277344e-05,
      "loss": 1.5925,
      "step": 1450
    },
    {
      "epoch": 2.577264653641208,
      "grad_norm": 0.8724719285964966,
      "learning_rate": 9.541778657559026e-05,
      "loss": 1.7395,
      "step": 1451
    },
    {
      "epoch": 2.5790408525754884,
      "grad_norm": 0.8548851013183594,
      "learning_rate": 9.530610644587117e-05,
      "loss": 1.8624,
      "step": 1452
    },
    {
      "epoch": 2.580817051509769,
      "grad_norm": 0.8636969327926636,
      "learning_rate": 9.519443218320894e-05,
      "loss": 1.5626,
      "step": 1453
    },
    {
      "epoch": 2.5825932504440496,
      "grad_norm": 0.8269128203392029,
      "learning_rate": 9.508276392718896e-05,
      "loss": 1.3033,
      "step": 1454
    },
    {
      "epoch": 2.5843694493783302,
      "grad_norm": 0.8694418668746948,
      "learning_rate": 9.497110181738929e-05,
      "loss": 1.691,
      "step": 1455
    },
    {
      "epoch": 2.586145648312611,
      "grad_norm": 0.8592385053634644,
      "learning_rate": 9.485944599338012e-05,
      "loss": 1.6766,
      "step": 1456
    },
    {
      "epoch": 2.587921847246892,
      "grad_norm": 0.8920082449913025,
      "learning_rate": 9.474779659472385e-05,
      "loss": 1.8122,
      "step": 1457
    },
    {
      "epoch": 2.589698046181172,
      "grad_norm": 0.8448912501335144,
      "learning_rate": 9.463615376097489e-05,
      "loss": 1.7353,
      "step": 1458
    },
    {
      "epoch": 2.591474245115453,
      "grad_norm": 0.7773298621177673,
      "learning_rate": 9.45245176316794e-05,
      "loss": 1.6651,
      "step": 1459
    },
    {
      "epoch": 2.5932504440497337,
      "grad_norm": 0.9364956617355347,
      "learning_rate": 9.441288834637509e-05,
      "loss": 1.4783,
      "step": 1460
    },
    {
      "epoch": 2.5950266429840143,
      "grad_norm": 0.8652542233467102,
      "learning_rate": 9.430126604459131e-05,
      "loss": 1.5187,
      "step": 1461
    },
    {
      "epoch": 2.596802841918295,
      "grad_norm": 0.7689319252967834,
      "learning_rate": 9.41896508658485e-05,
      "loss": 1.2226,
      "step": 1462
    },
    {
      "epoch": 2.5985790408525755,
      "grad_norm": 0.844535768032074,
      "learning_rate": 9.407804294965829e-05,
      "loss": 1.6152,
      "step": 1463
    },
    {
      "epoch": 2.600355239786856,
      "grad_norm": 0.9316681623458862,
      "learning_rate": 9.396644243552313e-05,
      "loss": 1.3524,
      "step": 1464
    },
    {
      "epoch": 2.6021314387211367,
      "grad_norm": 0.862743079662323,
      "learning_rate": 9.385484946293637e-05,
      "loss": 1.5532,
      "step": 1465
    },
    {
      "epoch": 2.6039076376554173,
      "grad_norm": 0.9435526728630066,
      "learning_rate": 9.374326417138183e-05,
      "loss": 1.5869,
      "step": 1466
    },
    {
      "epoch": 2.605683836589698,
      "grad_norm": 0.8262038826942444,
      "learning_rate": 9.363168670033372e-05,
      "loss": 1.4324,
      "step": 1467
    },
    {
      "epoch": 2.6074600355239785,
      "grad_norm": 1.0070301294326782,
      "learning_rate": 9.352011718925656e-05,
      "loss": 1.4247,
      "step": 1468
    },
    {
      "epoch": 2.609236234458259,
      "grad_norm": 0.8710716962814331,
      "learning_rate": 9.340855577760484e-05,
      "loss": 1.749,
      "step": 1469
    },
    {
      "epoch": 2.61101243339254,
      "grad_norm": 0.8519285321235657,
      "learning_rate": 9.329700260482293e-05,
      "loss": 1.6613,
      "step": 1470
    },
    {
      "epoch": 2.6127886323268203,
      "grad_norm": 0.8889058828353882,
      "learning_rate": 9.318545781034498e-05,
      "loss": 1.6171,
      "step": 1471
    },
    {
      "epoch": 2.6145648312611014,
      "grad_norm": 0.8208001852035522,
      "learning_rate": 9.30739215335946e-05,
      "loss": 1.747,
      "step": 1472
    },
    {
      "epoch": 2.616341030195382,
      "grad_norm": 0.9103639125823975,
      "learning_rate": 9.296239391398472e-05,
      "loss": 1.828,
      "step": 1473
    },
    {
      "epoch": 2.6181172291296626,
      "grad_norm": 0.9303703904151917,
      "learning_rate": 9.285087509091758e-05,
      "loss": 1.6195,
      "step": 1474
    },
    {
      "epoch": 2.619893428063943,
      "grad_norm": 0.8669819235801697,
      "learning_rate": 9.273936520378428e-05,
      "loss": 1.5917,
      "step": 1475
    },
    {
      "epoch": 2.621669626998224,
      "grad_norm": 0.8313897252082825,
      "learning_rate": 9.262786439196483e-05,
      "loss": 1.6816,
      "step": 1476
    },
    {
      "epoch": 2.6234458259325044,
      "grad_norm": 0.8169348239898682,
      "learning_rate": 9.251637279482784e-05,
      "loss": 1.4982,
      "step": 1477
    },
    {
      "epoch": 2.625222024866785,
      "grad_norm": 0.8742682933807373,
      "learning_rate": 9.240489055173052e-05,
      "loss": 1.6063,
      "step": 1478
    },
    {
      "epoch": 2.6269982238010656,
      "grad_norm": 0.8406792879104614,
      "learning_rate": 9.229341780201826e-05,
      "loss": 1.5421,
      "step": 1479
    },
    {
      "epoch": 2.6287744227353462,
      "grad_norm": 0.8808075189590454,
      "learning_rate": 9.218195468502461e-05,
      "loss": 1.6958,
      "step": 1480
    },
    {
      "epoch": 2.630550621669627,
      "grad_norm": 0.7392557263374329,
      "learning_rate": 9.207050134007117e-05,
      "loss": 1.559,
      "step": 1481
    },
    {
      "epoch": 2.6323268206039074,
      "grad_norm": 0.9078789949417114,
      "learning_rate": 9.195905790646722e-05,
      "loss": 1.7441,
      "step": 1482
    },
    {
      "epoch": 2.6341030195381885,
      "grad_norm": 0.8162659406661987,
      "learning_rate": 9.184762452350967e-05,
      "loss": 1.5467,
      "step": 1483
    },
    {
      "epoch": 2.6358792184724686,
      "grad_norm": 0.8756282925605774,
      "learning_rate": 9.173620133048293e-05,
      "loss": 1.5675,
      "step": 1484
    },
    {
      "epoch": 2.6376554174067497,
      "grad_norm": 0.853918731212616,
      "learning_rate": 9.162478846665861e-05,
      "loss": 2.0469,
      "step": 1485
    },
    {
      "epoch": 2.6394316163410303,
      "grad_norm": 0.878707230091095,
      "learning_rate": 9.151338607129538e-05,
      "loss": 1.5593,
      "step": 1486
    },
    {
      "epoch": 2.641207815275311,
      "grad_norm": 0.8135293126106262,
      "learning_rate": 9.140199428363897e-05,
      "loss": 1.2588,
      "step": 1487
    },
    {
      "epoch": 2.6429840142095915,
      "grad_norm": 0.8580237627029419,
      "learning_rate": 9.129061324292172e-05,
      "loss": 1.6575,
      "step": 1488
    },
    {
      "epoch": 2.644760213143872,
      "grad_norm": 0.9541330933570862,
      "learning_rate": 9.117924308836252e-05,
      "loss": 1.816,
      "step": 1489
    },
    {
      "epoch": 2.6465364120781527,
      "grad_norm": 0.8324117064476013,
      "learning_rate": 9.106788395916678e-05,
      "loss": 1.8067,
      "step": 1490
    },
    {
      "epoch": 2.6483126110124333,
      "grad_norm": 0.8491565585136414,
      "learning_rate": 9.095653599452604e-05,
      "loss": 1.843,
      "step": 1491
    },
    {
      "epoch": 2.650088809946714,
      "grad_norm": 0.9029340744018555,
      "learning_rate": 9.084519933361786e-05,
      "loss": 1.6961,
      "step": 1492
    },
    {
      "epoch": 2.6518650088809945,
      "grad_norm": 0.8126603364944458,
      "learning_rate": 9.07338741156057e-05,
      "loss": 1.6853,
      "step": 1493
    },
    {
      "epoch": 2.6536412078152756,
      "grad_norm": 0.8095827698707581,
      "learning_rate": 9.06225604796388e-05,
      "loss": 1.8452,
      "step": 1494
    },
    {
      "epoch": 2.6554174067495557,
      "grad_norm": 0.9003620147705078,
      "learning_rate": 9.051125856485184e-05,
      "loss": 1.7574,
      "step": 1495
    },
    {
      "epoch": 2.657193605683837,
      "grad_norm": 0.8177784085273743,
      "learning_rate": 9.039996851036479e-05,
      "loss": 1.6541,
      "step": 1496
    },
    {
      "epoch": 2.6589698046181174,
      "grad_norm": 0.7882513403892517,
      "learning_rate": 9.028869045528293e-05,
      "loss": 1.5821,
      "step": 1497
    },
    {
      "epoch": 2.660746003552398,
      "grad_norm": 0.7825910449028015,
      "learning_rate": 9.017742453869646e-05,
      "loss": 1.7991,
      "step": 1498
    },
    {
      "epoch": 2.6625222024866786,
      "grad_norm": 0.8697084188461304,
      "learning_rate": 9.00661708996804e-05,
      "loss": 1.7166,
      "step": 1499
    },
    {
      "epoch": 2.664298401420959,
      "grad_norm": 0.9318445920944214,
      "learning_rate": 8.99549296772945e-05,
      "loss": 1.5715,
      "step": 1500
    },
    {
      "epoch": 2.664298401420959,
      "eval_loss": 1.9433693885803223,
      "eval_runtime": 17.5222,
      "eval_samples_per_second": 57.128,
      "eval_steps_per_second": 28.592,
      "step": 1500
    },
    {
      "epoch": 2.66607460035524,
      "grad_norm": 0.7918385863304138,
      "learning_rate": 8.984370101058293e-05,
      "loss": 1.699,
      "step": 1501
    },
    {
      "epoch": 2.6678507992895204,
      "grad_norm": 0.8077159523963928,
      "learning_rate": 8.97324850385741e-05,
      "loss": 1.951,
      "step": 1502
    },
    {
      "epoch": 2.669626998223801,
      "grad_norm": 0.8512002825737,
      "learning_rate": 8.962128190028073e-05,
      "loss": 1.9131,
      "step": 1503
    },
    {
      "epoch": 2.6714031971580816,
      "grad_norm": 0.829663097858429,
      "learning_rate": 8.951009173469935e-05,
      "loss": 1.5855,
      "step": 1504
    },
    {
      "epoch": 2.673179396092362,
      "grad_norm": 0.8225855827331543,
      "learning_rate": 8.939891468081033e-05,
      "loss": 1.7412,
      "step": 1505
    },
    {
      "epoch": 2.674955595026643,
      "grad_norm": 0.8899734616279602,
      "learning_rate": 8.928775087757762e-05,
      "loss": 1.5891,
      "step": 1506
    },
    {
      "epoch": 2.676731793960924,
      "grad_norm": 1.607966423034668,
      "learning_rate": 8.917660046394864e-05,
      "loss": 1.5484,
      "step": 1507
    },
    {
      "epoch": 2.678507992895204,
      "grad_norm": 0.8157251477241516,
      "learning_rate": 8.90654635788541e-05,
      "loss": 1.8568,
      "step": 1508
    },
    {
      "epoch": 2.680284191829485,
      "grad_norm": 0.8278682827949524,
      "learning_rate": 8.895434036120769e-05,
      "loss": 1.6121,
      "step": 1509
    },
    {
      "epoch": 2.6820603907637657,
      "grad_norm": 0.9024254679679871,
      "learning_rate": 8.884323094990617e-05,
      "loss": 1.9089,
      "step": 1510
    },
    {
      "epoch": 2.6838365896980463,
      "grad_norm": 0.8565186858177185,
      "learning_rate": 8.873213548382895e-05,
      "loss": 1.736,
      "step": 1511
    },
    {
      "epoch": 2.685612788632327,
      "grad_norm": 0.8484647274017334,
      "learning_rate": 8.862105410183796e-05,
      "loss": 1.3398,
      "step": 1512
    },
    {
      "epoch": 2.6873889875666075,
      "grad_norm": 0.8429504632949829,
      "learning_rate": 8.850998694277767e-05,
      "loss": 1.5687,
      "step": 1513
    },
    {
      "epoch": 2.689165186500888,
      "grad_norm": 0.8191843032836914,
      "learning_rate": 8.839893414547465e-05,
      "loss": 1.6047,
      "step": 1514
    },
    {
      "epoch": 2.6909413854351687,
      "grad_norm": 0.7599840760231018,
      "learning_rate": 8.828789584873754e-05,
      "loss": 1.8013,
      "step": 1515
    },
    {
      "epoch": 2.6927175843694493,
      "grad_norm": 0.8686965703964233,
      "learning_rate": 8.817687219135692e-05,
      "loss": 1.6721,
      "step": 1516
    },
    {
      "epoch": 2.69449378330373,
      "grad_norm": 0.8468033671379089,
      "learning_rate": 8.806586331210502e-05,
      "loss": 1.5803,
      "step": 1517
    },
    {
      "epoch": 2.6962699822380105,
      "grad_norm": 0.7932190895080566,
      "learning_rate": 8.795486934973558e-05,
      "loss": 1.9788,
      "step": 1518
    },
    {
      "epoch": 2.698046181172291,
      "grad_norm": 0.8692934513092041,
      "learning_rate": 8.784389044298374e-05,
      "loss": 1.5117,
      "step": 1519
    },
    {
      "epoch": 2.699822380106572,
      "grad_norm": 0.8183004260063171,
      "learning_rate": 8.773292673056581e-05,
      "loss": 1.4623,
      "step": 1520
    },
    {
      "epoch": 2.7015985790408523,
      "grad_norm": 0.8337655663490295,
      "learning_rate": 8.762197835117909e-05,
      "loss": 1.4397,
      "step": 1521
    },
    {
      "epoch": 2.7033747779751334,
      "grad_norm": 0.8803383111953735,
      "learning_rate": 8.75110454435017e-05,
      "loss": 1.7686,
      "step": 1522
    },
    {
      "epoch": 2.705150976909414,
      "grad_norm": 0.8733647465705872,
      "learning_rate": 8.740012814619248e-05,
      "loss": 1.6591,
      "step": 1523
    },
    {
      "epoch": 2.7069271758436946,
      "grad_norm": 0.75514817237854,
      "learning_rate": 8.728922659789072e-05,
      "loss": 1.4234,
      "step": 1524
    },
    {
      "epoch": 2.708703374777975,
      "grad_norm": 0.8201850652694702,
      "learning_rate": 8.717834093721597e-05,
      "loss": 1.4644,
      "step": 1525
    },
    {
      "epoch": 2.710479573712256,
      "grad_norm": 0.8972424864768982,
      "learning_rate": 8.706747130276809e-05,
      "loss": 1.4692,
      "step": 1526
    },
    {
      "epoch": 2.7122557726465364,
      "grad_norm": 0.851721465587616,
      "learning_rate": 8.695661783312674e-05,
      "loss": 1.4261,
      "step": 1527
    },
    {
      "epoch": 2.714031971580817,
      "grad_norm": 0.7993935942649841,
      "learning_rate": 8.684578066685141e-05,
      "loss": 1.7247,
      "step": 1528
    },
    {
      "epoch": 2.7158081705150976,
      "grad_norm": 0.7758212685585022,
      "learning_rate": 8.673495994248127e-05,
      "loss": 1.2464,
      "step": 1529
    },
    {
      "epoch": 2.717584369449378,
      "grad_norm": 0.8420720100402832,
      "learning_rate": 8.662415579853491e-05,
      "loss": 1.5093,
      "step": 1530
    },
    {
      "epoch": 2.719360568383659,
      "grad_norm": 0.8589715361595154,
      "learning_rate": 8.651336837351013e-05,
      "loss": 1.6158,
      "step": 1531
    },
    {
      "epoch": 2.7211367673179394,
      "grad_norm": 0.8268781304359436,
      "learning_rate": 8.640259780588398e-05,
      "loss": 1.5493,
      "step": 1532
    },
    {
      "epoch": 2.7229129662522205,
      "grad_norm": 0.9516502618789673,
      "learning_rate": 8.629184423411232e-05,
      "loss": 1.6729,
      "step": 1533
    },
    {
      "epoch": 2.7246891651865006,
      "grad_norm": 0.9204826354980469,
      "learning_rate": 8.618110779662978e-05,
      "loss": 1.5931,
      "step": 1534
    },
    {
      "epoch": 2.7264653641207817,
      "grad_norm": 0.9850299954414368,
      "learning_rate": 8.607038863184957e-05,
      "loss": 1.495,
      "step": 1535
    },
    {
      "epoch": 2.7282415630550623,
      "grad_norm": 0.9185080528259277,
      "learning_rate": 8.595968687816339e-05,
      "loss": 1.7798,
      "step": 1536
    },
    {
      "epoch": 2.730017761989343,
      "grad_norm": 0.845314085483551,
      "learning_rate": 8.584900267394109e-05,
      "loss": 1.6874,
      "step": 1537
    },
    {
      "epoch": 2.7317939609236235,
      "grad_norm": 0.8879929184913635,
      "learning_rate": 8.57383361575306e-05,
      "loss": 1.7393,
      "step": 1538
    },
    {
      "epoch": 2.733570159857904,
      "grad_norm": 0.8536198139190674,
      "learning_rate": 8.562768746725777e-05,
      "loss": 1.6483,
      "step": 1539
    },
    {
      "epoch": 2.7353463587921847,
      "grad_norm": 0.8811099529266357,
      "learning_rate": 8.551705674142617e-05,
      "loss": 1.5081,
      "step": 1540
    },
    {
      "epoch": 2.7371225577264653,
      "grad_norm": 0.9160573482513428,
      "learning_rate": 8.540644411831683e-05,
      "loss": 1.748,
      "step": 1541
    },
    {
      "epoch": 2.738898756660746,
      "grad_norm": 0.8951646685600281,
      "learning_rate": 8.529584973618832e-05,
      "loss": 1.7315,
      "step": 1542
    },
    {
      "epoch": 2.7406749555950265,
      "grad_norm": 0.9154130816459656,
      "learning_rate": 8.518527373327626e-05,
      "loss": 1.6819,
      "step": 1543
    },
    {
      "epoch": 2.7424511545293075,
      "grad_norm": 0.8770776987075806,
      "learning_rate": 8.507471624779333e-05,
      "loss": 1.6456,
      "step": 1544
    },
    {
      "epoch": 2.7442273534635877,
      "grad_norm": 0.8980404734611511,
      "learning_rate": 8.496417741792913e-05,
      "loss": 1.4356,
      "step": 1545
    },
    {
      "epoch": 2.7460035523978688,
      "grad_norm": 0.8283612132072449,
      "learning_rate": 8.485365738184987e-05,
      "loss": 1.5475,
      "step": 1546
    },
    {
      "epoch": 2.7477797513321494,
      "grad_norm": 0.8833936452865601,
      "learning_rate": 8.474315627769826e-05,
      "loss": 1.7222,
      "step": 1547
    },
    {
      "epoch": 2.74955595026643,
      "grad_norm": 0.9131923317909241,
      "learning_rate": 8.463267424359345e-05,
      "loss": 1.6856,
      "step": 1548
    },
    {
      "epoch": 2.7513321492007106,
      "grad_norm": 0.8431984782218933,
      "learning_rate": 8.452221141763065e-05,
      "loss": 2.0011,
      "step": 1549
    },
    {
      "epoch": 2.753108348134991,
      "grad_norm": 0.8373619914054871,
      "learning_rate": 8.44117679378811e-05,
      "loss": 1.5053,
      "step": 1550
    },
    {
      "epoch": 2.7548845470692718,
      "grad_norm": 0.8155874013900757,
      "learning_rate": 8.430134394239182e-05,
      "loss": 1.3152,
      "step": 1551
    },
    {
      "epoch": 2.7566607460035524,
      "grad_norm": 0.8957754373550415,
      "learning_rate": 8.419093956918556e-05,
      "loss": 1.5018,
      "step": 1552
    },
    {
      "epoch": 2.758436944937833,
      "grad_norm": 0.8036988377571106,
      "learning_rate": 8.408055495626046e-05,
      "loss": 1.4358,
      "step": 1553
    },
    {
      "epoch": 2.7602131438721136,
      "grad_norm": 0.8176487684249878,
      "learning_rate": 8.397019024158999e-05,
      "loss": 1.7747,
      "step": 1554
    },
    {
      "epoch": 2.761989342806394,
      "grad_norm": 0.8200182318687439,
      "learning_rate": 8.385984556312281e-05,
      "loss": 1.5257,
      "step": 1555
    },
    {
      "epoch": 2.763765541740675,
      "grad_norm": 0.8520057201385498,
      "learning_rate": 8.374952105878243e-05,
      "loss": 1.8752,
      "step": 1556
    },
    {
      "epoch": 2.765541740674956,
      "grad_norm": 0.8803423047065735,
      "learning_rate": 8.363921686646713e-05,
      "loss": 1.663,
      "step": 1557
    },
    {
      "epoch": 2.767317939609236,
      "grad_norm": 0.8125527501106262,
      "learning_rate": 8.352893312405e-05,
      "loss": 1.6481,
      "step": 1558
    },
    {
      "epoch": 2.769094138543517,
      "grad_norm": 0.9041139483451843,
      "learning_rate": 8.341866996937835e-05,
      "loss": 1.4711,
      "step": 1559
    },
    {
      "epoch": 2.7708703374777977,
      "grad_norm": 0.8892507553100586,
      "learning_rate": 8.330842754027382e-05,
      "loss": 1.5079,
      "step": 1560
    },
    {
      "epoch": 2.7726465364120783,
      "grad_norm": 0.8268682360649109,
      "learning_rate": 8.319820597453226e-05,
      "loss": 1.4294,
      "step": 1561
    },
    {
      "epoch": 2.774422735346359,
      "grad_norm": 0.880126416683197,
      "learning_rate": 8.308800540992324e-05,
      "loss": 1.8064,
      "step": 1562
    },
    {
      "epoch": 2.7761989342806395,
      "grad_norm": 0.8841212391853333,
      "learning_rate": 8.297782598419025e-05,
      "loss": 1.3619,
      "step": 1563
    },
    {
      "epoch": 2.77797513321492,
      "grad_norm": 0.9097767472267151,
      "learning_rate": 8.286766783505022e-05,
      "loss": 1.7629,
      "step": 1564
    },
    {
      "epoch": 2.7797513321492007,
      "grad_norm": 0.8674019575119019,
      "learning_rate": 8.275753110019367e-05,
      "loss": 1.5679,
      "step": 1565
    },
    {
      "epoch": 2.7815275310834813,
      "grad_norm": 0.9321354627609253,
      "learning_rate": 8.264741591728417e-05,
      "loss": 1.7596,
      "step": 1566
    },
    {
      "epoch": 2.783303730017762,
      "grad_norm": 0.8213515281677246,
      "learning_rate": 8.253732242395842e-05,
      "loss": 1.5196,
      "step": 1567
    },
    {
      "epoch": 2.7850799289520425,
      "grad_norm": 0.86561518907547,
      "learning_rate": 8.242725075782605e-05,
      "loss": 1.8547,
      "step": 1568
    },
    {
      "epoch": 2.786856127886323,
      "grad_norm": 0.9134800434112549,
      "learning_rate": 8.231720105646936e-05,
      "loss": 1.4872,
      "step": 1569
    },
    {
      "epoch": 2.788632326820604,
      "grad_norm": 0.7813941836357117,
      "learning_rate": 8.220717345744316e-05,
      "loss": 1.5577,
      "step": 1570
    },
    {
      "epoch": 2.7904085257548843,
      "grad_norm": 0.8542492389678955,
      "learning_rate": 8.209716809827474e-05,
      "loss": 1.6066,
      "step": 1571
    },
    {
      "epoch": 2.7921847246891653,
      "grad_norm": 0.9112383127212524,
      "learning_rate": 8.198718511646357e-05,
      "loss": 1.9323,
      "step": 1572
    },
    {
      "epoch": 2.793960923623446,
      "grad_norm": 0.8075822591781616,
      "learning_rate": 8.187722464948096e-05,
      "loss": 1.9677,
      "step": 1573
    },
    {
      "epoch": 2.7957371225577266,
      "grad_norm": 0.832591712474823,
      "learning_rate": 8.176728683477038e-05,
      "loss": 1.617,
      "step": 1574
    },
    {
      "epoch": 2.797513321492007,
      "grad_norm": 0.8341981172561646,
      "learning_rate": 8.165737180974678e-05,
      "loss": 1.5534,
      "step": 1575
    },
    {
      "epoch": 2.7992895204262878,
      "grad_norm": 0.8384018540382385,
      "learning_rate": 8.154747971179665e-05,
      "loss": 1.3686,
      "step": 1576
    },
    {
      "epoch": 2.8010657193605684,
      "grad_norm": 0.880469799041748,
      "learning_rate": 8.143761067827791e-05,
      "loss": 1.7591,
      "step": 1577
    },
    {
      "epoch": 2.802841918294849,
      "grad_norm": 0.865852952003479,
      "learning_rate": 8.132776484651956e-05,
      "loss": 1.6904,
      "step": 1578
    },
    {
      "epoch": 2.8046181172291296,
      "grad_norm": 0.7851837873458862,
      "learning_rate": 8.121794235382164e-05,
      "loss": 2.0699,
      "step": 1579
    },
    {
      "epoch": 2.80639431616341,
      "grad_norm": 0.9019510746002197,
      "learning_rate": 8.110814333745497e-05,
      "loss": 1.7166,
      "step": 1580
    },
    {
      "epoch": 2.808170515097691,
      "grad_norm": 0.8000392317771912,
      "learning_rate": 8.099836793466114e-05,
      "loss": 1.4951,
      "step": 1581
    },
    {
      "epoch": 2.8099467140319714,
      "grad_norm": 0.8359841704368591,
      "learning_rate": 8.088861628265214e-05,
      "loss": 1.6921,
      "step": 1582
    },
    {
      "epoch": 2.8117229129662524,
      "grad_norm": 0.8573190569877625,
      "learning_rate": 8.077888851861021e-05,
      "loss": 1.6068,
      "step": 1583
    },
    {
      "epoch": 2.8134991119005326,
      "grad_norm": 0.9309484958648682,
      "learning_rate": 8.066918477968791e-05,
      "loss": 1.6154,
      "step": 1584
    },
    {
      "epoch": 2.8152753108348136,
      "grad_norm": 0.8595494627952576,
      "learning_rate": 8.055950520300761e-05,
      "loss": 1.6166,
      "step": 1585
    },
    {
      "epoch": 2.8170515097690942,
      "grad_norm": 0.8870896697044373,
      "learning_rate": 8.04498499256615e-05,
      "loss": 1.6721,
      "step": 1586
    },
    {
      "epoch": 2.818827708703375,
      "grad_norm": 0.8461597561836243,
      "learning_rate": 8.034021908471149e-05,
      "loss": 1.4368,
      "step": 1587
    },
    {
      "epoch": 2.8206039076376554,
      "grad_norm": 0.8696126937866211,
      "learning_rate": 8.023061281718892e-05,
      "loss": 1.6124,
      "step": 1588
    },
    {
      "epoch": 2.822380106571936,
      "grad_norm": 0.853430986404419,
      "learning_rate": 8.012103126009427e-05,
      "loss": 1.9472,
      "step": 1589
    },
    {
      "epoch": 2.8241563055062167,
      "grad_norm": 0.9237697720527649,
      "learning_rate": 8.001147455039735e-05,
      "loss": 1.8071,
      "step": 1590
    },
    {
      "epoch": 2.8259325044404973,
      "grad_norm": 0.8782017827033997,
      "learning_rate": 7.990194282503679e-05,
      "loss": 1.8101,
      "step": 1591
    },
    {
      "epoch": 2.827708703374778,
      "grad_norm": 0.8707759976387024,
      "learning_rate": 7.979243622091998e-05,
      "loss": 1.5026,
      "step": 1592
    },
    {
      "epoch": 2.8294849023090585,
      "grad_norm": 0.8356510400772095,
      "learning_rate": 7.968295487492292e-05,
      "loss": 1.7838,
      "step": 1593
    },
    {
      "epoch": 2.8312611012433395,
      "grad_norm": 0.7936140298843384,
      "learning_rate": 7.957349892389015e-05,
      "loss": 1.4674,
      "step": 1594
    },
    {
      "epoch": 2.8330373001776197,
      "grad_norm": 0.8541965484619141,
      "learning_rate": 7.94640685046343e-05,
      "loss": 1.8605,
      "step": 1595
    },
    {
      "epoch": 2.8348134991119007,
      "grad_norm": 0.8210464119911194,
      "learning_rate": 7.935466375393615e-05,
      "loss": 1.3603,
      "step": 1596
    },
    {
      "epoch": 2.8365896980461813,
      "grad_norm": 0.861502468585968,
      "learning_rate": 7.924528480854448e-05,
      "loss": 1.7817,
      "step": 1597
    },
    {
      "epoch": 2.838365896980462,
      "grad_norm": 0.8906079530715942,
      "learning_rate": 7.913593180517574e-05,
      "loss": 1.7022,
      "step": 1598
    },
    {
      "epoch": 2.8401420959147425,
      "grad_norm": 0.844153642654419,
      "learning_rate": 7.90266048805139e-05,
      "loss": 1.5932,
      "step": 1599
    },
    {
      "epoch": 2.841918294849023,
      "grad_norm": 0.9019467234611511,
      "learning_rate": 7.891730417121044e-05,
      "loss": 1.9356,
      "step": 1600
    },
    {
      "epoch": 2.841918294849023,
      "eval_loss": 1.9511243104934692,
      "eval_runtime": 17.5569,
      "eval_samples_per_second": 57.015,
      "eval_steps_per_second": 28.536,
      "step": 1600
    },
    {
      "epoch": 2.8436944937833037,
      "grad_norm": 0.8985891342163086,
      "learning_rate": 7.880802981388402e-05,
      "loss": 1.566,
      "step": 1601
    },
    {
      "epoch": 2.8454706927175843,
      "grad_norm": 1.0865901708602905,
      "learning_rate": 7.869878194512033e-05,
      "loss": 1.7285,
      "step": 1602
    },
    {
      "epoch": 2.847246891651865,
      "grad_norm": 0.8182195425033569,
      "learning_rate": 7.858956070147206e-05,
      "loss": 1.4929,
      "step": 1603
    },
    {
      "epoch": 2.8490230905861456,
      "grad_norm": 0.8206174969673157,
      "learning_rate": 7.848036621945851e-05,
      "loss": 1.496,
      "step": 1604
    },
    {
      "epoch": 2.850799289520426,
      "grad_norm": 0.9253233671188354,
      "learning_rate": 7.837119863556554e-05,
      "loss": 1.6014,
      "step": 1605
    },
    {
      "epoch": 2.8525754884547068,
      "grad_norm": 0.8764801621437073,
      "learning_rate": 7.826205808624548e-05,
      "loss": 1.4048,
      "step": 1606
    },
    {
      "epoch": 2.854351687388988,
      "grad_norm": 0.8577253818511963,
      "learning_rate": 7.815294470791677e-05,
      "loss": 1.7543,
      "step": 1607
    },
    {
      "epoch": 2.856127886323268,
      "grad_norm": 0.8698010444641113,
      "learning_rate": 7.804385863696394e-05,
      "loss": 1.7504,
      "step": 1608
    },
    {
      "epoch": 2.857904085257549,
      "grad_norm": 0.8172439336776733,
      "learning_rate": 7.793480000973733e-05,
      "loss": 1.6521,
      "step": 1609
    },
    {
      "epoch": 2.8596802841918296,
      "grad_norm": 0.879926860332489,
      "learning_rate": 7.782576896255306e-05,
      "loss": 1.5276,
      "step": 1610
    },
    {
      "epoch": 2.8614564831261102,
      "grad_norm": 0.8680874109268188,
      "learning_rate": 7.771676563169276e-05,
      "loss": 1.5696,
      "step": 1611
    },
    {
      "epoch": 2.863232682060391,
      "grad_norm": 0.816207230091095,
      "learning_rate": 7.76077901534033e-05,
      "loss": 1.3131,
      "step": 1612
    },
    {
      "epoch": 2.8650088809946714,
      "grad_norm": 0.8037435412406921,
      "learning_rate": 7.749884266389694e-05,
      "loss": 1.3958,
      "step": 1613
    },
    {
      "epoch": 2.866785079928952,
      "grad_norm": 0.8342409133911133,
      "learning_rate": 7.738992329935078e-05,
      "loss": 1.8149,
      "step": 1614
    },
    {
      "epoch": 2.8685612788632326,
      "grad_norm": 0.8636154532432556,
      "learning_rate": 7.728103219590681e-05,
      "loss": 1.9382,
      "step": 1615
    },
    {
      "epoch": 2.8703374777975132,
      "grad_norm": 0.7936214804649353,
      "learning_rate": 7.71721694896718e-05,
      "loss": 1.6052,
      "step": 1616
    },
    {
      "epoch": 2.872113676731794,
      "grad_norm": 0.897838830947876,
      "learning_rate": 7.706333531671687e-05,
      "loss": 1.7377,
      "step": 1617
    },
    {
      "epoch": 2.8738898756660745,
      "grad_norm": 0.8995990753173828,
      "learning_rate": 7.695452981307753e-05,
      "loss": 1.6473,
      "step": 1618
    },
    {
      "epoch": 2.875666074600355,
      "grad_norm": 0.7890574932098389,
      "learning_rate": 7.684575311475356e-05,
      "loss": 1.5817,
      "step": 1619
    },
    {
      "epoch": 2.877442273534636,
      "grad_norm": 0.8394104242324829,
      "learning_rate": 7.673700535770859e-05,
      "loss": 1.5918,
      "step": 1620
    },
    {
      "epoch": 2.8792184724689163,
      "grad_norm": 0.8907750248908997,
      "learning_rate": 7.662828667787015e-05,
      "loss": 1.7032,
      "step": 1621
    },
    {
      "epoch": 2.8809946714031973,
      "grad_norm": 0.9236953258514404,
      "learning_rate": 7.651959721112936e-05,
      "loss": 1.7065,
      "step": 1622
    },
    {
      "epoch": 2.882770870337478,
      "grad_norm": 0.8510025143623352,
      "learning_rate": 7.641093709334094e-05,
      "loss": 1.8307,
      "step": 1623
    },
    {
      "epoch": 2.8845470692717585,
      "grad_norm": 0.9365525245666504,
      "learning_rate": 7.630230646032282e-05,
      "loss": 1.7854,
      "step": 1624
    },
    {
      "epoch": 2.886323268206039,
      "grad_norm": 0.9350879192352295,
      "learning_rate": 7.619370544785608e-05,
      "loss": 1.6279,
      "step": 1625
    },
    {
      "epoch": 2.8880994671403197,
      "grad_norm": 0.8760465383529663,
      "learning_rate": 7.60851341916849e-05,
      "loss": 1.8051,
      "step": 1626
    },
    {
      "epoch": 2.8898756660746003,
      "grad_norm": 0.8202375769615173,
      "learning_rate": 7.597659282751615e-05,
      "loss": 1.9748,
      "step": 1627
    },
    {
      "epoch": 2.891651865008881,
      "grad_norm": 0.9252364039421082,
      "learning_rate": 7.58680814910193e-05,
      "loss": 1.3808,
      "step": 1628
    },
    {
      "epoch": 2.8934280639431615,
      "grad_norm": 0.7803196310997009,
      "learning_rate": 7.575960031782643e-05,
      "loss": 1.7643,
      "step": 1629
    },
    {
      "epoch": 2.895204262877442,
      "grad_norm": 0.8633018136024475,
      "learning_rate": 7.565114944353179e-05,
      "loss": 1.6476,
      "step": 1630
    },
    {
      "epoch": 2.8969804618117228,
      "grad_norm": 0.8162235021591187,
      "learning_rate": 7.554272900369183e-05,
      "loss": 1.983,
      "step": 1631
    },
    {
      "epoch": 2.8987566607460034,
      "grad_norm": 0.9474201798439026,
      "learning_rate": 7.543433913382496e-05,
      "loss": 1.6551,
      "step": 1632
    },
    {
      "epoch": 2.9005328596802844,
      "grad_norm": 0.7988765239715576,
      "learning_rate": 7.532597996941132e-05,
      "loss": 1.2682,
      "step": 1633
    },
    {
      "epoch": 2.9023090586145646,
      "grad_norm": 0.8228864073753357,
      "learning_rate": 7.521765164589268e-05,
      "loss": 1.6717,
      "step": 1634
    },
    {
      "epoch": 2.9040852575488456,
      "grad_norm": 0.8563727140426636,
      "learning_rate": 7.510935429867238e-05,
      "loss": 1.7537,
      "step": 1635
    },
    {
      "epoch": 2.905861456483126,
      "grad_norm": 0.8863992094993591,
      "learning_rate": 7.500108806311489e-05,
      "loss": 1.8218,
      "step": 1636
    },
    {
      "epoch": 2.907637655417407,
      "grad_norm": 0.8877423405647278,
      "learning_rate": 7.489285307454586e-05,
      "loss": 1.5754,
      "step": 1637
    },
    {
      "epoch": 2.9094138543516874,
      "grad_norm": 0.8244193196296692,
      "learning_rate": 7.478464946825184e-05,
      "loss": 1.7599,
      "step": 1638
    },
    {
      "epoch": 2.911190053285968,
      "grad_norm": 0.9474585652351379,
      "learning_rate": 7.467647737948027e-05,
      "loss": 1.9876,
      "step": 1639
    },
    {
      "epoch": 2.9129662522202486,
      "grad_norm": 0.9136055111885071,
      "learning_rate": 7.456833694343906e-05,
      "loss": 1.7759,
      "step": 1640
    },
    {
      "epoch": 2.9147424511545292,
      "grad_norm": 0.8830569386482239,
      "learning_rate": 7.446022829529657e-05,
      "loss": 1.8285,
      "step": 1641
    },
    {
      "epoch": 2.91651865008881,
      "grad_norm": 0.8211177587509155,
      "learning_rate": 7.435215157018156e-05,
      "loss": 1.3846,
      "step": 1642
    },
    {
      "epoch": 2.9182948490230904,
      "grad_norm": 0.8348009586334229,
      "learning_rate": 7.424410690318277e-05,
      "loss": 1.6974,
      "step": 1643
    },
    {
      "epoch": 2.9200710479573715,
      "grad_norm": 0.9696959257125854,
      "learning_rate": 7.413609442934886e-05,
      "loss": 1.6698,
      "step": 1644
    },
    {
      "epoch": 2.9218472468916517,
      "grad_norm": 0.8997682332992554,
      "learning_rate": 7.402811428368832e-05,
      "loss": 1.6709,
      "step": 1645
    },
    {
      "epoch": 2.9236234458259327,
      "grad_norm": 0.9437649250030518,
      "learning_rate": 7.392016660116919e-05,
      "loss": 1.5,
      "step": 1646
    },
    {
      "epoch": 2.9253996447602133,
      "grad_norm": 0.810899019241333,
      "learning_rate": 7.381225151671893e-05,
      "loss": 1.3418,
      "step": 1647
    },
    {
      "epoch": 2.927175843694494,
      "grad_norm": 0.8799175024032593,
      "learning_rate": 7.370436916522428e-05,
      "loss": 1.8149,
      "step": 1648
    },
    {
      "epoch": 2.9289520426287745,
      "grad_norm": 0.8207417130470276,
      "learning_rate": 7.359651968153108e-05,
      "loss": 1.5897,
      "step": 1649
    },
    {
      "epoch": 2.930728241563055,
      "grad_norm": 0.8632379174232483,
      "learning_rate": 7.348870320044399e-05,
      "loss": 1.9618,
      "step": 1650
    },
    {
      "epoch": 2.9325044404973357,
      "grad_norm": 0.7046266794204712,
      "learning_rate": 7.338091985672651e-05,
      "loss": 1.5254,
      "step": 1651
    },
    {
      "epoch": 2.9342806394316163,
      "grad_norm": 0.821221649646759,
      "learning_rate": 7.327316978510075e-05,
      "loss": 1.6866,
      "step": 1652
    },
    {
      "epoch": 2.936056838365897,
      "grad_norm": 0.8105358481407166,
      "learning_rate": 7.316545312024714e-05,
      "loss": 1.555,
      "step": 1653
    },
    {
      "epoch": 2.9378330373001775,
      "grad_norm": 0.8267503380775452,
      "learning_rate": 7.30577699968044e-05,
      "loss": 1.7833,
      "step": 1654
    },
    {
      "epoch": 2.939609236234458,
      "grad_norm": 0.8870266079902649,
      "learning_rate": 7.295012054936934e-05,
      "loss": 1.6176,
      "step": 1655
    },
    {
      "epoch": 2.9413854351687387,
      "grad_norm": 0.8617192506790161,
      "learning_rate": 7.284250491249665e-05,
      "loss": 1.4772,
      "step": 1656
    },
    {
      "epoch": 2.94316163410302,
      "grad_norm": 0.8639336228370667,
      "learning_rate": 7.27349232206987e-05,
      "loss": 1.6254,
      "step": 1657
    },
    {
      "epoch": 2.9449378330373,
      "grad_norm": 0.8535436987876892,
      "learning_rate": 7.26273756084456e-05,
      "loss": 1.8443,
      "step": 1658
    },
    {
      "epoch": 2.946714031971581,
      "grad_norm": 0.8667997717857361,
      "learning_rate": 7.251986221016473e-05,
      "loss": 1.8097,
      "step": 1659
    },
    {
      "epoch": 2.9484902309058616,
      "grad_norm": 0.9378008842468262,
      "learning_rate": 7.241238316024069e-05,
      "loss": 1.6797,
      "step": 1660
    },
    {
      "epoch": 2.950266429840142,
      "grad_norm": 0.8577426075935364,
      "learning_rate": 7.230493859301526e-05,
      "loss": 1.4129,
      "step": 1661
    },
    {
      "epoch": 2.952042628774423,
      "grad_norm": 0.800424337387085,
      "learning_rate": 7.219752864278699e-05,
      "loss": 1.5533,
      "step": 1662
    },
    {
      "epoch": 2.9538188277087034,
      "grad_norm": 0.8443882465362549,
      "learning_rate": 7.209015344381123e-05,
      "loss": 1.8087,
      "step": 1663
    },
    {
      "epoch": 2.955595026642984,
      "grad_norm": 0.9260624051094055,
      "learning_rate": 7.198281313029995e-05,
      "loss": 1.8308,
      "step": 1664
    },
    {
      "epoch": 2.9573712255772646,
      "grad_norm": 0.9376252889633179,
      "learning_rate": 7.18755078364214e-05,
      "loss": 1.4743,
      "step": 1665
    },
    {
      "epoch": 2.959147424511545,
      "grad_norm": 0.9351133704185486,
      "learning_rate": 7.17682376963001e-05,
      "loss": 1.6337,
      "step": 1666
    },
    {
      "epoch": 2.960923623445826,
      "grad_norm": 0.9358673095703125,
      "learning_rate": 7.166100284401662e-05,
      "loss": 1.6305,
      "step": 1667
    },
    {
      "epoch": 2.9626998223801064,
      "grad_norm": 0.9658148884773254,
      "learning_rate": 7.155380341360752e-05,
      "loss": 1.5475,
      "step": 1668
    },
    {
      "epoch": 2.964476021314387,
      "grad_norm": 0.8388633728027344,
      "learning_rate": 7.144663953906494e-05,
      "loss": 1.3774,
      "step": 1669
    },
    {
      "epoch": 2.966252220248668,
      "grad_norm": 0.8764585256576538,
      "learning_rate": 7.133951135433666e-05,
      "loss": 1.6857,
      "step": 1670
    },
    {
      "epoch": 2.9680284191829482,
      "grad_norm": 0.9113872051239014,
      "learning_rate": 7.123241899332583e-05,
      "loss": 2.0364,
      "step": 1671
    },
    {
      "epoch": 2.9698046181172293,
      "grad_norm": 0.8843749165534973,
      "learning_rate": 7.112536258989086e-05,
      "loss": 1.5896,
      "step": 1672
    },
    {
      "epoch": 2.97158081705151,
      "grad_norm": 0.837096095085144,
      "learning_rate": 7.10183422778451e-05,
      "loss": 1.7012,
      "step": 1673
    },
    {
      "epoch": 2.9733570159857905,
      "grad_norm": 0.8365213871002197,
      "learning_rate": 7.091135819095698e-05,
      "loss": 1.8533,
      "step": 1674
    },
    {
      "epoch": 2.975133214920071,
      "grad_norm": 0.9145013689994812,
      "learning_rate": 7.080441046294947e-05,
      "loss": 1.8673,
      "step": 1675
    },
    {
      "epoch": 2.9769094138543517,
      "grad_norm": 0.8963333964347839,
      "learning_rate": 7.069749922750018e-05,
      "loss": 1.8468,
      "step": 1676
    },
    {
      "epoch": 2.9786856127886323,
      "grad_norm": 0.8586370944976807,
      "learning_rate": 7.05906246182411e-05,
      "loss": 1.8294,
      "step": 1677
    },
    {
      "epoch": 2.980461811722913,
      "grad_norm": 0.8373517394065857,
      "learning_rate": 7.048378676875842e-05,
      "loss": 1.8176,
      "step": 1678
    },
    {
      "epoch": 2.9822380106571935,
      "grad_norm": 0.9353559613227844,
      "learning_rate": 7.037698581259241e-05,
      "loss": 1.7414,
      "step": 1679
    },
    {
      "epoch": 2.984014209591474,
      "grad_norm": 0.8870553374290466,
      "learning_rate": 7.027022188323716e-05,
      "loss": 1.569,
      "step": 1680
    },
    {
      "epoch": 2.9857904085257547,
      "grad_norm": 0.8919901251792908,
      "learning_rate": 7.016349511414062e-05,
      "loss": 1.7788,
      "step": 1681
    },
    {
      "epoch": 2.9875666074600353,
      "grad_norm": 0.836243748664856,
      "learning_rate": 7.005680563870414e-05,
      "loss": 1.4974,
      "step": 1682
    },
    {
      "epoch": 2.9893428063943164,
      "grad_norm": 0.893593430519104,
      "learning_rate": 6.995015359028247e-05,
      "loss": 1.4865,
      "step": 1683
    },
    {
      "epoch": 2.9911190053285965,
      "grad_norm": 0.8734187483787537,
      "learning_rate": 6.984353910218371e-05,
      "loss": 1.7041,
      "step": 1684
    },
    {
      "epoch": 2.9928952042628776,
      "grad_norm": 0.8605254888534546,
      "learning_rate": 6.97369623076689e-05,
      "loss": 1.8351,
      "step": 1685
    },
    {
      "epoch": 2.994671403197158,
      "grad_norm": 0.8660004734992981,
      "learning_rate": 6.963042333995198e-05,
      "loss": 1.6336,
      "step": 1686
    },
    {
      "epoch": 2.996447602131439,
      "grad_norm": 0.805020272731781,
      "learning_rate": 6.952392233219966e-05,
      "loss": 1.6786,
      "step": 1687
    },
    {
      "epoch": 2.9982238010657194,
      "grad_norm": 0.9295530319213867,
      "learning_rate": 6.941745941753113e-05,
      "loss": 1.4461,
      "step": 1688
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.9144086837768555,
      "learning_rate": 6.931103472901795e-05,
      "loss": 1.5707,
      "step": 1689
    },
    {
      "epoch": 3.0017761989342806,
      "grad_norm": 0.7330948114395142,
      "learning_rate": 6.920464839968405e-05,
      "loss": 1.4466,
      "step": 1690
    },
    {
      "epoch": 3.003552397868561,
      "grad_norm": 0.8053832054138184,
      "learning_rate": 6.909830056250527e-05,
      "loss": 1.4275,
      "step": 1691
    },
    {
      "epoch": 3.005328596802842,
      "grad_norm": 0.7888354659080505,
      "learning_rate": 6.899199135040936e-05,
      "loss": 1.4625,
      "step": 1692
    },
    {
      "epoch": 3.0071047957371224,
      "grad_norm": 0.826702892780304,
      "learning_rate": 6.888572089627585e-05,
      "loss": 1.2404,
      "step": 1693
    },
    {
      "epoch": 3.008880994671403,
      "grad_norm": 0.7922016978263855,
      "learning_rate": 6.877948933293577e-05,
      "loss": 1.3447,
      "step": 1694
    },
    {
      "epoch": 3.0106571936056836,
      "grad_norm": 0.7858552932739258,
      "learning_rate": 6.867329679317155e-05,
      "loss": 1.4732,
      "step": 1695
    },
    {
      "epoch": 3.0124333925399647,
      "grad_norm": 0.7638794183731079,
      "learning_rate": 6.856714340971681e-05,
      "loss": 1.3497,
      "step": 1696
    },
    {
      "epoch": 3.0142095914742453,
      "grad_norm": 0.8893672823905945,
      "learning_rate": 6.846102931525637e-05,
      "loss": 1.6222,
      "step": 1697
    },
    {
      "epoch": 3.015985790408526,
      "grad_norm": 0.8200019001960754,
      "learning_rate": 6.835495464242578e-05,
      "loss": 1.4249,
      "step": 1698
    },
    {
      "epoch": 3.0177619893428065,
      "grad_norm": 0.8402576446533203,
      "learning_rate": 6.824891952381133e-05,
      "loss": 1.4351,
      "step": 1699
    },
    {
      "epoch": 3.019538188277087,
      "grad_norm": 0.8975090980529785,
      "learning_rate": 6.814292409194997e-05,
      "loss": 1.4726,
      "step": 1700
    },
    {
      "epoch": 3.019538188277087,
      "eval_loss": 1.9882135391235352,
      "eval_runtime": 17.6051,
      "eval_samples_per_second": 56.858,
      "eval_steps_per_second": 28.458,
      "step": 1700
    },
    {
      "epoch": 3.0213143872113677,
      "grad_norm": 0.8519137501716614,
      "learning_rate": 6.803696847932901e-05,
      "loss": 1.6226,
      "step": 1701
    },
    {
      "epoch": 3.0230905861456483,
      "grad_norm": 0.8372290134429932,
      "learning_rate": 6.793105281838589e-05,
      "loss": 1.4665,
      "step": 1702
    },
    {
      "epoch": 3.024866785079929,
      "grad_norm": 0.8495118618011475,
      "learning_rate": 6.782517724150825e-05,
      "loss": 1.6096,
      "step": 1703
    },
    {
      "epoch": 3.0266429840142095,
      "grad_norm": 0.9201812744140625,
      "learning_rate": 6.771934188103355e-05,
      "loss": 1.4853,
      "step": 1704
    },
    {
      "epoch": 3.02841918294849,
      "grad_norm": 0.8982337713241577,
      "learning_rate": 6.761354686924895e-05,
      "loss": 1.3935,
      "step": 1705
    },
    {
      "epoch": 3.0301953818827707,
      "grad_norm": 0.9166777729988098,
      "learning_rate": 6.750779233839131e-05,
      "loss": 1.5211,
      "step": 1706
    },
    {
      "epoch": 3.0319715808170513,
      "grad_norm": 1.0381824970245361,
      "learning_rate": 6.740207842064677e-05,
      "loss": 1.5422,
      "step": 1707
    },
    {
      "epoch": 3.0337477797513324,
      "grad_norm": 0.9225248098373413,
      "learning_rate": 6.729640524815075e-05,
      "loss": 1.1772,
      "step": 1708
    },
    {
      "epoch": 3.035523978685613,
      "grad_norm": 0.9861879348754883,
      "learning_rate": 6.71907729529877e-05,
      "loss": 1.4737,
      "step": 1709
    },
    {
      "epoch": 3.0373001776198936,
      "grad_norm": 0.9831342697143555,
      "learning_rate": 6.708518166719107e-05,
      "loss": 1.2849,
      "step": 1710
    },
    {
      "epoch": 3.039076376554174,
      "grad_norm": 1.1001237630844116,
      "learning_rate": 6.697963152274294e-05,
      "loss": 1.3309,
      "step": 1711
    },
    {
      "epoch": 3.040852575488455,
      "grad_norm": 1.099215030670166,
      "learning_rate": 6.687412265157401e-05,
      "loss": 1.5668,
      "step": 1712
    },
    {
      "epoch": 3.0426287744227354,
      "grad_norm": 1.0355212688446045,
      "learning_rate": 6.676865518556346e-05,
      "loss": 1.2482,
      "step": 1713
    },
    {
      "epoch": 3.044404973357016,
      "grad_norm": 1.0642870664596558,
      "learning_rate": 6.666322925653861e-05,
      "loss": 1.6478,
      "step": 1714
    },
    {
      "epoch": 3.0461811722912966,
      "grad_norm": 1.098315715789795,
      "learning_rate": 6.65578449962749e-05,
      "loss": 1.3028,
      "step": 1715
    },
    {
      "epoch": 3.047957371225577,
      "grad_norm": 0.8824766874313354,
      "learning_rate": 6.645250253649575e-05,
      "loss": 0.8911,
      "step": 1716
    },
    {
      "epoch": 3.049733570159858,
      "grad_norm": 1.03688383102417,
      "learning_rate": 6.634720200887223e-05,
      "loss": 1.3547,
      "step": 1717
    },
    {
      "epoch": 3.0515097690941384,
      "grad_norm": 1.02106511592865,
      "learning_rate": 6.624194354502302e-05,
      "loss": 1.546,
      "step": 1718
    },
    {
      "epoch": 3.053285968028419,
      "grad_norm": 1.0728042125701904,
      "learning_rate": 6.61367272765143e-05,
      "loss": 1.4367,
      "step": 1719
    },
    {
      "epoch": 3.0550621669626996,
      "grad_norm": 1.064403772354126,
      "learning_rate": 6.603155333485944e-05,
      "loss": 1.4536,
      "step": 1720
    },
    {
      "epoch": 3.0568383658969807,
      "grad_norm": 1.0475084781646729,
      "learning_rate": 6.592642185151886e-05,
      "loss": 1.4995,
      "step": 1721
    },
    {
      "epoch": 3.0586145648312613,
      "grad_norm": 1.0047968626022339,
      "learning_rate": 6.582133295790007e-05,
      "loss": 1.5182,
      "step": 1722
    },
    {
      "epoch": 3.060390763765542,
      "grad_norm": 0.9043979644775391,
      "learning_rate": 6.571628678535717e-05,
      "loss": 1.3081,
      "step": 1723
    },
    {
      "epoch": 3.0621669626998225,
      "grad_norm": 0.9802244901657104,
      "learning_rate": 6.561128346519098e-05,
      "loss": 1.2879,
      "step": 1724
    },
    {
      "epoch": 3.063943161634103,
      "grad_norm": 1.0562390089035034,
      "learning_rate": 6.550632312864868e-05,
      "loss": 1.2467,
      "step": 1725
    },
    {
      "epoch": 3.0657193605683837,
      "grad_norm": 0.9347144365310669,
      "learning_rate": 6.540140590692379e-05,
      "loss": 1.4503,
      "step": 1726
    },
    {
      "epoch": 3.0674955595026643,
      "grad_norm": 1.0728398561477661,
      "learning_rate": 6.529653193115587e-05,
      "loss": 1.4138,
      "step": 1727
    },
    {
      "epoch": 3.069271758436945,
      "grad_norm": 1.056671142578125,
      "learning_rate": 6.519170133243044e-05,
      "loss": 1.5063,
      "step": 1728
    },
    {
      "epoch": 3.0710479573712255,
      "grad_norm": 0.9664649963378906,
      "learning_rate": 6.50869142417789e-05,
      "loss": 1.3176,
      "step": 1729
    },
    {
      "epoch": 3.072824156305506,
      "grad_norm": 1.002807855606079,
      "learning_rate": 6.498217079017818e-05,
      "loss": 1.5659,
      "step": 1730
    },
    {
      "epoch": 3.0746003552397867,
      "grad_norm": 0.9674978852272034,
      "learning_rate": 6.48774711085506e-05,
      "loss": 1.4346,
      "step": 1731
    },
    {
      "epoch": 3.0763765541740673,
      "grad_norm": 0.9721518158912659,
      "learning_rate": 6.477281532776393e-05,
      "loss": 1.6688,
      "step": 1732
    },
    {
      "epoch": 3.0781527531083483,
      "grad_norm": 1.0016016960144043,
      "learning_rate": 6.466820357863093e-05,
      "loss": 1.6656,
      "step": 1733
    },
    {
      "epoch": 3.079928952042629,
      "grad_norm": 0.93742835521698,
      "learning_rate": 6.456363599190936e-05,
      "loss": 1.4402,
      "step": 1734
    },
    {
      "epoch": 3.0817051509769096,
      "grad_norm": 1.0826137065887451,
      "learning_rate": 6.445911269830189e-05,
      "loss": 1.4206,
      "step": 1735
    },
    {
      "epoch": 3.08348134991119,
      "grad_norm": 0.9963404536247253,
      "learning_rate": 6.435463382845565e-05,
      "loss": 1.3167,
      "step": 1736
    },
    {
      "epoch": 3.0852575488454708,
      "grad_norm": 1.0173535346984863,
      "learning_rate": 6.425019951296233e-05,
      "loss": 1.3012,
      "step": 1737
    },
    {
      "epoch": 3.0870337477797514,
      "grad_norm": 0.9275796413421631,
      "learning_rate": 6.41458098823579e-05,
      "loss": 1.519,
      "step": 1738
    },
    {
      "epoch": 3.088809946714032,
      "grad_norm": 1.0281367301940918,
      "learning_rate": 6.404146506712258e-05,
      "loss": 1.6796,
      "step": 1739
    },
    {
      "epoch": 3.0905861456483126,
      "grad_norm": 1.0357345342636108,
      "learning_rate": 6.393716519768047e-05,
      "loss": 1.4539,
      "step": 1740
    },
    {
      "epoch": 3.092362344582593,
      "grad_norm": 1.0135589838027954,
      "learning_rate": 6.38329104043995e-05,
      "loss": 1.4837,
      "step": 1741
    },
    {
      "epoch": 3.094138543516874,
      "grad_norm": 1.069537878036499,
      "learning_rate": 6.372870081759125e-05,
      "loss": 1.7567,
      "step": 1742
    },
    {
      "epoch": 3.0959147424511544,
      "grad_norm": 1.30342698097229,
      "learning_rate": 6.362453656751088e-05,
      "loss": 1.515,
      "step": 1743
    },
    {
      "epoch": 3.097690941385435,
      "grad_norm": 1.0498642921447754,
      "learning_rate": 6.352041778435675e-05,
      "loss": 1.329,
      "step": 1744
    },
    {
      "epoch": 3.0994671403197156,
      "grad_norm": 1.0704816579818726,
      "learning_rate": 6.341634459827052e-05,
      "loss": 1.6074,
      "step": 1745
    },
    {
      "epoch": 3.1012433392539966,
      "grad_norm": 1.1345956325531006,
      "learning_rate": 6.331231713933681e-05,
      "loss": 1.5254,
      "step": 1746
    },
    {
      "epoch": 3.1030195381882772,
      "grad_norm": 1.0471196174621582,
      "learning_rate": 6.320833553758298e-05,
      "loss": 1.355,
      "step": 1747
    },
    {
      "epoch": 3.104795737122558,
      "grad_norm": 1.0298606157302856,
      "learning_rate": 6.310439992297928e-05,
      "loss": 1.4334,
      "step": 1748
    },
    {
      "epoch": 3.1065719360568385,
      "grad_norm": 1.0314183235168457,
      "learning_rate": 6.300051042543826e-05,
      "loss": 1.4003,
      "step": 1749
    },
    {
      "epoch": 3.108348134991119,
      "grad_norm": 1.0037355422973633,
      "learning_rate": 6.289666717481498e-05,
      "loss": 1.4734,
      "step": 1750
    },
    {
      "epoch": 3.1101243339253997,
      "grad_norm": 1.073384165763855,
      "learning_rate": 6.279287030090664e-05,
      "loss": 1.6965,
      "step": 1751
    },
    {
      "epoch": 3.1119005328596803,
      "grad_norm": 1.0694587230682373,
      "learning_rate": 6.26891199334525e-05,
      "loss": 1.3377,
      "step": 1752
    },
    {
      "epoch": 3.113676731793961,
      "grad_norm": 1.0200855731964111,
      "learning_rate": 6.258541620213361e-05,
      "loss": 1.2514,
      "step": 1753
    },
    {
      "epoch": 3.1154529307282415,
      "grad_norm": 1.056326985359192,
      "learning_rate": 6.248175923657278e-05,
      "loss": 1.4221,
      "step": 1754
    },
    {
      "epoch": 3.117229129662522,
      "grad_norm": 0.9493615627288818,
      "learning_rate": 6.237814916633444e-05,
      "loss": 1.6634,
      "step": 1755
    },
    {
      "epoch": 3.1190053285968027,
      "grad_norm": 1.0179766416549683,
      "learning_rate": 6.227458612092429e-05,
      "loss": 1.7606,
      "step": 1756
    },
    {
      "epoch": 3.1207815275310833,
      "grad_norm": 1.022514820098877,
      "learning_rate": 6.217107022978928e-05,
      "loss": 1.6583,
      "step": 1757
    },
    {
      "epoch": 3.122557726465364,
      "grad_norm": 0.9803706407546997,
      "learning_rate": 6.206760162231746e-05,
      "loss": 1.2998,
      "step": 1758
    },
    {
      "epoch": 3.124333925399645,
      "grad_norm": 0.9396275281906128,
      "learning_rate": 6.196418042783776e-05,
      "loss": 1.6658,
      "step": 1759
    },
    {
      "epoch": 3.1261101243339255,
      "grad_norm": 1.0062824487686157,
      "learning_rate": 6.186080677561979e-05,
      "loss": 1.2784,
      "step": 1760
    },
    {
      "epoch": 3.127886323268206,
      "grad_norm": 0.9882250428199768,
      "learning_rate": 6.175748079487386e-05,
      "loss": 1.2928,
      "step": 1761
    },
    {
      "epoch": 3.1296625222024868,
      "grad_norm": 1.01216459274292,
      "learning_rate": 6.165420261475058e-05,
      "loss": 1.3835,
      "step": 1762
    },
    {
      "epoch": 3.1314387211367674,
      "grad_norm": 1.0460925102233887,
      "learning_rate": 6.155097236434084e-05,
      "loss": 1.4888,
      "step": 1763
    },
    {
      "epoch": 3.133214920071048,
      "grad_norm": 1.0283753871917725,
      "learning_rate": 6.144779017267567e-05,
      "loss": 1.2193,
      "step": 1764
    },
    {
      "epoch": 3.1349911190053286,
      "grad_norm": 1.0480154752731323,
      "learning_rate": 6.134465616872598e-05,
      "loss": 1.4901,
      "step": 1765
    },
    {
      "epoch": 3.136767317939609,
      "grad_norm": 1.034855842590332,
      "learning_rate": 6.124157048140244e-05,
      "loss": 1.3806,
      "step": 1766
    },
    {
      "epoch": 3.1385435168738898,
      "grad_norm": 1.0307527780532837,
      "learning_rate": 6.113853323955535e-05,
      "loss": 1.5205,
      "step": 1767
    },
    {
      "epoch": 3.1403197158081704,
      "grad_norm": 1.0338730812072754,
      "learning_rate": 6.103554457197448e-05,
      "loss": 1.3679,
      "step": 1768
    },
    {
      "epoch": 3.142095914742451,
      "grad_norm": 1.0415067672729492,
      "learning_rate": 6.0932604607388866e-05,
      "loss": 1.3148,
      "step": 1769
    },
    {
      "epoch": 3.143872113676732,
      "grad_norm": 0.9801323413848877,
      "learning_rate": 6.0829713474466624e-05,
      "loss": 1.3016,
      "step": 1770
    },
    {
      "epoch": 3.1456483126110126,
      "grad_norm": 1.0904278755187988,
      "learning_rate": 6.0726871301814926e-05,
      "loss": 1.1846,
      "step": 1771
    },
    {
      "epoch": 3.1474245115452932,
      "grad_norm": 1.1559871435165405,
      "learning_rate": 6.062407821797966e-05,
      "loss": 1.4585,
      "step": 1772
    },
    {
      "epoch": 3.149200710479574,
      "grad_norm": 1.0655499696731567,
      "learning_rate": 6.052133435144538e-05,
      "loss": 1.5137,
      "step": 1773
    },
    {
      "epoch": 3.1509769094138544,
      "grad_norm": 1.084878921508789,
      "learning_rate": 6.041863983063516e-05,
      "loss": 1.4372,
      "step": 1774
    },
    {
      "epoch": 3.152753108348135,
      "grad_norm": 1.9667811393737793,
      "learning_rate": 6.0315994783910345e-05,
      "loss": 1.5377,
      "step": 1775
    },
    {
      "epoch": 3.1545293072824157,
      "grad_norm": 1.0475472211837769,
      "learning_rate": 6.021339933957044e-05,
      "loss": 1.5187,
      "step": 1776
    },
    {
      "epoch": 3.1563055062166963,
      "grad_norm": 1.0654336214065552,
      "learning_rate": 6.011085362585306e-05,
      "loss": 1.5832,
      "step": 1777
    },
    {
      "epoch": 3.158081705150977,
      "grad_norm": 0.9893504977226257,
      "learning_rate": 6.00083577709335e-05,
      "loss": 1.133,
      "step": 1778
    },
    {
      "epoch": 3.1598579040852575,
      "grad_norm": 1.064050316810608,
      "learning_rate": 5.9905911902924804e-05,
      "loss": 1.4463,
      "step": 1779
    },
    {
      "epoch": 3.161634103019538,
      "grad_norm": 1.37350332736969,
      "learning_rate": 5.980351614987759e-05,
      "loss": 1.3727,
      "step": 1780
    },
    {
      "epoch": 3.1634103019538187,
      "grad_norm": 1.048861026763916,
      "learning_rate": 5.9701170639779755e-05,
      "loss": 1.5395,
      "step": 1781
    },
    {
      "epoch": 3.1651865008880993,
      "grad_norm": 1.1374026536941528,
      "learning_rate": 5.9598875500556425e-05,
      "loss": 1.2111,
      "step": 1782
    },
    {
      "epoch": 3.1669626998223803,
      "grad_norm": 0.9637338519096375,
      "learning_rate": 5.949663086006971e-05,
      "loss": 1.5676,
      "step": 1783
    },
    {
      "epoch": 3.168738898756661,
      "grad_norm": 0.9325466752052307,
      "learning_rate": 5.9394436846118775e-05,
      "loss": 0.9315,
      "step": 1784
    },
    {
      "epoch": 3.1705150976909415,
      "grad_norm": 0.9810510873794556,
      "learning_rate": 5.929229358643932e-05,
      "loss": 1.3088,
      "step": 1785
    },
    {
      "epoch": 3.172291296625222,
      "grad_norm": 1.0110737085342407,
      "learning_rate": 5.9190201208703644e-05,
      "loss": 1.3026,
      "step": 1786
    },
    {
      "epoch": 3.1740674955595027,
      "grad_norm": 1.0059785842895508,
      "learning_rate": 5.908815984052054e-05,
      "loss": 1.5572,
      "step": 1787
    },
    {
      "epoch": 3.1758436944937833,
      "grad_norm": 1.0277178287506104,
      "learning_rate": 5.898616960943494e-05,
      "loss": 1.4738,
      "step": 1788
    },
    {
      "epoch": 3.177619893428064,
      "grad_norm": 1.0823659896850586,
      "learning_rate": 5.888423064292785e-05,
      "loss": 1.4634,
      "step": 1789
    },
    {
      "epoch": 3.1793960923623446,
      "grad_norm": 1.0590537786483765,
      "learning_rate": 5.878234306841637e-05,
      "loss": 1.5,
      "step": 1790
    },
    {
      "epoch": 3.181172291296625,
      "grad_norm": 0.9767324924468994,
      "learning_rate": 5.868050701325314e-05,
      "loss": 1.1748,
      "step": 1791
    },
    {
      "epoch": 3.1829484902309058,
      "grad_norm": 1.0694776773452759,
      "learning_rate": 5.8578722604726476e-05,
      "loss": 1.5169,
      "step": 1792
    },
    {
      "epoch": 3.1847246891651864,
      "grad_norm": 0.957991898059845,
      "learning_rate": 5.847698997006025e-05,
      "loss": 1.7518,
      "step": 1793
    },
    {
      "epoch": 3.186500888099467,
      "grad_norm": 1.012564778327942,
      "learning_rate": 5.83753092364135e-05,
      "loss": 1.4918,
      "step": 1794
    },
    {
      "epoch": 3.1882770870337476,
      "grad_norm": 1.059879183769226,
      "learning_rate": 5.827368053088042e-05,
      "loss": 1.4752,
      "step": 1795
    },
    {
      "epoch": 3.1900532859680286,
      "grad_norm": 1.079362154006958,
      "learning_rate": 5.817210398049014e-05,
      "loss": 1.4984,
      "step": 1796
    },
    {
      "epoch": 3.191829484902309,
      "grad_norm": 0.9850338697433472,
      "learning_rate": 5.807057971220673e-05,
      "loss": 1.4518,
      "step": 1797
    },
    {
      "epoch": 3.19360568383659,
      "grad_norm": 0.9982918500900269,
      "learning_rate": 5.796910785292873e-05,
      "loss": 1.1937,
      "step": 1798
    },
    {
      "epoch": 3.1953818827708704,
      "grad_norm": 1.0197614431381226,
      "learning_rate": 5.7867688529489296e-05,
      "loss": 1.1914,
      "step": 1799
    },
    {
      "epoch": 3.197158081705151,
      "grad_norm": 1.0374959707260132,
      "learning_rate": 5.776632186865594e-05,
      "loss": 1.1974,
      "step": 1800
    },
    {
      "epoch": 3.197158081705151,
      "eval_loss": 2.0441384315490723,
      "eval_runtime": 17.5367,
      "eval_samples_per_second": 57.08,
      "eval_steps_per_second": 28.569,
      "step": 1800
    },
    {
      "epoch": 3.1989342806394316,
      "grad_norm": 1.0508131980895996,
      "learning_rate": 5.766500799713017e-05,
      "loss": 1.4126,
      "step": 1801
    },
    {
      "epoch": 3.2007104795737122,
      "grad_norm": 1.0107721090316772,
      "learning_rate": 5.756374704154771e-05,
      "loss": 1.3842,
      "step": 1802
    },
    {
      "epoch": 3.202486678507993,
      "grad_norm": 1.0781404972076416,
      "learning_rate": 5.746253912847805e-05,
      "loss": 1.3491,
      "step": 1803
    },
    {
      "epoch": 3.2042628774422734,
      "grad_norm": 1.073528528213501,
      "learning_rate": 5.7361384384424445e-05,
      "loss": 1.5058,
      "step": 1804
    },
    {
      "epoch": 3.206039076376554,
      "grad_norm": 1.1871099472045898,
      "learning_rate": 5.726028293582355e-05,
      "loss": 1.5194,
      "step": 1805
    },
    {
      "epoch": 3.2078152753108347,
      "grad_norm": 1.0366045236587524,
      "learning_rate": 5.715923490904554e-05,
      "loss": 1.6295,
      "step": 1806
    },
    {
      "epoch": 3.2095914742451153,
      "grad_norm": 1.0805737972259521,
      "learning_rate": 5.70582404303938e-05,
      "loss": 1.2686,
      "step": 1807
    },
    {
      "epoch": 3.211367673179396,
      "grad_norm": 1.1300888061523438,
      "learning_rate": 5.695729962610472e-05,
      "loss": 1.4157,
      "step": 1808
    },
    {
      "epoch": 3.213143872113677,
      "grad_norm": 1.0264946222305298,
      "learning_rate": 5.6856412622347645e-05,
      "loss": 1.4428,
      "step": 1809
    },
    {
      "epoch": 3.2149200710479575,
      "grad_norm": 1.0306413173675537,
      "learning_rate": 5.6755579545224705e-05,
      "loss": 1.1973,
      "step": 1810
    },
    {
      "epoch": 3.216696269982238,
      "grad_norm": 1.0479633808135986,
      "learning_rate": 5.6654800520770505e-05,
      "loss": 1.3335,
      "step": 1811
    },
    {
      "epoch": 3.2184724689165187,
      "grad_norm": 1.118105411529541,
      "learning_rate": 5.65540756749522e-05,
      "loss": 1.3998,
      "step": 1812
    },
    {
      "epoch": 3.2202486678507993,
      "grad_norm": 1.0744856595993042,
      "learning_rate": 5.645340513366921e-05,
      "loss": 1.3246,
      "step": 1813
    },
    {
      "epoch": 3.22202486678508,
      "grad_norm": 1.0385111570358276,
      "learning_rate": 5.635278902275312e-05,
      "loss": 1.3475,
      "step": 1814
    },
    {
      "epoch": 3.2238010657193605,
      "grad_norm": 1.0805782079696655,
      "learning_rate": 5.62522274679673e-05,
      "loss": 1.3962,
      "step": 1815
    },
    {
      "epoch": 3.225577264653641,
      "grad_norm": 1.1308550834655762,
      "learning_rate": 5.6151720595007104e-05,
      "loss": 1.327,
      "step": 1816
    },
    {
      "epoch": 3.2273534635879217,
      "grad_norm": 1.0888639688491821,
      "learning_rate": 5.6051268529499535e-05,
      "loss": 1.4843,
      "step": 1817
    },
    {
      "epoch": 3.2291296625222023,
      "grad_norm": 1.091389536857605,
      "learning_rate": 5.595087139700293e-05,
      "loss": 1.2592,
      "step": 1818
    },
    {
      "epoch": 3.230905861456483,
      "grad_norm": 1.0028609037399292,
      "learning_rate": 5.585052932300718e-05,
      "loss": 1.2869,
      "step": 1819
    },
    {
      "epoch": 3.232682060390764,
      "grad_norm": 1.0554451942443848,
      "learning_rate": 5.5750242432933185e-05,
      "loss": 1.6577,
      "step": 1820
    },
    {
      "epoch": 3.2344582593250446,
      "grad_norm": 1.0006511211395264,
      "learning_rate": 5.565001085213295e-05,
      "loss": 1.5836,
      "step": 1821
    },
    {
      "epoch": 3.236234458259325,
      "grad_norm": 1.1457301378250122,
      "learning_rate": 5.554983470588937e-05,
      "loss": 1.5174,
      "step": 1822
    },
    {
      "epoch": 3.238010657193606,
      "grad_norm": 1.1343613862991333,
      "learning_rate": 5.544971411941592e-05,
      "loss": 1.5071,
      "step": 1823
    },
    {
      "epoch": 3.2397868561278864,
      "grad_norm": 1.1253936290740967,
      "learning_rate": 5.5349649217856817e-05,
      "loss": 1.313,
      "step": 1824
    },
    {
      "epoch": 3.241563055062167,
      "grad_norm": 1.0376707315444946,
      "learning_rate": 5.524964012628648e-05,
      "loss": 1.3836,
      "step": 1825
    },
    {
      "epoch": 3.2433392539964476,
      "grad_norm": 1.0888432264328003,
      "learning_rate": 5.5149686969709725e-05,
      "loss": 1.5727,
      "step": 1826
    },
    {
      "epoch": 3.2451154529307282,
      "grad_norm": 1.1322011947631836,
      "learning_rate": 5.504978987306143e-05,
      "loss": 1.5783,
      "step": 1827
    },
    {
      "epoch": 3.246891651865009,
      "grad_norm": 1.0566027164459229,
      "learning_rate": 5.4949948961206245e-05,
      "loss": 1.3176,
      "step": 1828
    },
    {
      "epoch": 3.2486678507992894,
      "grad_norm": 1.0388704538345337,
      "learning_rate": 5.485016435893886e-05,
      "loss": 1.6311,
      "step": 1829
    },
    {
      "epoch": 3.25044404973357,
      "grad_norm": 1.108933448791504,
      "learning_rate": 5.475043619098333e-05,
      "loss": 1.7025,
      "step": 1830
    },
    {
      "epoch": 3.2522202486678506,
      "grad_norm": 1.1138887405395508,
      "learning_rate": 5.465076458199332e-05,
      "loss": 1.6152,
      "step": 1831
    },
    {
      "epoch": 3.2539964476021312,
      "grad_norm": 1.0467584133148193,
      "learning_rate": 5.455114965655179e-05,
      "loss": 1.5538,
      "step": 1832
    },
    {
      "epoch": 3.2557726465364123,
      "grad_norm": 1.0463061332702637,
      "learning_rate": 5.445159153917073e-05,
      "loss": 1.398,
      "step": 1833
    },
    {
      "epoch": 3.257548845470693,
      "grad_norm": 1.0443847179412842,
      "learning_rate": 5.435209035429127e-05,
      "loss": 1.1855,
      "step": 1834
    },
    {
      "epoch": 3.2593250444049735,
      "grad_norm": 1.0407837629318237,
      "learning_rate": 5.42526462262833e-05,
      "loss": 1.4424,
      "step": 1835
    },
    {
      "epoch": 3.261101243339254,
      "grad_norm": 1.1153547763824463,
      "learning_rate": 5.415325927944544e-05,
      "loss": 1.4654,
      "step": 1836
    },
    {
      "epoch": 3.2628774422735347,
      "grad_norm": 1.1238479614257812,
      "learning_rate": 5.4053929638004766e-05,
      "loss": 1.821,
      "step": 1837
    },
    {
      "epoch": 3.2646536412078153,
      "grad_norm": 1.0606207847595215,
      "learning_rate": 5.3954657426116786e-05,
      "loss": 1.6093,
      "step": 1838
    },
    {
      "epoch": 3.266429840142096,
      "grad_norm": 1.1040399074554443,
      "learning_rate": 5.385544276786523e-05,
      "loss": 1.6325,
      "step": 1839
    },
    {
      "epoch": 3.2682060390763765,
      "grad_norm": 1.08716881275177,
      "learning_rate": 5.375628578726181e-05,
      "loss": 1.3484,
      "step": 1840
    },
    {
      "epoch": 3.269982238010657,
      "grad_norm": 1.0740422010421753,
      "learning_rate": 5.3657186608246235e-05,
      "loss": 1.4198,
      "step": 1841
    },
    {
      "epoch": 3.2717584369449377,
      "grad_norm": 1.0719945430755615,
      "learning_rate": 5.355814535468593e-05,
      "loss": 1.244,
      "step": 1842
    },
    {
      "epoch": 3.2735346358792183,
      "grad_norm": 1.0490493774414062,
      "learning_rate": 5.345916215037596e-05,
      "loss": 1.3288,
      "step": 1843
    },
    {
      "epoch": 3.275310834813499,
      "grad_norm": 1.0412800312042236,
      "learning_rate": 5.3360237119038716e-05,
      "loss": 1.5497,
      "step": 1844
    },
    {
      "epoch": 3.2770870337477795,
      "grad_norm": 1.038068413734436,
      "learning_rate": 5.3261370384323984e-05,
      "loss": 1.399,
      "step": 1845
    },
    {
      "epoch": 3.2788632326820606,
      "grad_norm": 0.9962735772132874,
      "learning_rate": 5.3162562069808696e-05,
      "loss": 1.2918,
      "step": 1846
    },
    {
      "epoch": 3.280639431616341,
      "grad_norm": 1.049436330795288,
      "learning_rate": 5.306381229899664e-05,
      "loss": 1.3026,
      "step": 1847
    },
    {
      "epoch": 3.282415630550622,
      "grad_norm": 1.090657114982605,
      "learning_rate": 5.2965121195318536e-05,
      "loss": 1.3019,
      "step": 1848
    },
    {
      "epoch": 3.2841918294849024,
      "grad_norm": 1.1103596687316895,
      "learning_rate": 5.286648888213179e-05,
      "loss": 1.4205,
      "step": 1849
    },
    {
      "epoch": 3.285968028419183,
      "grad_norm": 1.1168385744094849,
      "learning_rate": 5.2767915482720175e-05,
      "loss": 1.5696,
      "step": 1850
    },
    {
      "epoch": 3.2877442273534636,
      "grad_norm": 1.0862128734588623,
      "learning_rate": 5.266940112029406e-05,
      "loss": 1.6285,
      "step": 1851
    },
    {
      "epoch": 3.289520426287744,
      "grad_norm": 0.9936047792434692,
      "learning_rate": 5.2570945917989776e-05,
      "loss": 1.3184,
      "step": 1852
    },
    {
      "epoch": 3.291296625222025,
      "grad_norm": 1.1123030185699463,
      "learning_rate": 5.2472549998869915e-05,
      "loss": 1.3623,
      "step": 1853
    },
    {
      "epoch": 3.2930728241563054,
      "grad_norm": 1.0901720523834229,
      "learning_rate": 5.2374213485922784e-05,
      "loss": 1.3622,
      "step": 1854
    },
    {
      "epoch": 3.294849023090586,
      "grad_norm": 1.0471370220184326,
      "learning_rate": 5.2275936502062575e-05,
      "loss": 1.4613,
      "step": 1855
    },
    {
      "epoch": 3.2966252220248666,
      "grad_norm": 1.1568607091903687,
      "learning_rate": 5.217771917012907e-05,
      "loss": 1.64,
      "step": 1856
    },
    {
      "epoch": 3.2984014209591472,
      "grad_norm": 1.0524855852127075,
      "learning_rate": 5.207956161288732e-05,
      "loss": 1.4962,
      "step": 1857
    },
    {
      "epoch": 3.300177619893428,
      "grad_norm": 1.105089783668518,
      "learning_rate": 5.198146395302792e-05,
      "loss": 1.5227,
      "step": 1858
    },
    {
      "epoch": 3.301953818827709,
      "grad_norm": 1.0046799182891846,
      "learning_rate": 5.188342631316639e-05,
      "loss": 1.3742,
      "step": 1859
    },
    {
      "epoch": 3.3037300177619895,
      "grad_norm": 1.1138134002685547,
      "learning_rate": 5.178544881584333e-05,
      "loss": 1.7521,
      "step": 1860
    },
    {
      "epoch": 3.30550621669627,
      "grad_norm": 1.1111178398132324,
      "learning_rate": 5.168753158352415e-05,
      "loss": 1.4028,
      "step": 1861
    },
    {
      "epoch": 3.3072824156305507,
      "grad_norm": 1.140303373336792,
      "learning_rate": 5.158967473859888e-05,
      "loss": 1.3644,
      "step": 1862
    },
    {
      "epoch": 3.3090586145648313,
      "grad_norm": 1.0273438692092896,
      "learning_rate": 5.149187840338214e-05,
      "loss": 1.4295,
      "step": 1863
    },
    {
      "epoch": 3.310834813499112,
      "grad_norm": 1.0995681285858154,
      "learning_rate": 5.139414270011291e-05,
      "loss": 1.3504,
      "step": 1864
    },
    {
      "epoch": 3.3126110124333925,
      "grad_norm": 0.9998159408569336,
      "learning_rate": 5.1296467750954314e-05,
      "loss": 1.4072,
      "step": 1865
    },
    {
      "epoch": 3.314387211367673,
      "grad_norm": 1.0445266962051392,
      "learning_rate": 5.11988536779936e-05,
      "loss": 1.32,
      "step": 1866
    },
    {
      "epoch": 3.3161634103019537,
      "grad_norm": 1.101479411125183,
      "learning_rate": 5.110130060324192e-05,
      "loss": 1.6422,
      "step": 1867
    },
    {
      "epoch": 3.3179396092362343,
      "grad_norm": 1.1421079635620117,
      "learning_rate": 5.100380864863421e-05,
      "loss": 1.3292,
      "step": 1868
    },
    {
      "epoch": 3.319715808170515,
      "grad_norm": 1.1192529201507568,
      "learning_rate": 5.090637793602889e-05,
      "loss": 1.3571,
      "step": 1869
    },
    {
      "epoch": 3.321492007104796,
      "grad_norm": 1.0644574165344238,
      "learning_rate": 5.0809008587207965e-05,
      "loss": 1.6639,
      "step": 1870
    },
    {
      "epoch": 3.323268206039076,
      "grad_norm": 1.1371158361434937,
      "learning_rate": 5.071170072387672e-05,
      "loss": 1.4402,
      "step": 1871
    },
    {
      "epoch": 3.325044404973357,
      "grad_norm": 1.0755574703216553,
      "learning_rate": 5.061445446766349e-05,
      "loss": 1.4554,
      "step": 1872
    },
    {
      "epoch": 3.326820603907638,
      "grad_norm": 1.1243345737457275,
      "learning_rate": 5.051726994011969e-05,
      "loss": 1.3463,
      "step": 1873
    },
    {
      "epoch": 3.3285968028419184,
      "grad_norm": 1.089727520942688,
      "learning_rate": 5.042014726271957e-05,
      "loss": 1.372,
      "step": 1874
    },
    {
      "epoch": 3.330373001776199,
      "grad_norm": 1.1693603992462158,
      "learning_rate": 5.032308655686011e-05,
      "loss": 1.6366,
      "step": 1875
    },
    {
      "epoch": 3.3321492007104796,
      "grad_norm": 1.1382503509521484,
      "learning_rate": 5.022608794386068e-05,
      "loss": 1.6238,
      "step": 1876
    },
    {
      "epoch": 3.33392539964476,
      "grad_norm": 1.0305681228637695,
      "learning_rate": 5.012915154496319e-05,
      "loss": 1.1776,
      "step": 1877
    },
    {
      "epoch": 3.335701598579041,
      "grad_norm": 1.138103723526001,
      "learning_rate": 5.003227748133177e-05,
      "loss": 1.3279,
      "step": 1878
    },
    {
      "epoch": 3.3374777975133214,
      "grad_norm": 1.089134693145752,
      "learning_rate": 4.99354658740525e-05,
      "loss": 1.4477,
      "step": 1879
    },
    {
      "epoch": 3.339253996447602,
      "grad_norm": 1.133567452430725,
      "learning_rate": 4.9838716844133626e-05,
      "loss": 1.6006,
      "step": 1880
    },
    {
      "epoch": 3.3410301953818826,
      "grad_norm": 1.0609989166259766,
      "learning_rate": 4.9742030512504963e-05,
      "loss": 1.3393,
      "step": 1881
    },
    {
      "epoch": 3.342806394316163,
      "grad_norm": 1.0124709606170654,
      "learning_rate": 4.964540700001803e-05,
      "loss": 1.1828,
      "step": 1882
    },
    {
      "epoch": 3.3445825932504443,
      "grad_norm": 1.1236801147460938,
      "learning_rate": 4.954884642744585e-05,
      "loss": 1.3338,
      "step": 1883
    },
    {
      "epoch": 3.346358792184725,
      "grad_norm": 1.0850884914398193,
      "learning_rate": 4.945234891548277e-05,
      "loss": 1.4268,
      "step": 1884
    },
    {
      "epoch": 3.3481349911190055,
      "grad_norm": 1.091925859451294,
      "learning_rate": 4.9355914584744334e-05,
      "loss": 1.565,
      "step": 1885
    },
    {
      "epoch": 3.349911190053286,
      "grad_norm": 1.1514075994491577,
      "learning_rate": 4.925954355576701e-05,
      "loss": 1.6325,
      "step": 1886
    },
    {
      "epoch": 3.3516873889875667,
      "grad_norm": 1.15871000289917,
      "learning_rate": 4.916323594900826e-05,
      "loss": 1.5391,
      "step": 1887
    },
    {
      "epoch": 3.3534635879218473,
      "grad_norm": 1.0189355611801147,
      "learning_rate": 4.906699188484626e-05,
      "loss": 1.2674,
      "step": 1888
    },
    {
      "epoch": 3.355239786856128,
      "grad_norm": 1.096841812133789,
      "learning_rate": 4.897081148357964e-05,
      "loss": 1.3461,
      "step": 1889
    },
    {
      "epoch": 3.3570159857904085,
      "grad_norm": 1.0688989162445068,
      "learning_rate": 4.8874694865427676e-05,
      "loss": 1.4093,
      "step": 1890
    },
    {
      "epoch": 3.358792184724689,
      "grad_norm": 1.139940857887268,
      "learning_rate": 4.877864215052968e-05,
      "loss": 1.2515,
      "step": 1891
    },
    {
      "epoch": 3.3605683836589697,
      "grad_norm": 1.0956696271896362,
      "learning_rate": 4.868265345894526e-05,
      "loss": 1.463,
      "step": 1892
    },
    {
      "epoch": 3.3623445825932503,
      "grad_norm": 1.056410551071167,
      "learning_rate": 4.858672891065395e-05,
      "loss": 1.3428,
      "step": 1893
    },
    {
      "epoch": 3.364120781527531,
      "grad_norm": 1.0810210704803467,
      "learning_rate": 4.849086862555504e-05,
      "loss": 1.4817,
      "step": 1894
    },
    {
      "epoch": 3.3658969804618115,
      "grad_norm": 1.1318143606185913,
      "learning_rate": 4.839507272346758e-05,
      "loss": 1.4341,
      "step": 1895
    },
    {
      "epoch": 3.3676731793960926,
      "grad_norm": 1.0584518909454346,
      "learning_rate": 4.829934132413013e-05,
      "loss": 1.1975,
      "step": 1896
    },
    {
      "epoch": 3.369449378330373,
      "grad_norm": 1.1626660823822021,
      "learning_rate": 4.820367454720065e-05,
      "loss": 1.3302,
      "step": 1897
    },
    {
      "epoch": 3.3712255772646538,
      "grad_norm": 1.0845201015472412,
      "learning_rate": 4.8108072512256244e-05,
      "loss": 1.4656,
      "step": 1898
    },
    {
      "epoch": 3.3730017761989344,
      "grad_norm": 1.1195605993270874,
      "learning_rate": 4.801253533879307e-05,
      "loss": 1.6548,
      "step": 1899
    },
    {
      "epoch": 3.374777975133215,
      "grad_norm": 1.066386103630066,
      "learning_rate": 4.791706314622645e-05,
      "loss": 1.5262,
      "step": 1900
    },
    {
      "epoch": 3.374777975133215,
      "eval_loss": 2.050687789916992,
      "eval_runtime": 17.5179,
      "eval_samples_per_second": 57.141,
      "eval_steps_per_second": 28.599,
      "step": 1900
    },
    {
      "epoch": 3.3765541740674956,
      "grad_norm": 1.0027353763580322,
      "learning_rate": 4.7821656053890184e-05,
      "loss": 1.4311,
      "step": 1901
    },
    {
      "epoch": 3.378330373001776,
      "grad_norm": 1.158294916152954,
      "learning_rate": 4.772631418103688e-05,
      "loss": 1.2116,
      "step": 1902
    },
    {
      "epoch": 3.380106571936057,
      "grad_norm": 1.1235394477844238,
      "learning_rate": 4.763103764683762e-05,
      "loss": 1.5984,
      "step": 1903
    },
    {
      "epoch": 3.3818827708703374,
      "grad_norm": 1.139746069908142,
      "learning_rate": 4.753582657038169e-05,
      "loss": 1.4206,
      "step": 1904
    },
    {
      "epoch": 3.383658969804618,
      "grad_norm": 1.0147958993911743,
      "learning_rate": 4.7440681070676696e-05,
      "loss": 1.5862,
      "step": 1905
    },
    {
      "epoch": 3.3854351687388986,
      "grad_norm": 1.157562017440796,
      "learning_rate": 4.7345601266648234e-05,
      "loss": 1.3543,
      "step": 1906
    },
    {
      "epoch": 3.387211367673179,
      "grad_norm": 1.1000195741653442,
      "learning_rate": 4.7250587277139804e-05,
      "loss": 1.5256,
      "step": 1907
    },
    {
      "epoch": 3.38898756660746,
      "grad_norm": 1.0155277252197266,
      "learning_rate": 4.715563922091253e-05,
      "loss": 1.6464,
      "step": 1908
    },
    {
      "epoch": 3.390763765541741,
      "grad_norm": 1.0342185497283936,
      "learning_rate": 4.706075721664527e-05,
      "loss": 1.4985,
      "step": 1909
    },
    {
      "epoch": 3.3925399644760215,
      "grad_norm": 1.0552568435668945,
      "learning_rate": 4.6965941382934294e-05,
      "loss": 1.6444,
      "step": 1910
    },
    {
      "epoch": 3.394316163410302,
      "grad_norm": 1.1559544801712036,
      "learning_rate": 4.6871191838293046e-05,
      "loss": 1.4705,
      "step": 1911
    },
    {
      "epoch": 3.3960923623445827,
      "grad_norm": 1.0762925148010254,
      "learning_rate": 4.677650870115224e-05,
      "loss": 1.4251,
      "step": 1912
    },
    {
      "epoch": 3.3978685612788633,
      "grad_norm": 1.095782995223999,
      "learning_rate": 4.6681892089859546e-05,
      "loss": 1.426,
      "step": 1913
    },
    {
      "epoch": 3.399644760213144,
      "grad_norm": 1.0399808883666992,
      "learning_rate": 4.65873421226795e-05,
      "loss": 1.3461,
      "step": 1914
    },
    {
      "epoch": 3.4014209591474245,
      "grad_norm": 1.0014359951019287,
      "learning_rate": 4.649285891779327e-05,
      "loss": 1.3376,
      "step": 1915
    },
    {
      "epoch": 3.403197158081705,
      "grad_norm": 1.03327476978302,
      "learning_rate": 4.639844259329863e-05,
      "loss": 1.4684,
      "step": 1916
    },
    {
      "epoch": 3.4049733570159857,
      "grad_norm": 1.052683711051941,
      "learning_rate": 4.6304093267209794e-05,
      "loss": 1.2968,
      "step": 1917
    },
    {
      "epoch": 3.4067495559502663,
      "grad_norm": 1.1449977159500122,
      "learning_rate": 4.620981105745711e-05,
      "loss": 1.4572,
      "step": 1918
    },
    {
      "epoch": 3.408525754884547,
      "grad_norm": 1.0401054620742798,
      "learning_rate": 4.611559608188716e-05,
      "loss": 1.0527,
      "step": 1919
    },
    {
      "epoch": 3.410301953818828,
      "grad_norm": 1.1510218381881714,
      "learning_rate": 4.602144845826246e-05,
      "loss": 1.3981,
      "step": 1920
    },
    {
      "epoch": 3.412078152753108,
      "grad_norm": 1.1779810190200806,
      "learning_rate": 4.592736830426122e-05,
      "loss": 1.4235,
      "step": 1921
    },
    {
      "epoch": 3.413854351687389,
      "grad_norm": 1.1584643125534058,
      "learning_rate": 4.583335573747757e-05,
      "loss": 1.3883,
      "step": 1922
    },
    {
      "epoch": 3.4156305506216698,
      "grad_norm": 1.0270919799804688,
      "learning_rate": 4.573941087542088e-05,
      "loss": 1.5315,
      "step": 1923
    },
    {
      "epoch": 3.4174067495559504,
      "grad_norm": 1.2258005142211914,
      "learning_rate": 4.5645533835516076e-05,
      "loss": 1.5522,
      "step": 1924
    },
    {
      "epoch": 3.419182948490231,
      "grad_norm": 1.0379966497421265,
      "learning_rate": 4.555172473510329e-05,
      "loss": 1.4511,
      "step": 1925
    },
    {
      "epoch": 3.4209591474245116,
      "grad_norm": 1.0775089263916016,
      "learning_rate": 4.5457983691437614e-05,
      "loss": 1.5815,
      "step": 1926
    },
    {
      "epoch": 3.422735346358792,
      "grad_norm": 1.0562105178833008,
      "learning_rate": 4.5364310821689236e-05,
      "loss": 1.2664,
      "step": 1927
    },
    {
      "epoch": 3.424511545293073,
      "grad_norm": 1.4498718976974487,
      "learning_rate": 4.5270706242942964e-05,
      "loss": 1.3772,
      "step": 1928
    },
    {
      "epoch": 3.4262877442273534,
      "grad_norm": 1.1237870454788208,
      "learning_rate": 4.517717007219847e-05,
      "loss": 1.438,
      "step": 1929
    },
    {
      "epoch": 3.428063943161634,
      "grad_norm": 1.1117017269134521,
      "learning_rate": 4.5083702426369676e-05,
      "loss": 1.3805,
      "step": 1930
    },
    {
      "epoch": 3.4298401420959146,
      "grad_norm": 1.0698740482330322,
      "learning_rate": 4.4990303422285006e-05,
      "loss": 1.2755,
      "step": 1931
    },
    {
      "epoch": 3.431616341030195,
      "grad_norm": 1.070164680480957,
      "learning_rate": 4.4896973176687075e-05,
      "loss": 1.4982,
      "step": 1932
    },
    {
      "epoch": 3.4333925399644762,
      "grad_norm": 1.0640286207199097,
      "learning_rate": 4.4803711806232464e-05,
      "loss": 1.5609,
      "step": 1933
    },
    {
      "epoch": 3.435168738898757,
      "grad_norm": 1.0930413007736206,
      "learning_rate": 4.4710519427491745e-05,
      "loss": 1.6204,
      "step": 1934
    },
    {
      "epoch": 3.4369449378330375,
      "grad_norm": 1.0569239854812622,
      "learning_rate": 4.461739615694929e-05,
      "loss": 1.6066,
      "step": 1935
    },
    {
      "epoch": 3.438721136767318,
      "grad_norm": 1.0764873027801514,
      "learning_rate": 4.452434211100295e-05,
      "loss": 1.3558,
      "step": 1936
    },
    {
      "epoch": 3.4404973357015987,
      "grad_norm": 1.0294559001922607,
      "learning_rate": 4.443135740596418e-05,
      "loss": 1.1746,
      "step": 1937
    },
    {
      "epoch": 3.4422735346358793,
      "grad_norm": 1.0220413208007812,
      "learning_rate": 4.43384421580577e-05,
      "loss": 1.5295,
      "step": 1938
    },
    {
      "epoch": 3.44404973357016,
      "grad_norm": 1.0659925937652588,
      "learning_rate": 4.42455964834215e-05,
      "loss": 1.4753,
      "step": 1939
    },
    {
      "epoch": 3.4458259325044405,
      "grad_norm": 1.105203628540039,
      "learning_rate": 4.415282049810644e-05,
      "loss": 1.5235,
      "step": 1940
    },
    {
      "epoch": 3.447602131438721,
      "grad_norm": 1.1671185493469238,
      "learning_rate": 4.4060114318076417e-05,
      "loss": 1.6262,
      "step": 1941
    },
    {
      "epoch": 3.4493783303730017,
      "grad_norm": 1.1071847677230835,
      "learning_rate": 4.3967478059208066e-05,
      "loss": 1.1874,
      "step": 1942
    },
    {
      "epoch": 3.4511545293072823,
      "grad_norm": 1.0647435188293457,
      "learning_rate": 4.387491183729052e-05,
      "loss": 1.5647,
      "step": 1943
    },
    {
      "epoch": 3.452930728241563,
      "grad_norm": 1.0895241498947144,
      "learning_rate": 4.3782415768025485e-05,
      "loss": 1.6395,
      "step": 1944
    },
    {
      "epoch": 3.4547069271758435,
      "grad_norm": 1.0737557411193848,
      "learning_rate": 4.3689989967026935e-05,
      "loss": 1.4616,
      "step": 1945
    },
    {
      "epoch": 3.4564831261101245,
      "grad_norm": 1.1272401809692383,
      "learning_rate": 4.359763454982104e-05,
      "loss": 1.2988,
      "step": 1946
    },
    {
      "epoch": 3.458259325044405,
      "grad_norm": 1.0880684852600098,
      "learning_rate": 4.350534963184591e-05,
      "loss": 1.6519,
      "step": 1947
    },
    {
      "epoch": 3.4600355239786857,
      "grad_norm": 0.9099233150482178,
      "learning_rate": 4.341313532845162e-05,
      "loss": 1.3748,
      "step": 1948
    },
    {
      "epoch": 3.4618117229129663,
      "grad_norm": 1.075027346611023,
      "learning_rate": 4.332099175490002e-05,
      "loss": 1.5078,
      "step": 1949
    },
    {
      "epoch": 3.463587921847247,
      "grad_norm": 1.0387603044509888,
      "learning_rate": 4.322891902636435e-05,
      "loss": 1.2937,
      "step": 1950
    },
    {
      "epoch": 3.4653641207815276,
      "grad_norm": 1.0406978130340576,
      "learning_rate": 4.31369172579296e-05,
      "loss": 1.5114,
      "step": 1951
    },
    {
      "epoch": 3.467140319715808,
      "grad_norm": 1.1209431886672974,
      "learning_rate": 4.304498656459182e-05,
      "loss": 1.5327,
      "step": 1952
    },
    {
      "epoch": 3.4689165186500888,
      "grad_norm": 1.0796732902526855,
      "learning_rate": 4.295312706125824e-05,
      "loss": 1.3913,
      "step": 1953
    },
    {
      "epoch": 3.4706927175843694,
      "grad_norm": 1.107485055923462,
      "learning_rate": 4.286133886274731e-05,
      "loss": 1.3849,
      "step": 1954
    },
    {
      "epoch": 3.47246891651865,
      "grad_norm": 1.0477993488311768,
      "learning_rate": 4.276962208378811e-05,
      "loss": 1.5004,
      "step": 1955
    },
    {
      "epoch": 3.4742451154529306,
      "grad_norm": 1.104066252708435,
      "learning_rate": 4.267797683902063e-05,
      "loss": 1.3599,
      "step": 1956
    },
    {
      "epoch": 3.476021314387211,
      "grad_norm": 1.101604700088501,
      "learning_rate": 4.25864032429953e-05,
      "loss": 1.5194,
      "step": 1957
    },
    {
      "epoch": 3.477797513321492,
      "grad_norm": 1.0487689971923828,
      "learning_rate": 4.2494901410173084e-05,
      "loss": 1.5873,
      "step": 1958
    },
    {
      "epoch": 3.479573712255773,
      "grad_norm": 1.0447125434875488,
      "learning_rate": 4.2403471454925284e-05,
      "loss": 1.23,
      "step": 1959
    },
    {
      "epoch": 3.4813499111900534,
      "grad_norm": 1.118928074836731,
      "learning_rate": 4.231211349153319e-05,
      "loss": 1.2681,
      "step": 1960
    },
    {
      "epoch": 3.483126110124334,
      "grad_norm": 1.14365553855896,
      "learning_rate": 4.222082763418836e-05,
      "loss": 1.201,
      "step": 1961
    },
    {
      "epoch": 3.4849023090586146,
      "grad_norm": 1.065850853919983,
      "learning_rate": 4.212961399699199e-05,
      "loss": 1.5862,
      "step": 1962
    },
    {
      "epoch": 3.4866785079928952,
      "grad_norm": 1.070366621017456,
      "learning_rate": 4.2038472693955124e-05,
      "loss": 1.4956,
      "step": 1963
    },
    {
      "epoch": 3.488454706927176,
      "grad_norm": 1.1631715297698975,
      "learning_rate": 4.194740383899841e-05,
      "loss": 1.4126,
      "step": 1964
    },
    {
      "epoch": 3.4902309058614565,
      "grad_norm": 1.129729151725769,
      "learning_rate": 4.1856407545951834e-05,
      "loss": 1.2494,
      "step": 1965
    },
    {
      "epoch": 3.492007104795737,
      "grad_norm": 1.0552763938903809,
      "learning_rate": 4.176548392855477e-05,
      "loss": 1.2783,
      "step": 1966
    },
    {
      "epoch": 3.4937833037300177,
      "grad_norm": 1.0787051916122437,
      "learning_rate": 4.167463310045576e-05,
      "loss": 1.3997,
      "step": 1967
    },
    {
      "epoch": 3.4955595026642983,
      "grad_norm": 1.1526240110397339,
      "learning_rate": 4.158385517521235e-05,
      "loss": 1.335,
      "step": 1968
    },
    {
      "epoch": 3.497335701598579,
      "grad_norm": 1.0800906419754028,
      "learning_rate": 4.1493150266290924e-05,
      "loss": 1.3382,
      "step": 1969
    },
    {
      "epoch": 3.49911190053286,
      "grad_norm": 1.0814611911773682,
      "learning_rate": 4.1402518487066555e-05,
      "loss": 1.3869,
      "step": 1970
    },
    {
      "epoch": 3.50088809946714,
      "grad_norm": 1.1629329919815063,
      "learning_rate": 4.131195995082312e-05,
      "loss": 1.4074,
      "step": 1971
    },
    {
      "epoch": 3.502664298401421,
      "grad_norm": 1.2110309600830078,
      "learning_rate": 4.12214747707527e-05,
      "loss": 1.4182,
      "step": 1972
    },
    {
      "epoch": 3.5044404973357017,
      "grad_norm": 1.259371280670166,
      "learning_rate": 4.113106305995582e-05,
      "loss": 1.3598,
      "step": 1973
    },
    {
      "epoch": 3.5062166962699823,
      "grad_norm": 1.1323922872543335,
      "learning_rate": 4.10407249314412e-05,
      "loss": 1.2536,
      "step": 1974
    },
    {
      "epoch": 3.507992895204263,
      "grad_norm": 1.1809675693511963,
      "learning_rate": 4.095046049812545e-05,
      "loss": 1.4791,
      "step": 1975
    },
    {
      "epoch": 3.5097690941385435,
      "grad_norm": 1.0650463104248047,
      "learning_rate": 4.086026987283318e-05,
      "loss": 1.4745,
      "step": 1976
    },
    {
      "epoch": 3.511545293072824,
      "grad_norm": 1.2307205200195312,
      "learning_rate": 4.077015316829672e-05,
      "loss": 1.3375,
      "step": 1977
    },
    {
      "epoch": 3.5133214920071048,
      "grad_norm": 1.083781361579895,
      "learning_rate": 4.0680110497156045e-05,
      "loss": 1.4491,
      "step": 1978
    },
    {
      "epoch": 3.5150976909413854,
      "grad_norm": 1.1480668783187866,
      "learning_rate": 4.059014197195845e-05,
      "loss": 1.3606,
      "step": 1979
    },
    {
      "epoch": 3.516873889875666,
      "grad_norm": 1.1230388879776,
      "learning_rate": 4.050024770515869e-05,
      "loss": 1.244,
      "step": 1980
    },
    {
      "epoch": 3.5186500888099466,
      "grad_norm": 1.101178526878357,
      "learning_rate": 4.041042780911871e-05,
      "loss": 1.4722,
      "step": 1981
    },
    {
      "epoch": 3.520426287744227,
      "grad_norm": 1.1045626401901245,
      "learning_rate": 4.0320682396107334e-05,
      "loss": 1.3446,
      "step": 1982
    },
    {
      "epoch": 3.522202486678508,
      "grad_norm": 1.095078706741333,
      "learning_rate": 4.023101157830053e-05,
      "loss": 1.2932,
      "step": 1983
    },
    {
      "epoch": 3.5239786856127884,
      "grad_norm": 1.1361110210418701,
      "learning_rate": 4.01414154677808e-05,
      "loss": 1.3725,
      "step": 1984
    },
    {
      "epoch": 3.5257548845470694,
      "grad_norm": 1.0653564929962158,
      "learning_rate": 4.005189417653743e-05,
      "loss": 1.5269,
      "step": 1985
    },
    {
      "epoch": 3.52753108348135,
      "grad_norm": 1.1056482791900635,
      "learning_rate": 3.9962447816466067e-05,
      "loss": 1.3133,
      "step": 1986
    },
    {
      "epoch": 3.5293072824156306,
      "grad_norm": 1.1433920860290527,
      "learning_rate": 3.987307649936875e-05,
      "loss": 1.2834,
      "step": 1987
    },
    {
      "epoch": 3.5310834813499112,
      "grad_norm": 1.1474870443344116,
      "learning_rate": 3.978378033695379e-05,
      "loss": 1.5201,
      "step": 1988
    },
    {
      "epoch": 3.532859680284192,
      "grad_norm": 1.0402657985687256,
      "learning_rate": 3.969455944083541e-05,
      "loss": 1.0754,
      "step": 1989
    },
    {
      "epoch": 3.5346358792184724,
      "grad_norm": 1.0969603061676025,
      "learning_rate": 3.9605413922533874e-05,
      "loss": 1.2843,
      "step": 1990
    },
    {
      "epoch": 3.536412078152753,
      "grad_norm": 1.0953115224838257,
      "learning_rate": 3.951634389347522e-05,
      "loss": 1.5237,
      "step": 1991
    },
    {
      "epoch": 3.5381882770870337,
      "grad_norm": 1.070373296737671,
      "learning_rate": 3.942734946499099e-05,
      "loss": 1.6393,
      "step": 1992
    },
    {
      "epoch": 3.5399644760213143,
      "grad_norm": 1.1228113174438477,
      "learning_rate": 3.93384307483185e-05,
      "loss": 1.4619,
      "step": 1993
    },
    {
      "epoch": 3.5417406749555953,
      "grad_norm": 1.1329989433288574,
      "learning_rate": 3.9249587854600145e-05,
      "loss": 1.3466,
      "step": 1994
    },
    {
      "epoch": 3.5435168738898755,
      "grad_norm": 1.0556336641311646,
      "learning_rate": 3.916082089488372e-05,
      "loss": 1.3084,
      "step": 1995
    },
    {
      "epoch": 3.5452930728241565,
      "grad_norm": 1.0533862113952637,
      "learning_rate": 3.9072129980122105e-05,
      "loss": 1.3371,
      "step": 1996
    },
    {
      "epoch": 3.5470692717584367,
      "grad_norm": 1.1306347846984863,
      "learning_rate": 3.898351522117299e-05,
      "loss": 1.5629,
      "step": 1997
    },
    {
      "epoch": 3.5488454706927177,
      "grad_norm": 1.1135751008987427,
      "learning_rate": 3.889497672879906e-05,
      "loss": 1.4459,
      "step": 1998
    },
    {
      "epoch": 3.5506216696269983,
      "grad_norm": 0.980671226978302,
      "learning_rate": 3.880651461366747e-05,
      "loss": 1.2885,
      "step": 1999
    },
    {
      "epoch": 3.552397868561279,
      "grad_norm": 1.1009517908096313,
      "learning_rate": 3.8718128986350156e-05,
      "loss": 1.5532,
      "step": 2000
    },
    {
      "epoch": 3.552397868561279,
      "eval_loss": 2.0511550903320312,
      "eval_runtime": 17.4726,
      "eval_samples_per_second": 57.29,
      "eval_steps_per_second": 28.673,
      "step": 2000
    },
    {
      "epoch": 3.5541740674955595,
      "grad_norm": 1.1052039861679077,
      "learning_rate": 3.862981995732321e-05,
      "loss": 1.6765,
      "step": 2001
    },
    {
      "epoch": 3.55595026642984,
      "grad_norm": 1.083280324935913,
      "learning_rate": 3.8541587636967124e-05,
      "loss": 1.3288,
      "step": 2002
    },
    {
      "epoch": 3.5577264653641207,
      "grad_norm": 1.0587209463119507,
      "learning_rate": 3.8453432135566506e-05,
      "loss": 1.4152,
      "step": 2003
    },
    {
      "epoch": 3.5595026642984013,
      "grad_norm": 1.150234580039978,
      "learning_rate": 3.8365353563309824e-05,
      "loss": 1.6906,
      "step": 2004
    },
    {
      "epoch": 3.561278863232682,
      "grad_norm": 1.1550483703613281,
      "learning_rate": 3.827735203028953e-05,
      "loss": 1.5719,
      "step": 2005
    },
    {
      "epoch": 3.5630550621669625,
      "grad_norm": 1.1788091659545898,
      "learning_rate": 3.8189427646501744e-05,
      "loss": 1.5173,
      "step": 2006
    },
    {
      "epoch": 3.5648312611012436,
      "grad_norm": 1.2197484970092773,
      "learning_rate": 3.810158052184608e-05,
      "loss": 1.3983,
      "step": 2007
    },
    {
      "epoch": 3.5666074600355238,
      "grad_norm": 1.1934890747070312,
      "learning_rate": 3.801381076612567e-05,
      "loss": 1.3034,
      "step": 2008
    },
    {
      "epoch": 3.568383658969805,
      "grad_norm": 1.0916544198989868,
      "learning_rate": 3.792611848904691e-05,
      "loss": 1.4527,
      "step": 2009
    },
    {
      "epoch": 3.5701598579040854,
      "grad_norm": 1.1657888889312744,
      "learning_rate": 3.7838503800219396e-05,
      "loss": 1.477,
      "step": 2010
    },
    {
      "epoch": 3.571936056838366,
      "grad_norm": 1.0965913534164429,
      "learning_rate": 3.775096680915563e-05,
      "loss": 1.4183,
      "step": 2011
    },
    {
      "epoch": 3.5737122557726466,
      "grad_norm": 1.1780571937561035,
      "learning_rate": 3.766350762527109e-05,
      "loss": 1.6433,
      "step": 2012
    },
    {
      "epoch": 3.575488454706927,
      "grad_norm": 1.0754345655441284,
      "learning_rate": 3.757612635788401e-05,
      "loss": 1.4334,
      "step": 2013
    },
    {
      "epoch": 3.577264653641208,
      "grad_norm": 1.0553926229476929,
      "learning_rate": 3.7488823116215166e-05,
      "loss": 1.4858,
      "step": 2014
    },
    {
      "epoch": 3.5790408525754884,
      "grad_norm": 1.142409324645996,
      "learning_rate": 3.740159800938784e-05,
      "loss": 1.3652,
      "step": 2015
    },
    {
      "epoch": 3.580817051509769,
      "grad_norm": 1.12831711769104,
      "learning_rate": 3.7314451146427666e-05,
      "loss": 1.3612,
      "step": 2016
    },
    {
      "epoch": 3.5825932504440496,
      "grad_norm": 1.1545954942703247,
      "learning_rate": 3.7227382636262506e-05,
      "loss": 1.4584,
      "step": 2017
    },
    {
      "epoch": 3.5843694493783302,
      "grad_norm": 1.0143197774887085,
      "learning_rate": 3.7140392587722185e-05,
      "loss": 1.448,
      "step": 2018
    },
    {
      "epoch": 3.586145648312611,
      "grad_norm": 1.0799744129180908,
      "learning_rate": 3.7053481109538526e-05,
      "loss": 1.5047,
      "step": 2019
    },
    {
      "epoch": 3.587921847246892,
      "grad_norm": 1.110858678817749,
      "learning_rate": 3.6966648310345185e-05,
      "loss": 1.2965,
      "step": 2020
    },
    {
      "epoch": 3.589698046181172,
      "grad_norm": 1.0504409074783325,
      "learning_rate": 3.687989429867732e-05,
      "loss": 1.1474,
      "step": 2021
    },
    {
      "epoch": 3.591474245115453,
      "grad_norm": 1.0886286497116089,
      "learning_rate": 3.6793219182971846e-05,
      "loss": 1.4501,
      "step": 2022
    },
    {
      "epoch": 3.5932504440497337,
      "grad_norm": 1.1528801918029785,
      "learning_rate": 3.670662307156688e-05,
      "loss": 1.7079,
      "step": 2023
    },
    {
      "epoch": 3.5950266429840143,
      "grad_norm": 1.0595293045043945,
      "learning_rate": 3.6620106072701754e-05,
      "loss": 1.4496,
      "step": 2024
    },
    {
      "epoch": 3.596802841918295,
      "grad_norm": 1.117262840270996,
      "learning_rate": 3.6533668294517155e-05,
      "loss": 1.4981,
      "step": 2025
    },
    {
      "epoch": 3.5985790408525755,
      "grad_norm": 1.0525046586990356,
      "learning_rate": 3.6447309845054465e-05,
      "loss": 1.6016,
      "step": 2026
    },
    {
      "epoch": 3.600355239786856,
      "grad_norm": 1.117070198059082,
      "learning_rate": 3.636103083225614e-05,
      "loss": 1.5017,
      "step": 2027
    },
    {
      "epoch": 3.6021314387211367,
      "grad_norm": 1.1861525774002075,
      "learning_rate": 3.627483136396517e-05,
      "loss": 1.629,
      "step": 2028
    },
    {
      "epoch": 3.6039076376554173,
      "grad_norm": 1.1597572565078735,
      "learning_rate": 3.6188711547925215e-05,
      "loss": 1.3232,
      "step": 2029
    },
    {
      "epoch": 3.605683836589698,
      "grad_norm": 1.1869094371795654,
      "learning_rate": 3.6102671491780393e-05,
      "loss": 1.2679,
      "step": 2030
    },
    {
      "epoch": 3.6074600355239785,
      "grad_norm": 1.1516889333724976,
      "learning_rate": 3.6016711303075e-05,
      "loss": 1.3675,
      "step": 2031
    },
    {
      "epoch": 3.609236234458259,
      "grad_norm": 1.2474884986877441,
      "learning_rate": 3.5930831089253735e-05,
      "loss": 1.3919,
      "step": 2032
    },
    {
      "epoch": 3.61101243339254,
      "grad_norm": 1.182405948638916,
      "learning_rate": 3.5845030957661076e-05,
      "loss": 1.6557,
      "step": 2033
    },
    {
      "epoch": 3.6127886323268203,
      "grad_norm": 1.2159252166748047,
      "learning_rate": 3.5759311015541555e-05,
      "loss": 1.3782,
      "step": 2034
    },
    {
      "epoch": 3.6145648312611014,
      "grad_norm": 1.1590663194656372,
      "learning_rate": 3.5673671370039466e-05,
      "loss": 1.414,
      "step": 2035
    },
    {
      "epoch": 3.616341030195382,
      "grad_norm": 1.0275675058364868,
      "learning_rate": 3.5588112128198645e-05,
      "loss": 1.2126,
      "step": 2036
    },
    {
      "epoch": 3.6181172291296626,
      "grad_norm": 1.213402509689331,
      "learning_rate": 3.550263339696254e-05,
      "loss": 1.2624,
      "step": 2037
    },
    {
      "epoch": 3.619893428063943,
      "grad_norm": 1.0521084070205688,
      "learning_rate": 3.54172352831739e-05,
      "loss": 1.3258,
      "step": 2038
    },
    {
      "epoch": 3.621669626998224,
      "grad_norm": 1.1516962051391602,
      "learning_rate": 3.533191789357477e-05,
      "loss": 1.5588,
      "step": 2039
    },
    {
      "epoch": 3.6234458259325044,
      "grad_norm": 1.1503486633300781,
      "learning_rate": 3.5246681334806175e-05,
      "loss": 1.4459,
      "step": 2040
    },
    {
      "epoch": 3.625222024866785,
      "grad_norm": 1.0843669176101685,
      "learning_rate": 3.516152571340823e-05,
      "loss": 1.7668,
      "step": 2041
    },
    {
      "epoch": 3.6269982238010656,
      "grad_norm": 1.0348631143569946,
      "learning_rate": 3.5076451135819844e-05,
      "loss": 1.3913,
      "step": 2042
    },
    {
      "epoch": 3.6287744227353462,
      "grad_norm": 1.1092771291732788,
      "learning_rate": 3.499145770837858e-05,
      "loss": 1.7565,
      "step": 2043
    },
    {
      "epoch": 3.630550621669627,
      "grad_norm": 1.1243544816970825,
      "learning_rate": 3.490654553732062e-05,
      "loss": 1.5837,
      "step": 2044
    },
    {
      "epoch": 3.6323268206039074,
      "grad_norm": 1.224273443222046,
      "learning_rate": 3.482171472878062e-05,
      "loss": 1.6446,
      "step": 2045
    },
    {
      "epoch": 3.6341030195381885,
      "grad_norm": 1.0486154556274414,
      "learning_rate": 3.473696538879141e-05,
      "loss": 1.2796,
      "step": 2046
    },
    {
      "epoch": 3.6358792184724686,
      "grad_norm": 1.004062294960022,
      "learning_rate": 3.4652297623284104e-05,
      "loss": 1.6458,
      "step": 2047
    },
    {
      "epoch": 3.6376554174067497,
      "grad_norm": 1.086116909980774,
      "learning_rate": 3.45677115380878e-05,
      "loss": 1.4226,
      "step": 2048
    },
    {
      "epoch": 3.6394316163410303,
      "grad_norm": 1.07793128490448,
      "learning_rate": 3.448320723892957e-05,
      "loss": 1.1286,
      "step": 2049
    },
    {
      "epoch": 3.641207815275311,
      "grad_norm": 1.035266399383545,
      "learning_rate": 3.439878483143413e-05,
      "loss": 1.6744,
      "step": 2050
    },
    {
      "epoch": 3.6429840142095915,
      "grad_norm": 1.2461204528808594,
      "learning_rate": 3.431444442112395e-05,
      "loss": 1.7753,
      "step": 2051
    },
    {
      "epoch": 3.644760213143872,
      "grad_norm": 1.154484510421753,
      "learning_rate": 3.4230186113419e-05,
      "loss": 1.7448,
      "step": 2052
    },
    {
      "epoch": 3.6465364120781527,
      "grad_norm": 1.1917140483856201,
      "learning_rate": 3.414601001363649e-05,
      "loss": 1.2249,
      "step": 2053
    },
    {
      "epoch": 3.6483126110124333,
      "grad_norm": 1.1389023065567017,
      "learning_rate": 3.406191622699113e-05,
      "loss": 1.341,
      "step": 2054
    },
    {
      "epoch": 3.650088809946714,
      "grad_norm": 1.159632682800293,
      "learning_rate": 3.39779048585945e-05,
      "loss": 1.5422,
      "step": 2055
    },
    {
      "epoch": 3.6518650088809945,
      "grad_norm": 1.051576018333435,
      "learning_rate": 3.389397601345532e-05,
      "loss": 1.4269,
      "step": 2056
    },
    {
      "epoch": 3.6536412078152756,
      "grad_norm": 1.137779712677002,
      "learning_rate": 3.381012979647904e-05,
      "loss": 1.4753,
      "step": 2057
    },
    {
      "epoch": 3.6554174067495557,
      "grad_norm": 1.0670201778411865,
      "learning_rate": 3.3726366312467916e-05,
      "loss": 1.127,
      "step": 2058
    },
    {
      "epoch": 3.657193605683837,
      "grad_norm": 1.1156136989593506,
      "learning_rate": 3.364268566612082e-05,
      "loss": 2.0069,
      "step": 2059
    },
    {
      "epoch": 3.6589698046181174,
      "grad_norm": 1.1633177995681763,
      "learning_rate": 3.355908796203296e-05,
      "loss": 1.5383,
      "step": 2060
    },
    {
      "epoch": 3.660746003552398,
      "grad_norm": 1.1433814764022827,
      "learning_rate": 3.3475573304695953e-05,
      "loss": 1.5631,
      "step": 2061
    },
    {
      "epoch": 3.6625222024866786,
      "grad_norm": 1.0837042331695557,
      "learning_rate": 3.339214179849767e-05,
      "loss": 1.456,
      "step": 2062
    },
    {
      "epoch": 3.664298401420959,
      "grad_norm": 1.1692091226577759,
      "learning_rate": 3.330879354772185e-05,
      "loss": 1.4818,
      "step": 2063
    },
    {
      "epoch": 3.66607460035524,
      "grad_norm": 1.0764104127883911,
      "learning_rate": 3.3225528656548464e-05,
      "loss": 1.3811,
      "step": 2064
    },
    {
      "epoch": 3.6678507992895204,
      "grad_norm": 1.1752861738204956,
      "learning_rate": 3.3142347229053015e-05,
      "loss": 1.5069,
      "step": 2065
    },
    {
      "epoch": 3.669626998223801,
      "grad_norm": 1.05287504196167,
      "learning_rate": 3.3059249369206826e-05,
      "loss": 1.3584,
      "step": 2066
    },
    {
      "epoch": 3.6714031971580816,
      "grad_norm": 1.204601526260376,
      "learning_rate": 3.297623518087676e-05,
      "loss": 1.4812,
      "step": 2067
    },
    {
      "epoch": 3.673179396092362,
      "grad_norm": 1.1011037826538086,
      "learning_rate": 3.2893304767824996e-05,
      "loss": 1.4296,
      "step": 2068
    },
    {
      "epoch": 3.674955595026643,
      "grad_norm": 1.1230741739273071,
      "learning_rate": 3.281045823370913e-05,
      "loss": 1.2563,
      "step": 2069
    },
    {
      "epoch": 3.676731793960924,
      "grad_norm": 1.1433664560317993,
      "learning_rate": 3.272769568208183e-05,
      "loss": 1.6522,
      "step": 2070
    },
    {
      "epoch": 3.678507992895204,
      "grad_norm": 1.1254918575286865,
      "learning_rate": 3.264501721639086e-05,
      "loss": 1.5073,
      "step": 2071
    },
    {
      "epoch": 3.680284191829485,
      "grad_norm": 1.0818065404891968,
      "learning_rate": 3.2562422939978786e-05,
      "loss": 1.1731,
      "step": 2072
    },
    {
      "epoch": 3.6820603907637657,
      "grad_norm": 1.1211386919021606,
      "learning_rate": 3.247991295608301e-05,
      "loss": 1.3807,
      "step": 2073
    },
    {
      "epoch": 3.6838365896980463,
      "grad_norm": 1.0887376070022583,
      "learning_rate": 3.239748736783561e-05,
      "loss": 1.3259,
      "step": 2074
    },
    {
      "epoch": 3.685612788632327,
      "grad_norm": 1.1433992385864258,
      "learning_rate": 3.231514627826305e-05,
      "loss": 1.4131,
      "step": 2075
    },
    {
      "epoch": 3.6873889875666075,
      "grad_norm": 1.163492202758789,
      "learning_rate": 3.2232889790286305e-05,
      "loss": 1.6121,
      "step": 2076
    },
    {
      "epoch": 3.689165186500888,
      "grad_norm": 0.9530165195465088,
      "learning_rate": 3.215071800672055e-05,
      "loss": 1.2531,
      "step": 2077
    },
    {
      "epoch": 3.6909413854351687,
      "grad_norm": 1.045608401298523,
      "learning_rate": 3.2068631030275056e-05,
      "loss": 1.5253,
      "step": 2078
    },
    {
      "epoch": 3.6927175843694493,
      "grad_norm": 1.0834242105484009,
      "learning_rate": 3.198662896355312e-05,
      "loss": 1.2407,
      "step": 2079
    },
    {
      "epoch": 3.69449378330373,
      "grad_norm": 1.1003338098526,
      "learning_rate": 3.190471190905193e-05,
      "loss": 1.6755,
      "step": 2080
    },
    {
      "epoch": 3.6962699822380105,
      "grad_norm": 1.0994842052459717,
      "learning_rate": 3.182287996916243e-05,
      "loss": 1.6867,
      "step": 2081
    },
    {
      "epoch": 3.698046181172291,
      "grad_norm": 1.1390236616134644,
      "learning_rate": 3.1741133246169056e-05,
      "loss": 1.6291,
      "step": 2082
    },
    {
      "epoch": 3.699822380106572,
      "grad_norm": 1.1017299890518188,
      "learning_rate": 3.165947184224985e-05,
      "loss": 1.3227,
      "step": 2083
    },
    {
      "epoch": 3.7015985790408523,
      "grad_norm": 1.1354396343231201,
      "learning_rate": 3.1577895859476215e-05,
      "loss": 1.5387,
      "step": 2084
    },
    {
      "epoch": 3.7033747779751334,
      "grad_norm": 1.0884323120117188,
      "learning_rate": 3.149640539981267e-05,
      "loss": 1.6457,
      "step": 2085
    },
    {
      "epoch": 3.705150976909414,
      "grad_norm": 1.0466173887252808,
      "learning_rate": 3.1415000565116926e-05,
      "loss": 1.3588,
      "step": 2086
    },
    {
      "epoch": 3.7069271758436946,
      "grad_norm": 1.090470552444458,
      "learning_rate": 3.1333681457139666e-05,
      "loss": 1.4714,
      "step": 2087
    },
    {
      "epoch": 3.708703374777975,
      "grad_norm": 1.097572922706604,
      "learning_rate": 3.125244817752442e-05,
      "loss": 1.5955,
      "step": 2088
    },
    {
      "epoch": 3.710479573712256,
      "grad_norm": 1.138367772102356,
      "learning_rate": 3.117130082780737e-05,
      "loss": 1.374,
      "step": 2089
    },
    {
      "epoch": 3.7122557726465364,
      "grad_norm": 1.1557339429855347,
      "learning_rate": 3.109023950941736e-05,
      "loss": 1.5098,
      "step": 2090
    },
    {
      "epoch": 3.714031971580817,
      "grad_norm": 1.0340180397033691,
      "learning_rate": 3.1009264323675734e-05,
      "loss": 1.5578,
      "step": 2091
    },
    {
      "epoch": 3.7158081705150976,
      "grad_norm": 1.0969414710998535,
      "learning_rate": 3.092837537179602e-05,
      "loss": 1.5503,
      "step": 2092
    },
    {
      "epoch": 3.717584369449378,
      "grad_norm": 1.0989428758621216,
      "learning_rate": 3.084757275488419e-05,
      "loss": 1.2116,
      "step": 2093
    },
    {
      "epoch": 3.719360568383659,
      "grad_norm": 1.1052607297897339,
      "learning_rate": 3.0766856573938105e-05,
      "loss": 1.6646,
      "step": 2094
    },
    {
      "epoch": 3.7211367673179394,
      "grad_norm": 1.1107698678970337,
      "learning_rate": 3.068622692984762e-05,
      "loss": 1.3864,
      "step": 2095
    },
    {
      "epoch": 3.7229129662522205,
      "grad_norm": 1.1393194198608398,
      "learning_rate": 3.060568392339457e-05,
      "loss": 1.2875,
      "step": 2096
    },
    {
      "epoch": 3.7246891651865006,
      "grad_norm": 1.0741033554077148,
      "learning_rate": 3.05252276552523e-05,
      "loss": 1.4493,
      "step": 2097
    },
    {
      "epoch": 3.7264653641207817,
      "grad_norm": 1.036516547203064,
      "learning_rate": 3.0444858225985862e-05,
      "loss": 1.8122,
      "step": 2098
    },
    {
      "epoch": 3.7282415630550623,
      "grad_norm": 1.134826421737671,
      "learning_rate": 3.036457573605177e-05,
      "loss": 1.6034,
      "step": 2099
    },
    {
      "epoch": 3.730017761989343,
      "grad_norm": 1.075326681137085,
      "learning_rate": 3.0284380285797766e-05,
      "loss": 1.5039,
      "step": 2100
    },
    {
      "epoch": 3.730017761989343,
      "eval_loss": 2.051356792449951,
      "eval_runtime": 17.544,
      "eval_samples_per_second": 57.056,
      "eval_steps_per_second": 28.557,
      "step": 2100
    },
    {
      "epoch": 3.7317939609236235,
      "grad_norm": 1.1656877994537354,
      "learning_rate": 3.0204271975462928e-05,
      "loss": 1.3499,
      "step": 2101
    },
    {
      "epoch": 3.733570159857904,
      "grad_norm": 1.0729142427444458,
      "learning_rate": 3.0124250905177265e-05,
      "loss": 1.0801,
      "step": 2102
    },
    {
      "epoch": 3.7353463587921847,
      "grad_norm": 1.126194715499878,
      "learning_rate": 3.0044317174961946e-05,
      "loss": 1.427,
      "step": 2103
    },
    {
      "epoch": 3.7371225577264653,
      "grad_norm": 1.08242928981781,
      "learning_rate": 2.9964470884728758e-05,
      "loss": 1.4196,
      "step": 2104
    },
    {
      "epoch": 3.738898756660746,
      "grad_norm": 1.1509464979171753,
      "learning_rate": 2.9884712134280324e-05,
      "loss": 1.4783,
      "step": 2105
    },
    {
      "epoch": 3.7406749555950265,
      "grad_norm": 1.162174105644226,
      "learning_rate": 2.9805041023309844e-05,
      "loss": 1.1668,
      "step": 2106
    },
    {
      "epoch": 3.7424511545293075,
      "grad_norm": 1.2353051900863647,
      "learning_rate": 2.9725457651400877e-05,
      "loss": 1.5464,
      "step": 2107
    },
    {
      "epoch": 3.7442273534635877,
      "grad_norm": 1.2930132150650024,
      "learning_rate": 2.9645962118027414e-05,
      "loss": 1.4135,
      "step": 2108
    },
    {
      "epoch": 3.7460035523978688,
      "grad_norm": 1.1779890060424805,
      "learning_rate": 2.9566554522553626e-05,
      "loss": 1.4898,
      "step": 2109
    },
    {
      "epoch": 3.7477797513321494,
      "grad_norm": 1.1019542217254639,
      "learning_rate": 2.948723496423379e-05,
      "loss": 1.3589,
      "step": 2110
    },
    {
      "epoch": 3.74955595026643,
      "grad_norm": 1.076531171798706,
      "learning_rate": 2.940800354221205e-05,
      "loss": 1.236,
      "step": 2111
    },
    {
      "epoch": 3.7513321492007106,
      "grad_norm": 1.1625800132751465,
      "learning_rate": 2.932886035552248e-05,
      "loss": 1.459,
      "step": 2112
    },
    {
      "epoch": 3.753108348134991,
      "grad_norm": 1.0546910762786865,
      "learning_rate": 2.924980550308887e-05,
      "loss": 1.3955,
      "step": 2113
    },
    {
      "epoch": 3.7548845470692718,
      "grad_norm": 1.1046327352523804,
      "learning_rate": 2.917083908372449e-05,
      "loss": 1.4026,
      "step": 2114
    },
    {
      "epoch": 3.7566607460035524,
      "grad_norm": 1.0472081899642944,
      "learning_rate": 2.909196119613218e-05,
      "loss": 1.5677,
      "step": 2115
    },
    {
      "epoch": 3.758436944937833,
      "grad_norm": 1.1297235488891602,
      "learning_rate": 2.901317193890414e-05,
      "loss": 1.3668,
      "step": 2116
    },
    {
      "epoch": 3.7602131438721136,
      "grad_norm": 1.0757532119750977,
      "learning_rate": 2.8934471410521656e-05,
      "loss": 1.49,
      "step": 2117
    },
    {
      "epoch": 3.761989342806394,
      "grad_norm": 1.1000710725784302,
      "learning_rate": 2.8855859709355216e-05,
      "loss": 1.5137,
      "step": 2118
    },
    {
      "epoch": 3.763765541740675,
      "grad_norm": 1.1026667356491089,
      "learning_rate": 2.8777336933664277e-05,
      "loss": 1.4432,
      "step": 2119
    },
    {
      "epoch": 3.765541740674956,
      "grad_norm": 1.0878088474273682,
      "learning_rate": 2.8698903181597127e-05,
      "loss": 1.4802,
      "step": 2120
    },
    {
      "epoch": 3.767317939609236,
      "grad_norm": 1.3722575902938843,
      "learning_rate": 2.8620558551190722e-05,
      "loss": 1.4523,
      "step": 2121
    },
    {
      "epoch": 3.769094138543517,
      "grad_norm": 1.101001262664795,
      "learning_rate": 2.8542303140370718e-05,
      "loss": 1.3646,
      "step": 2122
    },
    {
      "epoch": 3.7708703374777977,
      "grad_norm": 1.0935676097869873,
      "learning_rate": 2.846413704695122e-05,
      "loss": 1.2983,
      "step": 2123
    },
    {
      "epoch": 3.7726465364120783,
      "grad_norm": 1.1594438552856445,
      "learning_rate": 2.8386060368634593e-05,
      "loss": 1.5788,
      "step": 2124
    },
    {
      "epoch": 3.774422735346359,
      "grad_norm": 1.1345628499984741,
      "learning_rate": 2.8308073203011663e-05,
      "loss": 1.3455,
      "step": 2125
    },
    {
      "epoch": 3.7761989342806395,
      "grad_norm": 1.1157679557800293,
      "learning_rate": 2.823017564756113e-05,
      "loss": 1.4971,
      "step": 2126
    },
    {
      "epoch": 3.77797513321492,
      "grad_norm": 1.2163368463516235,
      "learning_rate": 2.8152367799649858e-05,
      "loss": 1.4829,
      "step": 2127
    },
    {
      "epoch": 3.7797513321492007,
      "grad_norm": 1.0740982294082642,
      "learning_rate": 2.8074649756532455e-05,
      "loss": 1.7721,
      "step": 2128
    },
    {
      "epoch": 3.7815275310834813,
      "grad_norm": 1.196436882019043,
      "learning_rate": 2.799702161535137e-05,
      "loss": 1.4157,
      "step": 2129
    },
    {
      "epoch": 3.783303730017762,
      "grad_norm": 1.1279690265655518,
      "learning_rate": 2.7919483473136676e-05,
      "loss": 1.4149,
      "step": 2130
    },
    {
      "epoch": 3.7850799289520425,
      "grad_norm": 1.0297482013702393,
      "learning_rate": 2.7842035426805856e-05,
      "loss": 1.0398,
      "step": 2131
    },
    {
      "epoch": 3.786856127886323,
      "grad_norm": 1.1066267490386963,
      "learning_rate": 2.7764677573163955e-05,
      "loss": 1.1648,
      "step": 2132
    },
    {
      "epoch": 3.788632326820604,
      "grad_norm": 1.1701478958129883,
      "learning_rate": 2.7687410008903135e-05,
      "loss": 1.4733,
      "step": 2133
    },
    {
      "epoch": 3.7904085257548843,
      "grad_norm": 1.1136316061019897,
      "learning_rate": 2.7610232830602666e-05,
      "loss": 1.329,
      "step": 2134
    },
    {
      "epoch": 3.7921847246891653,
      "grad_norm": 1.0995712280273438,
      "learning_rate": 2.753314613472906e-05,
      "loss": 1.4002,
      "step": 2135
    },
    {
      "epoch": 3.793960923623446,
      "grad_norm": 1.0225800275802612,
      "learning_rate": 2.7456150017635497e-05,
      "loss": 1.372,
      "step": 2136
    },
    {
      "epoch": 3.7957371225577266,
      "grad_norm": 1.1612168550491333,
      "learning_rate": 2.737924457556207e-05,
      "loss": 1.5893,
      "step": 2137
    },
    {
      "epoch": 3.797513321492007,
      "grad_norm": 1.1229811906814575,
      "learning_rate": 2.730242990463552e-05,
      "loss": 1.4011,
      "step": 2138
    },
    {
      "epoch": 3.7992895204262878,
      "grad_norm": 1.082762598991394,
      "learning_rate": 2.7225706100869063e-05,
      "loss": 1.3283,
      "step": 2139
    },
    {
      "epoch": 3.8010657193605684,
      "grad_norm": 1.0474579334259033,
      "learning_rate": 2.7149073260162416e-05,
      "loss": 1.6564,
      "step": 2140
    },
    {
      "epoch": 3.802841918294849,
      "grad_norm": 1.0402724742889404,
      "learning_rate": 2.7072531478301565e-05,
      "loss": 1.3657,
      "step": 2141
    },
    {
      "epoch": 3.8046181172291296,
      "grad_norm": 1.0914216041564941,
      "learning_rate": 2.6996080850958717e-05,
      "loss": 1.3104,
      "step": 2142
    },
    {
      "epoch": 3.80639431616341,
      "grad_norm": 1.0638208389282227,
      "learning_rate": 2.691972147369203e-05,
      "loss": 1.4275,
      "step": 2143
    },
    {
      "epoch": 3.808170515097691,
      "grad_norm": 1.2112613916397095,
      "learning_rate": 2.684345344194572e-05,
      "loss": 1.5544,
      "step": 2144
    },
    {
      "epoch": 3.8099467140319714,
      "grad_norm": 1.1874898672103882,
      "learning_rate": 2.6767276851049816e-05,
      "loss": 1.3308,
      "step": 2145
    },
    {
      "epoch": 3.8117229129662524,
      "grad_norm": 1.1723966598510742,
      "learning_rate": 2.669119179621997e-05,
      "loss": 1.5169,
      "step": 2146
    },
    {
      "epoch": 3.8134991119005326,
      "grad_norm": 1.072736144065857,
      "learning_rate": 2.6615198372557494e-05,
      "loss": 1.2355,
      "step": 2147
    },
    {
      "epoch": 3.8152753108348136,
      "grad_norm": 1.040809154510498,
      "learning_rate": 2.6539296675049163e-05,
      "loss": 1.4758,
      "step": 2148
    },
    {
      "epoch": 3.8170515097690942,
      "grad_norm": 1.0975852012634277,
      "learning_rate": 2.6463486798567104e-05,
      "loss": 1.3787,
      "step": 2149
    },
    {
      "epoch": 3.818827708703375,
      "grad_norm": 1.1742340326309204,
      "learning_rate": 2.6387768837868597e-05,
      "loss": 1.422,
      "step": 2150
    },
    {
      "epoch": 3.8206039076376554,
      "grad_norm": 1.1681894063949585,
      "learning_rate": 2.6312142887596115e-05,
      "loss": 1.5006,
      "step": 2151
    },
    {
      "epoch": 3.822380106571936,
      "grad_norm": 1.0978641510009766,
      "learning_rate": 2.623660904227714e-05,
      "loss": 1.3928,
      "step": 2152
    },
    {
      "epoch": 3.8241563055062167,
      "grad_norm": 1.2107172012329102,
      "learning_rate": 2.6161167396323917e-05,
      "loss": 1.5441,
      "step": 2153
    },
    {
      "epoch": 3.8259325044404973,
      "grad_norm": 1.194037914276123,
      "learning_rate": 2.608581804403356e-05,
      "loss": 1.5719,
      "step": 2154
    },
    {
      "epoch": 3.827708703374778,
      "grad_norm": 1.1159114837646484,
      "learning_rate": 2.6010561079587813e-05,
      "loss": 1.5599,
      "step": 2155
    },
    {
      "epoch": 3.8294849023090585,
      "grad_norm": 1.0616298913955688,
      "learning_rate": 2.593539659705284e-05,
      "loss": 1.1419,
      "step": 2156
    },
    {
      "epoch": 3.8312611012433395,
      "grad_norm": 1.1153522729873657,
      "learning_rate": 2.5860324690379312e-05,
      "loss": 1.1999,
      "step": 2157
    },
    {
      "epoch": 3.8330373001776197,
      "grad_norm": 1.0111534595489502,
      "learning_rate": 2.5785345453402166e-05,
      "loss": 1.1901,
      "step": 2158
    },
    {
      "epoch": 3.8348134991119007,
      "grad_norm": 1.0614469051361084,
      "learning_rate": 2.5710458979840502e-05,
      "loss": 1.4994,
      "step": 2159
    },
    {
      "epoch": 3.8365896980461813,
      "grad_norm": 1.0261152982711792,
      "learning_rate": 2.5635665363297422e-05,
      "loss": 1.5035,
      "step": 2160
    },
    {
      "epoch": 3.838365896980462,
      "grad_norm": 1.1238117218017578,
      "learning_rate": 2.5560964697260025e-05,
      "loss": 1.4388,
      "step": 2161
    },
    {
      "epoch": 3.8401420959147425,
      "grad_norm": 1.1166445016860962,
      "learning_rate": 2.5486357075099255e-05,
      "loss": 1.4507,
      "step": 2162
    },
    {
      "epoch": 3.841918294849023,
      "grad_norm": 0.9890875816345215,
      "learning_rate": 2.54118425900696e-05,
      "loss": 1.4644,
      "step": 2163
    },
    {
      "epoch": 3.8436944937833037,
      "grad_norm": 1.056876540184021,
      "learning_rate": 2.5337421335309376e-05,
      "loss": 1.5258,
      "step": 2164
    },
    {
      "epoch": 3.8454706927175843,
      "grad_norm": 1.1506080627441406,
      "learning_rate": 2.5263093403840142e-05,
      "loss": 1.4171,
      "step": 2165
    },
    {
      "epoch": 3.847246891651865,
      "grad_norm": 1.109041452407837,
      "learning_rate": 2.5188858888566934e-05,
      "loss": 1.3021,
      "step": 2166
    },
    {
      "epoch": 3.8490230905861456,
      "grad_norm": 1.0794135332107544,
      "learning_rate": 2.5114717882278027e-05,
      "loss": 1.3937,
      "step": 2167
    },
    {
      "epoch": 3.850799289520426,
      "grad_norm": 1.1206148862838745,
      "learning_rate": 2.50406704776447e-05,
      "loss": 1.5706,
      "step": 2168
    },
    {
      "epoch": 3.8525754884547068,
      "grad_norm": 1.149990200996399,
      "learning_rate": 2.4966716767221376e-05,
      "loss": 1.3664,
      "step": 2169
    },
    {
      "epoch": 3.854351687388988,
      "grad_norm": 1.1082288026809692,
      "learning_rate": 2.4892856843445322e-05,
      "loss": 1.4376,
      "step": 2170
    },
    {
      "epoch": 3.856127886323268,
      "grad_norm": 1.0810023546218872,
      "learning_rate": 2.48190907986365e-05,
      "loss": 1.5086,
      "step": 2171
    },
    {
      "epoch": 3.857904085257549,
      "grad_norm": 1.0953458547592163,
      "learning_rate": 2.474541872499766e-05,
      "loss": 1.4304,
      "step": 2172
    },
    {
      "epoch": 3.8596802841918296,
      "grad_norm": 1.1141788959503174,
      "learning_rate": 2.4671840714613938e-05,
      "loss": 1.3391,
      "step": 2173
    },
    {
      "epoch": 3.8614564831261102,
      "grad_norm": 1.0355675220489502,
      "learning_rate": 2.4598356859453108e-05,
      "loss": 1.5946,
      "step": 2174
    },
    {
      "epoch": 3.863232682060391,
      "grad_norm": 0.9984564781188965,
      "learning_rate": 2.4524967251365026e-05,
      "loss": 1.301,
      "step": 2175
    },
    {
      "epoch": 3.8650088809946714,
      "grad_norm": 1.1386502981185913,
      "learning_rate": 2.4451671982081914e-05,
      "loss": 1.5887,
      "step": 2176
    },
    {
      "epoch": 3.866785079928952,
      "grad_norm": 1.0432261228561401,
      "learning_rate": 2.437847114321802e-05,
      "loss": 1.4849,
      "step": 2177
    },
    {
      "epoch": 3.8685612788632326,
      "grad_norm": 1.09169602394104,
      "learning_rate": 2.4305364826269518e-05,
      "loss": 1.3042,
      "step": 2178
    },
    {
      "epoch": 3.8703374777975132,
      "grad_norm": 1.1501144170761108,
      "learning_rate": 2.423235312261448e-05,
      "loss": 1.4239,
      "step": 2179
    },
    {
      "epoch": 3.872113676731794,
      "grad_norm": 1.1780692338943481,
      "learning_rate": 2.4159436123512735e-05,
      "loss": 1.5351,
      "step": 2180
    },
    {
      "epoch": 3.8738898756660745,
      "grad_norm": 1.1292458772659302,
      "learning_rate": 2.4086613920105737e-05,
      "loss": 1.5002,
      "step": 2181
    },
    {
      "epoch": 3.875666074600355,
      "grad_norm": 1.0857371091842651,
      "learning_rate": 2.401388660341637e-05,
      "loss": 1.2092,
      "step": 2182
    },
    {
      "epoch": 3.877442273534636,
      "grad_norm": 1.1860380172729492,
      "learning_rate": 2.3941254264349e-05,
      "loss": 1.4354,
      "step": 2183
    },
    {
      "epoch": 3.8792184724689163,
      "grad_norm": 1.1462552547454834,
      "learning_rate": 2.3868716993689276e-05,
      "loss": 1.5989,
      "step": 2184
    },
    {
      "epoch": 3.8809946714031973,
      "grad_norm": 1.0759389400482178,
      "learning_rate": 2.3796274882103964e-05,
      "loss": 1.4802,
      "step": 2185
    },
    {
      "epoch": 3.882770870337478,
      "grad_norm": 1.055137276649475,
      "learning_rate": 2.3723928020140927e-05,
      "loss": 1.4706,
      "step": 2186
    },
    {
      "epoch": 3.8845470692717585,
      "grad_norm": 1.0445693731307983,
      "learning_rate": 2.365167649822899e-05,
      "loss": 1.4415,
      "step": 2187
    },
    {
      "epoch": 3.886323268206039,
      "grad_norm": 1.138414978981018,
      "learning_rate": 2.3579520406677724e-05,
      "loss": 1.5712,
      "step": 2188
    },
    {
      "epoch": 3.8880994671403197,
      "grad_norm": 1.141145944595337,
      "learning_rate": 2.3507459835677514e-05,
      "loss": 1.4591,
      "step": 2189
    },
    {
      "epoch": 3.8898756660746003,
      "grad_norm": 1.1236213445663452,
      "learning_rate": 2.3435494875299314e-05,
      "loss": 1.5651,
      "step": 2190
    },
    {
      "epoch": 3.891651865008881,
      "grad_norm": 1.1254181861877441,
      "learning_rate": 2.336362561549459e-05,
      "loss": 1.6075,
      "step": 2191
    },
    {
      "epoch": 3.8934280639431615,
      "grad_norm": 1.072584629058838,
      "learning_rate": 2.3291852146095116e-05,
      "loss": 1.1975,
      "step": 2192
    },
    {
      "epoch": 3.895204262877442,
      "grad_norm": 1.087512493133545,
      "learning_rate": 2.3220174556813023e-05,
      "loss": 1.395,
      "step": 2193
    },
    {
      "epoch": 3.8969804618117228,
      "grad_norm": 1.0838813781738281,
      "learning_rate": 2.3148592937240586e-05,
      "loss": 1.5387,
      "step": 2194
    },
    {
      "epoch": 3.8987566607460034,
      "grad_norm": 1.1968729496002197,
      "learning_rate": 2.3077107376850005e-05,
      "loss": 1.377,
      "step": 2195
    },
    {
      "epoch": 3.9005328596802844,
      "grad_norm": 1.1019456386566162,
      "learning_rate": 2.3005717964993634e-05,
      "loss": 1.5026,
      "step": 2196
    },
    {
      "epoch": 3.9023090586145646,
      "grad_norm": 1.0984406471252441,
      "learning_rate": 2.293442479090343e-05,
      "loss": 1.5857,
      "step": 2197
    },
    {
      "epoch": 3.9040852575488456,
      "grad_norm": 1.0958980321884155,
      "learning_rate": 2.286322794369119e-05,
      "loss": 1.3757,
      "step": 2198
    },
    {
      "epoch": 3.905861456483126,
      "grad_norm": 1.137632131576538,
      "learning_rate": 2.2792127512348284e-05,
      "loss": 1.313,
      "step": 2199
    },
    {
      "epoch": 3.907637655417407,
      "grad_norm": 1.0586209297180176,
      "learning_rate": 2.2721123585745507e-05,
      "loss": 1.4392,
      "step": 2200
    },
    {
      "epoch": 3.907637655417407,
      "eval_loss": 2.054105520248413,
      "eval_runtime": 17.6023,
      "eval_samples_per_second": 56.868,
      "eval_steps_per_second": 28.462,
      "step": 2200
    },
    {
      "epoch": 3.9094138543516874,
      "grad_norm": 1.1043940782546997,
      "learning_rate": 2.265021625263313e-05,
      "loss": 1.8364,
      "step": 2201
    },
    {
      "epoch": 3.911190053285968,
      "grad_norm": 1.0811336040496826,
      "learning_rate": 2.2579405601640546e-05,
      "loss": 1.3751,
      "step": 2202
    },
    {
      "epoch": 3.9129662522202486,
      "grad_norm": 1.1057358980178833,
      "learning_rate": 2.250869172127651e-05,
      "loss": 1.4168,
      "step": 2203
    },
    {
      "epoch": 3.9147424511545292,
      "grad_norm": 1.188893437385559,
      "learning_rate": 2.2438074699928634e-05,
      "loss": 1.5238,
      "step": 2204
    },
    {
      "epoch": 3.91651865008881,
      "grad_norm": 1.0822021961212158,
      "learning_rate": 2.2367554625863497e-05,
      "loss": 1.1603,
      "step": 2205
    },
    {
      "epoch": 3.9182948490230904,
      "grad_norm": 1.1375683546066284,
      "learning_rate": 2.2297131587226626e-05,
      "loss": 1.3212,
      "step": 2206
    },
    {
      "epoch": 3.9200710479573715,
      "grad_norm": 1.3000050783157349,
      "learning_rate": 2.2226805672042082e-05,
      "loss": 1.3555,
      "step": 2207
    },
    {
      "epoch": 3.9218472468916517,
      "grad_norm": 1.1603838205337524,
      "learning_rate": 2.215657696821265e-05,
      "loss": 1.5518,
      "step": 2208
    },
    {
      "epoch": 3.9236234458259327,
      "grad_norm": 1.2164169549942017,
      "learning_rate": 2.2086445563519597e-05,
      "loss": 1.6041,
      "step": 2209
    },
    {
      "epoch": 3.9253996447602133,
      "grad_norm": 1.056269645690918,
      "learning_rate": 2.2016411545622495e-05,
      "loss": 1.3941,
      "step": 2210
    },
    {
      "epoch": 3.927175843694494,
      "grad_norm": 1.0847506523132324,
      "learning_rate": 2.194647500205925e-05,
      "loss": 1.2306,
      "step": 2211
    },
    {
      "epoch": 3.9289520426287745,
      "grad_norm": 1.106711983680725,
      "learning_rate": 2.1876636020245932e-05,
      "loss": 1.3772,
      "step": 2212
    },
    {
      "epoch": 3.930728241563055,
      "grad_norm": 1.0606565475463867,
      "learning_rate": 2.1806894687476687e-05,
      "loss": 1.358,
      "step": 2213
    },
    {
      "epoch": 3.9325044404973357,
      "grad_norm": 1.1418347358703613,
      "learning_rate": 2.1737251090923517e-05,
      "loss": 1.3269,
      "step": 2214
    },
    {
      "epoch": 3.9342806394316163,
      "grad_norm": 1.1850805282592773,
      "learning_rate": 2.166770531763633e-05,
      "loss": 1.6014,
      "step": 2215
    },
    {
      "epoch": 3.936056838365897,
      "grad_norm": 1.3456398248672485,
      "learning_rate": 2.159825745454279e-05,
      "loss": 1.4955,
      "step": 2216
    },
    {
      "epoch": 3.9378330373001775,
      "grad_norm": 1.1165255308151245,
      "learning_rate": 2.152890758844809e-05,
      "loss": 1.3343,
      "step": 2217
    },
    {
      "epoch": 3.939609236234458,
      "grad_norm": 1.069510579109192,
      "learning_rate": 2.1459655806034995e-05,
      "loss": 1.4553,
      "step": 2218
    },
    {
      "epoch": 3.9413854351687387,
      "grad_norm": 1.0902501344680786,
      "learning_rate": 2.139050219386366e-05,
      "loss": 1.455,
      "step": 2219
    },
    {
      "epoch": 3.94316163410302,
      "grad_norm": 1.09302818775177,
      "learning_rate": 2.132144683837155e-05,
      "loss": 1.3737,
      "step": 2220
    },
    {
      "epoch": 3.9449378330373,
      "grad_norm": 1.1253125667572021,
      "learning_rate": 2.1252489825873268e-05,
      "loss": 1.4048,
      "step": 2221
    },
    {
      "epoch": 3.946714031971581,
      "grad_norm": 1.1935315132141113,
      "learning_rate": 2.1183631242560542e-05,
      "loss": 1.5762,
      "step": 2222
    },
    {
      "epoch": 3.9484902309058616,
      "grad_norm": 1.1471505165100098,
      "learning_rate": 2.111487117450206e-05,
      "loss": 1.4121,
      "step": 2223
    },
    {
      "epoch": 3.950266429840142,
      "grad_norm": 1.1033707857131958,
      "learning_rate": 2.104620970764334e-05,
      "loss": 1.2926,
      "step": 2224
    },
    {
      "epoch": 3.952042628774423,
      "grad_norm": 1.1141239404678345,
      "learning_rate": 2.0977646927806683e-05,
      "loss": 1.4841,
      "step": 2225
    },
    {
      "epoch": 3.9538188277087034,
      "grad_norm": 1.1573832035064697,
      "learning_rate": 2.090918292069107e-05,
      "loss": 1.4579,
      "step": 2226
    },
    {
      "epoch": 3.955595026642984,
      "grad_norm": 1.0651423931121826,
      "learning_rate": 2.084081777187191e-05,
      "loss": 1.4185,
      "step": 2227
    },
    {
      "epoch": 3.9573712255772646,
      "grad_norm": 1.1385269165039062,
      "learning_rate": 2.0772551566801235e-05,
      "loss": 1.5115,
      "step": 2228
    },
    {
      "epoch": 3.959147424511545,
      "grad_norm": 1.1622244119644165,
      "learning_rate": 2.0704384390807196e-05,
      "loss": 1.4332,
      "step": 2229
    },
    {
      "epoch": 3.960923623445826,
      "grad_norm": 1.2460626363754272,
      "learning_rate": 2.0636316329094317e-05,
      "loss": 1.2847,
      "step": 2230
    },
    {
      "epoch": 3.9626998223801064,
      "grad_norm": 1.128118872642517,
      "learning_rate": 2.0568347466743133e-05,
      "loss": 1.7558,
      "step": 2231
    },
    {
      "epoch": 3.964476021314387,
      "grad_norm": 1.0907052755355835,
      "learning_rate": 2.0500477888710257e-05,
      "loss": 1.2766,
      "step": 2232
    },
    {
      "epoch": 3.966252220248668,
      "grad_norm": 1.1581077575683594,
      "learning_rate": 2.04327076798282e-05,
      "loss": 1.4975,
      "step": 2233
    },
    {
      "epoch": 3.9680284191829482,
      "grad_norm": 1.1880139112472534,
      "learning_rate": 2.036503692480516e-05,
      "loss": 1.6113,
      "step": 2234
    },
    {
      "epoch": 3.9698046181172293,
      "grad_norm": 1.1321567296981812,
      "learning_rate": 2.0297465708225238e-05,
      "loss": 1.5023,
      "step": 2235
    },
    {
      "epoch": 3.97158081705151,
      "grad_norm": 1.0073455572128296,
      "learning_rate": 2.022999411454789e-05,
      "loss": 1.2367,
      "step": 2236
    },
    {
      "epoch": 3.9733570159857905,
      "grad_norm": 1.1044034957885742,
      "learning_rate": 2.0162622228108184e-05,
      "loss": 1.4637,
      "step": 2237
    },
    {
      "epoch": 3.975133214920071,
      "grad_norm": 1.174420714378357,
      "learning_rate": 2.0095350133116565e-05,
      "loss": 1.4549,
      "step": 2238
    },
    {
      "epoch": 3.9769094138543517,
      "grad_norm": 1.0647978782653809,
      "learning_rate": 2.002817791365863e-05,
      "loss": 1.2781,
      "step": 2239
    },
    {
      "epoch": 3.9786856127886323,
      "grad_norm": 1.1182870864868164,
      "learning_rate": 1.9961105653695266e-05,
      "loss": 1.4863,
      "step": 2240
    },
    {
      "epoch": 3.980461811722913,
      "grad_norm": 1.1115847826004028,
      "learning_rate": 1.989413343706239e-05,
      "loss": 1.6344,
      "step": 2241
    },
    {
      "epoch": 3.9822380106571935,
      "grad_norm": 1.091386079788208,
      "learning_rate": 1.9827261347470783e-05,
      "loss": 1.4255,
      "step": 2242
    },
    {
      "epoch": 3.984014209591474,
      "grad_norm": 1.1465938091278076,
      "learning_rate": 1.9760489468506203e-05,
      "loss": 1.448,
      "step": 2243
    },
    {
      "epoch": 3.9857904085257547,
      "grad_norm": 1.1282317638397217,
      "learning_rate": 1.9693817883628996e-05,
      "loss": 1.4958,
      "step": 2244
    },
    {
      "epoch": 3.9875666074600353,
      "grad_norm": 1.101876974105835,
      "learning_rate": 1.962724667617436e-05,
      "loss": 1.2141,
      "step": 2245
    },
    {
      "epoch": 3.9893428063943164,
      "grad_norm": 1.2211717367172241,
      "learning_rate": 1.9560775929351826e-05,
      "loss": 1.506,
      "step": 2246
    },
    {
      "epoch": 3.9911190053285965,
      "grad_norm": 1.1737513542175293,
      "learning_rate": 1.9494405726245457e-05,
      "loss": 1.5756,
      "step": 2247
    },
    {
      "epoch": 3.9928952042628776,
      "grad_norm": 1.159709095954895,
      "learning_rate": 1.9428136149813636e-05,
      "loss": 1.1402,
      "step": 2248
    },
    {
      "epoch": 3.994671403197158,
      "grad_norm": 1.1388800144195557,
      "learning_rate": 1.9361967282888928e-05,
      "loss": 1.5758,
      "step": 2249
    },
    {
      "epoch": 3.996447602131439,
      "grad_norm": 1.0182596445083618,
      "learning_rate": 1.929589920817806e-05,
      "loss": 1.3135,
      "step": 2250
    },
    {
      "epoch": 3.9982238010657194,
      "grad_norm": 1.0706967115402222,
      "learning_rate": 1.9229932008261754e-05,
      "loss": 1.7308,
      "step": 2251
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.2946935892105103,
      "learning_rate": 1.9164065765594696e-05,
      "loss": 1.4264,
      "step": 2252
    },
    {
      "epoch": 4.001776198934281,
      "grad_norm": 1.0799843072891235,
      "learning_rate": 1.9098300562505266e-05,
      "loss": 1.4022,
      "step": 2253
    },
    {
      "epoch": 4.003552397868561,
      "grad_norm": 0.9380645751953125,
      "learning_rate": 1.9032636481195653e-05,
      "loss": 1.3819,
      "step": 2254
    },
    {
      "epoch": 4.005328596802842,
      "grad_norm": 1.0650078058242798,
      "learning_rate": 1.896707360374167e-05,
      "loss": 1.3095,
      "step": 2255
    },
    {
      "epoch": 4.007104795737122,
      "grad_norm": 0.999330461025238,
      "learning_rate": 1.8901612012092474e-05,
      "loss": 1.3685,
      "step": 2256
    },
    {
      "epoch": 4.0088809946714035,
      "grad_norm": 1.077622890472412,
      "learning_rate": 1.8836251788070845e-05,
      "loss": 1.4509,
      "step": 2257
    },
    {
      "epoch": 4.010657193605684,
      "grad_norm": 1.061011791229248,
      "learning_rate": 1.8770993013372697e-05,
      "loss": 1.5122,
      "step": 2258
    },
    {
      "epoch": 4.012433392539965,
      "grad_norm": 1.0450257062911987,
      "learning_rate": 1.8705835769567158e-05,
      "loss": 1.0873,
      "step": 2259
    },
    {
      "epoch": 4.014209591474245,
      "grad_norm": 1.0889990329742432,
      "learning_rate": 1.8640780138096513e-05,
      "loss": 1.2959,
      "step": 2260
    },
    {
      "epoch": 4.015985790408526,
      "grad_norm": 1.0577598810195923,
      "learning_rate": 1.857582620027599e-05,
      "loss": 1.2272,
      "step": 2261
    },
    {
      "epoch": 4.017761989342806,
      "grad_norm": 1.1142209768295288,
      "learning_rate": 1.851097403729376e-05,
      "loss": 1.1226,
      "step": 2262
    },
    {
      "epoch": 4.019538188277087,
      "grad_norm": 1.1506484746932983,
      "learning_rate": 1.8446223730210677e-05,
      "loss": 1.3081,
      "step": 2263
    },
    {
      "epoch": 4.021314387211367,
      "grad_norm": 1.1269322633743286,
      "learning_rate": 1.8381575359960378e-05,
      "loss": 1.2359,
      "step": 2264
    },
    {
      "epoch": 4.023090586145648,
      "grad_norm": 1.0068143606185913,
      "learning_rate": 1.8317029007349085e-05,
      "loss": 1.4094,
      "step": 2265
    },
    {
      "epoch": 4.024866785079929,
      "grad_norm": 1.00341796875,
      "learning_rate": 1.8252584753055392e-05,
      "loss": 1.0877,
      "step": 2266
    },
    {
      "epoch": 4.0266429840142095,
      "grad_norm": 1.0742933750152588,
      "learning_rate": 1.8188242677630472e-05,
      "loss": 1.5498,
      "step": 2267
    },
    {
      "epoch": 4.0284191829484906,
      "grad_norm": 1.0169180631637573,
      "learning_rate": 1.812400286149758e-05,
      "loss": 1.3174,
      "step": 2268
    },
    {
      "epoch": 4.030195381882771,
      "grad_norm": 1.133965253829956,
      "learning_rate": 1.805986538495228e-05,
      "loss": 1.4709,
      "step": 2269
    },
    {
      "epoch": 4.031971580817052,
      "grad_norm": 1.0438361167907715,
      "learning_rate": 1.799583032816219e-05,
      "loss": 1.2688,
      "step": 2270
    },
    {
      "epoch": 4.033747779751332,
      "grad_norm": 1.1849417686462402,
      "learning_rate": 1.793189777116686e-05,
      "loss": 1.2628,
      "step": 2271
    },
    {
      "epoch": 4.035523978685613,
      "grad_norm": 1.0754811763763428,
      "learning_rate": 1.7868067793877817e-05,
      "loss": 1.5918,
      "step": 2272
    },
    {
      "epoch": 4.037300177619893,
      "grad_norm": 1.2026489973068237,
      "learning_rate": 1.780434047607823e-05,
      "loss": 1.1989,
      "step": 2273
    },
    {
      "epoch": 4.039076376554174,
      "grad_norm": 1.1177257299423218,
      "learning_rate": 1.7740715897423143e-05,
      "loss": 1.3274,
      "step": 2274
    },
    {
      "epoch": 4.040852575488454,
      "grad_norm": 1.1411534547805786,
      "learning_rate": 1.7677194137439035e-05,
      "loss": 1.2488,
      "step": 2275
    },
    {
      "epoch": 4.042628774422735,
      "grad_norm": 1.1375263929367065,
      "learning_rate": 1.761377527552385e-05,
      "loss": 1.2029,
      "step": 2276
    },
    {
      "epoch": 4.044404973357016,
      "grad_norm": 1.1308329105377197,
      "learning_rate": 1.7550459390947073e-05,
      "loss": 1.2599,
      "step": 2277
    },
    {
      "epoch": 4.046181172291297,
      "grad_norm": 1.077714204788208,
      "learning_rate": 1.74872465628493e-05,
      "loss": 1.458,
      "step": 2278
    },
    {
      "epoch": 4.047957371225578,
      "grad_norm": 1.1388450860977173,
      "learning_rate": 1.7424136870242426e-05,
      "loss": 1.3956,
      "step": 2279
    },
    {
      "epoch": 4.049733570159858,
      "grad_norm": 1.265413522720337,
      "learning_rate": 1.7361130392009407e-05,
      "loss": 1.4322,
      "step": 2280
    },
    {
      "epoch": 4.051509769094139,
      "grad_norm": 1.1985814571380615,
      "learning_rate": 1.729822720690415e-05,
      "loss": 1.2058,
      "step": 2281
    },
    {
      "epoch": 4.053285968028419,
      "grad_norm": 1.233547568321228,
      "learning_rate": 1.7235427393551495e-05,
      "loss": 1.4354,
      "step": 2282
    },
    {
      "epoch": 4.0550621669627,
      "grad_norm": 1.1932215690612793,
      "learning_rate": 1.7172731030447054e-05,
      "loss": 1.3327,
      "step": 2283
    },
    {
      "epoch": 4.05683836589698,
      "grad_norm": 1.1466909646987915,
      "learning_rate": 1.711013819595717e-05,
      "loss": 1.2434,
      "step": 2284
    },
    {
      "epoch": 4.058614564831261,
      "grad_norm": 1.1713287830352783,
      "learning_rate": 1.7047648968318698e-05,
      "loss": 1.2837,
      "step": 2285
    },
    {
      "epoch": 4.060390763765541,
      "grad_norm": 1.2499364614486694,
      "learning_rate": 1.6985263425639074e-05,
      "loss": 1.2842,
      "step": 2286
    },
    {
      "epoch": 4.0621669626998225,
      "grad_norm": 1.228323221206665,
      "learning_rate": 1.692298164589614e-05,
      "loss": 1.4961,
      "step": 2287
    },
    {
      "epoch": 4.063943161634103,
      "grad_norm": 1.224833369255066,
      "learning_rate": 1.6860803706937934e-05,
      "loss": 1.4665,
      "step": 2288
    },
    {
      "epoch": 4.065719360568384,
      "grad_norm": 1.2640682458877563,
      "learning_rate": 1.6798729686482807e-05,
      "loss": 1.4707,
      "step": 2289
    },
    {
      "epoch": 4.067495559502665,
      "grad_norm": 1.1763793230056763,
      "learning_rate": 1.6736759662119183e-05,
      "loss": 1.361,
      "step": 2290
    },
    {
      "epoch": 4.069271758436945,
      "grad_norm": 1.342136025428772,
      "learning_rate": 1.667489371130553e-05,
      "loss": 1.2881,
      "step": 2291
    },
    {
      "epoch": 4.071047957371226,
      "grad_norm": 1.1958547830581665,
      "learning_rate": 1.661313191137015e-05,
      "loss": 1.2206,
      "step": 2292
    },
    {
      "epoch": 4.072824156305506,
      "grad_norm": 1.2155494689941406,
      "learning_rate": 1.655147433951122e-05,
      "loss": 1.4885,
      "step": 2293
    },
    {
      "epoch": 4.074600355239787,
      "grad_norm": 1.200904369354248,
      "learning_rate": 1.6489921072796678e-05,
      "loss": 1.0492,
      "step": 2294
    },
    {
      "epoch": 4.076376554174067,
      "grad_norm": 1.263750433921814,
      "learning_rate": 1.642847218816398e-05,
      "loss": 1.1647,
      "step": 2295
    },
    {
      "epoch": 4.078152753108348,
      "grad_norm": 1.2002410888671875,
      "learning_rate": 1.63671277624202e-05,
      "loss": 1.2948,
      "step": 2296
    },
    {
      "epoch": 4.0799289520426285,
      "grad_norm": 1.1918351650238037,
      "learning_rate": 1.6305887872241844e-05,
      "loss": 1.0807,
      "step": 2297
    },
    {
      "epoch": 4.08170515097691,
      "grad_norm": 1.2017722129821777,
      "learning_rate": 1.6244752594174652e-05,
      "loss": 1.3509,
      "step": 2298
    },
    {
      "epoch": 4.08348134991119,
      "grad_norm": 1.244428277015686,
      "learning_rate": 1.618372200463377e-05,
      "loss": 1.4579,
      "step": 2299
    },
    {
      "epoch": 4.085257548845471,
      "grad_norm": 1.2432879209518433,
      "learning_rate": 1.6122796179903354e-05,
      "loss": 1.4671,
      "step": 2300
    },
    {
      "epoch": 4.085257548845471,
      "eval_loss": 2.126424789428711,
      "eval_runtime": 17.6099,
      "eval_samples_per_second": 56.843,
      "eval_steps_per_second": 28.45,
      "step": 2300
    },
    {
      "epoch": 4.087033747779751,
      "grad_norm": 1.2446587085723877,
      "learning_rate": 1.6061975196136704e-05,
      "loss": 1.2505,
      "step": 2301
    },
    {
      "epoch": 4.088809946714032,
      "grad_norm": 1.1808855533599854,
      "learning_rate": 1.600125912935596e-05,
      "loss": 1.1965,
      "step": 2302
    },
    {
      "epoch": 4.090586145648313,
      "grad_norm": 1.203795075416565,
      "learning_rate": 1.5940648055452244e-05,
      "loss": 1.3041,
      "step": 2303
    },
    {
      "epoch": 4.092362344582593,
      "grad_norm": 1.2898679971694946,
      "learning_rate": 1.588014205018542e-05,
      "loss": 1.5213,
      "step": 2304
    },
    {
      "epoch": 4.094138543516874,
      "grad_norm": 1.105419635772705,
      "learning_rate": 1.58197411891839e-05,
      "loss": 1.242,
      "step": 2305
    },
    {
      "epoch": 4.095914742451154,
      "grad_norm": 1.2777671813964844,
      "learning_rate": 1.5759445547944885e-05,
      "loss": 1.3281,
      "step": 2306
    },
    {
      "epoch": 4.097690941385435,
      "grad_norm": 1.1864583492279053,
      "learning_rate": 1.569925520183386e-05,
      "loss": 1.4846,
      "step": 2307
    },
    {
      "epoch": 4.099467140319716,
      "grad_norm": 1.107593059539795,
      "learning_rate": 1.5639170226084822e-05,
      "loss": 1.4425,
      "step": 2308
    },
    {
      "epoch": 4.101243339253997,
      "grad_norm": 1.2016830444335938,
      "learning_rate": 1.557919069580004e-05,
      "loss": 1.1367,
      "step": 2309
    },
    {
      "epoch": 4.103019538188277,
      "grad_norm": 1.2605921030044556,
      "learning_rate": 1.5519316685949903e-05,
      "loss": 1.2162,
      "step": 2310
    },
    {
      "epoch": 4.104795737122558,
      "grad_norm": 1.2691400051116943,
      "learning_rate": 1.5459548271373003e-05,
      "loss": 1.2385,
      "step": 2311
    },
    {
      "epoch": 4.106571936056838,
      "grad_norm": 1.2242900133132935,
      "learning_rate": 1.5399885526775936e-05,
      "loss": 1.1235,
      "step": 2312
    },
    {
      "epoch": 4.108348134991119,
      "grad_norm": 1.2189511060714722,
      "learning_rate": 1.534032852673314e-05,
      "loss": 1.4393,
      "step": 2313
    },
    {
      "epoch": 4.110124333925399,
      "grad_norm": 1.27950119972229,
      "learning_rate": 1.528087734568695e-05,
      "loss": 1.2828,
      "step": 2314
    },
    {
      "epoch": 4.11190053285968,
      "grad_norm": 1.1812071800231934,
      "learning_rate": 1.5221532057947419e-05,
      "loss": 1.3024,
      "step": 2315
    },
    {
      "epoch": 4.113676731793961,
      "grad_norm": 1.2868247032165527,
      "learning_rate": 1.5162292737692263e-05,
      "loss": 1.3533,
      "step": 2316
    },
    {
      "epoch": 4.1154529307282415,
      "grad_norm": 1.2705073356628418,
      "learning_rate": 1.5103159458966675e-05,
      "loss": 1.3561,
      "step": 2317
    },
    {
      "epoch": 4.1172291296625225,
      "grad_norm": 1.214216947555542,
      "learning_rate": 1.5044132295683355e-05,
      "loss": 1.5674,
      "step": 2318
    },
    {
      "epoch": 4.119005328596803,
      "grad_norm": 1.199955940246582,
      "learning_rate": 1.4985211321622395e-05,
      "loss": 1.3222,
      "step": 2319
    },
    {
      "epoch": 4.120781527531084,
      "grad_norm": 1.2604339122772217,
      "learning_rate": 1.4926396610431059e-05,
      "loss": 1.223,
      "step": 2320
    },
    {
      "epoch": 4.122557726465364,
      "grad_norm": 1.3589098453521729,
      "learning_rate": 1.48676882356239e-05,
      "loss": 1.3212,
      "step": 2321
    },
    {
      "epoch": 4.124333925399645,
      "grad_norm": 1.136451244354248,
      "learning_rate": 1.4809086270582485e-05,
      "loss": 1.2455,
      "step": 2322
    },
    {
      "epoch": 4.126110124333925,
      "grad_norm": 1.212847352027893,
      "learning_rate": 1.4750590788555419e-05,
      "loss": 1.2755,
      "step": 2323
    },
    {
      "epoch": 4.127886323268206,
      "grad_norm": 1.2605713605880737,
      "learning_rate": 1.4692201862658161e-05,
      "loss": 1.3897,
      "step": 2324
    },
    {
      "epoch": 4.129662522202486,
      "grad_norm": 1.1650735139846802,
      "learning_rate": 1.4633919565873033e-05,
      "loss": 1.4541,
      "step": 2325
    },
    {
      "epoch": 4.131438721136767,
      "grad_norm": 1.1966606378555298,
      "learning_rate": 1.4575743971049072e-05,
      "loss": 1.3819,
      "step": 2326
    },
    {
      "epoch": 4.1332149200710475,
      "grad_norm": 1.2183171510696411,
      "learning_rate": 1.4517675150901855e-05,
      "loss": 1.1427,
      "step": 2327
    },
    {
      "epoch": 4.134991119005329,
      "grad_norm": 1.2518925666809082,
      "learning_rate": 1.4459713178013668e-05,
      "loss": 1.2545,
      "step": 2328
    },
    {
      "epoch": 4.13676731793961,
      "grad_norm": 1.19009530544281,
      "learning_rate": 1.4401858124833112e-05,
      "loss": 1.3077,
      "step": 2329
    },
    {
      "epoch": 4.13854351687389,
      "grad_norm": 1.1844996213912964,
      "learning_rate": 1.4344110063675142e-05,
      "loss": 1.4482,
      "step": 2330
    },
    {
      "epoch": 4.140319715808171,
      "grad_norm": 1.2405047416687012,
      "learning_rate": 1.4286469066721053e-05,
      "loss": 1.3543,
      "step": 2331
    },
    {
      "epoch": 4.142095914742451,
      "grad_norm": 1.0961858034133911,
      "learning_rate": 1.4228935206018268e-05,
      "loss": 1.3022,
      "step": 2332
    },
    {
      "epoch": 4.143872113676732,
      "grad_norm": 1.1821062564849854,
      "learning_rate": 1.4171508553480362e-05,
      "loss": 1.4661,
      "step": 2333
    },
    {
      "epoch": 4.145648312611012,
      "grad_norm": 1.1669563055038452,
      "learning_rate": 1.4114189180886794e-05,
      "loss": 1.1923,
      "step": 2334
    },
    {
      "epoch": 4.147424511545293,
      "grad_norm": 1.1996625661849976,
      "learning_rate": 1.4056977159883012e-05,
      "loss": 1.0575,
      "step": 2335
    },
    {
      "epoch": 4.149200710479573,
      "grad_norm": 1.2895885705947876,
      "learning_rate": 1.3999872561980299e-05,
      "loss": 1.3143,
      "step": 2336
    },
    {
      "epoch": 4.150976909413854,
      "grad_norm": 1.29115891456604,
      "learning_rate": 1.3942875458555548e-05,
      "loss": 1.3849,
      "step": 2337
    },
    {
      "epoch": 4.152753108348135,
      "grad_norm": 1.1769945621490479,
      "learning_rate": 1.3885985920851463e-05,
      "loss": 1.1368,
      "step": 2338
    },
    {
      "epoch": 4.154529307282416,
      "grad_norm": 1.2433671951293945,
      "learning_rate": 1.3829204019976161e-05,
      "loss": 1.079,
      "step": 2339
    },
    {
      "epoch": 4.156305506216697,
      "grad_norm": 1.167161226272583,
      "learning_rate": 1.3772529826903269e-05,
      "loss": 1.5239,
      "step": 2340
    },
    {
      "epoch": 4.158081705150977,
      "grad_norm": 1.0895473957061768,
      "learning_rate": 1.3715963412471822e-05,
      "loss": 1.337,
      "step": 2341
    },
    {
      "epoch": 4.159857904085258,
      "grad_norm": 1.185894250869751,
      "learning_rate": 1.365950484738604e-05,
      "loss": 1.3412,
      "step": 2342
    },
    {
      "epoch": 4.161634103019538,
      "grad_norm": 1.1209169626235962,
      "learning_rate": 1.360315420221543e-05,
      "loss": 1.2651,
      "step": 2343
    },
    {
      "epoch": 4.163410301953819,
      "grad_norm": 1.165810227394104,
      "learning_rate": 1.354691154739458e-05,
      "loss": 1.3632,
      "step": 2344
    },
    {
      "epoch": 4.165186500888099,
      "grad_norm": 1.2012913227081299,
      "learning_rate": 1.3490776953223105e-05,
      "loss": 1.2182,
      "step": 2345
    },
    {
      "epoch": 4.16696269982238,
      "grad_norm": 1.1513330936431885,
      "learning_rate": 1.3434750489865534e-05,
      "loss": 1.4158,
      "step": 2346
    },
    {
      "epoch": 4.1687388987566605,
      "grad_norm": 1.1800651550292969,
      "learning_rate": 1.3378832227351179e-05,
      "loss": 1.4644,
      "step": 2347
    },
    {
      "epoch": 4.1705150976909415,
      "grad_norm": 1.2061620950698853,
      "learning_rate": 1.3323022235574279e-05,
      "loss": 1.3601,
      "step": 2348
    },
    {
      "epoch": 4.172291296625222,
      "grad_norm": 1.1822172403335571,
      "learning_rate": 1.3267320584293563e-05,
      "loss": 1.5222,
      "step": 2349
    },
    {
      "epoch": 4.174067495559503,
      "grad_norm": 1.2202458381652832,
      "learning_rate": 1.321172734313244e-05,
      "loss": 1.3413,
      "step": 2350
    },
    {
      "epoch": 4.175843694493783,
      "grad_norm": 1.2968651056289673,
      "learning_rate": 1.3156242581578815e-05,
      "loss": 1.4591,
      "step": 2351
    },
    {
      "epoch": 4.177619893428064,
      "grad_norm": 1.153120756149292,
      "learning_rate": 1.3100866368984922e-05,
      "loss": 1.5872,
      "step": 2352
    },
    {
      "epoch": 4.179396092362345,
      "grad_norm": 1.220273733139038,
      "learning_rate": 1.3045598774567392e-05,
      "loss": 1.2297,
      "step": 2353
    },
    {
      "epoch": 4.181172291296625,
      "grad_norm": 1.1660001277923584,
      "learning_rate": 1.2990439867407078e-05,
      "loss": 1.3852,
      "step": 2354
    },
    {
      "epoch": 4.182948490230906,
      "grad_norm": 1.3270611763000488,
      "learning_rate": 1.2935389716448976e-05,
      "loss": 1.1794,
      "step": 2355
    },
    {
      "epoch": 4.184724689165186,
      "grad_norm": 1.2920899391174316,
      "learning_rate": 1.2880448390502108e-05,
      "loss": 1.3644,
      "step": 2356
    },
    {
      "epoch": 4.186500888099467,
      "grad_norm": 1.2450613975524902,
      "learning_rate": 1.2825615958239512e-05,
      "loss": 1.2366,
      "step": 2357
    },
    {
      "epoch": 4.188277087033748,
      "grad_norm": 1.126813530921936,
      "learning_rate": 1.2770892488198139e-05,
      "loss": 1.1649,
      "step": 2358
    },
    {
      "epoch": 4.190053285968029,
      "grad_norm": 1.1671197414398193,
      "learning_rate": 1.271627804877865e-05,
      "loss": 1.1005,
      "step": 2359
    },
    {
      "epoch": 4.191829484902309,
      "grad_norm": 1.185644268989563,
      "learning_rate": 1.2661772708245535e-05,
      "loss": 1.0602,
      "step": 2360
    },
    {
      "epoch": 4.19360568383659,
      "grad_norm": 1.244253158569336,
      "learning_rate": 1.2607376534726844e-05,
      "loss": 1.3071,
      "step": 2361
    },
    {
      "epoch": 4.19538188277087,
      "grad_norm": 1.2095822095870972,
      "learning_rate": 1.255308959621424e-05,
      "loss": 1.1847,
      "step": 2362
    },
    {
      "epoch": 4.197158081705151,
      "grad_norm": 1.200836181640625,
      "learning_rate": 1.2498911960562754e-05,
      "loss": 1.1779,
      "step": 2363
    },
    {
      "epoch": 4.198934280639431,
      "grad_norm": 1.2078169584274292,
      "learning_rate": 1.2444843695490881e-05,
      "loss": 1.2581,
      "step": 2364
    },
    {
      "epoch": 4.200710479573712,
      "grad_norm": 1.1536779403686523,
      "learning_rate": 1.23908848685804e-05,
      "loss": 1.2461,
      "step": 2365
    },
    {
      "epoch": 4.202486678507993,
      "grad_norm": 1.1270734071731567,
      "learning_rate": 1.2337035547276232e-05,
      "loss": 1.3752,
      "step": 2366
    },
    {
      "epoch": 4.2042628774422734,
      "grad_norm": 1.1105523109436035,
      "learning_rate": 1.2283295798886506e-05,
      "loss": 0.9982,
      "step": 2367
    },
    {
      "epoch": 4.2060390763765545,
      "grad_norm": 1.1719120740890503,
      "learning_rate": 1.2229665690582359e-05,
      "loss": 1.2748,
      "step": 2368
    },
    {
      "epoch": 4.207815275310835,
      "grad_norm": 1.2113937139511108,
      "learning_rate": 1.2176145289397844e-05,
      "loss": 1.36,
      "step": 2369
    },
    {
      "epoch": 4.209591474245116,
      "grad_norm": 1.150307536125183,
      "learning_rate": 1.2122734662229984e-05,
      "loss": 1.0219,
      "step": 2370
    },
    {
      "epoch": 4.211367673179396,
      "grad_norm": 1.3522576093673706,
      "learning_rate": 1.2069433875838498e-05,
      "loss": 1.3143,
      "step": 2371
    },
    {
      "epoch": 4.213143872113677,
      "grad_norm": 1.2594510316848755,
      "learning_rate": 1.2016242996845839e-05,
      "loss": 1.3124,
      "step": 2372
    },
    {
      "epoch": 4.214920071047957,
      "grad_norm": 1.2505494356155396,
      "learning_rate": 1.1963162091737146e-05,
      "loss": 1.3318,
      "step": 2373
    },
    {
      "epoch": 4.216696269982238,
      "grad_norm": 1.1431437730789185,
      "learning_rate": 1.1910191226859979e-05,
      "loss": 1.3911,
      "step": 2374
    },
    {
      "epoch": 4.218472468916518,
      "grad_norm": 1.1329905986785889,
      "learning_rate": 1.1857330468424466e-05,
      "loss": 1.2979,
      "step": 2375
    },
    {
      "epoch": 4.220248667850799,
      "grad_norm": 1.1438076496124268,
      "learning_rate": 1.1804579882503009e-05,
      "loss": 1.2136,
      "step": 2376
    },
    {
      "epoch": 4.22202486678508,
      "grad_norm": 1.170140266418457,
      "learning_rate": 1.1751939535030431e-05,
      "loss": 1.217,
      "step": 2377
    },
    {
      "epoch": 4.2238010657193605,
      "grad_norm": 1.2708622217178345,
      "learning_rate": 1.1699409491803626e-05,
      "loss": 1.3833,
      "step": 2378
    },
    {
      "epoch": 4.225577264653642,
      "grad_norm": 1.194948673248291,
      "learning_rate": 1.1646989818481713e-05,
      "loss": 1.4529,
      "step": 2379
    },
    {
      "epoch": 4.227353463587922,
      "grad_norm": 1.2881441116333008,
      "learning_rate": 1.1594680580585814e-05,
      "loss": 1.3507,
      "step": 2380
    },
    {
      "epoch": 4.229129662522203,
      "grad_norm": 1.210526466369629,
      "learning_rate": 1.1542481843498998e-05,
      "loss": 1.3308,
      "step": 2381
    },
    {
      "epoch": 4.230905861456483,
      "grad_norm": 1.1636940240859985,
      "learning_rate": 1.149039367246626e-05,
      "loss": 1.3153,
      "step": 2382
    },
    {
      "epoch": 4.232682060390764,
      "grad_norm": 1.3174697160720825,
      "learning_rate": 1.1438416132594388e-05,
      "loss": 1.3587,
      "step": 2383
    },
    {
      "epoch": 4.234458259325044,
      "grad_norm": 1.1497220993041992,
      "learning_rate": 1.1386549288851832e-05,
      "loss": 1.3195,
      "step": 2384
    },
    {
      "epoch": 4.236234458259325,
      "grad_norm": 1.2299267053604126,
      "learning_rate": 1.133479320606874e-05,
      "loss": 1.2388,
      "step": 2385
    },
    {
      "epoch": 4.238010657193605,
      "grad_norm": 1.289503812789917,
      "learning_rate": 1.12831479489368e-05,
      "loss": 1.1053,
      "step": 2386
    },
    {
      "epoch": 4.239786856127886,
      "grad_norm": 1.3234704732894897,
      "learning_rate": 1.1231613582009193e-05,
      "loss": 1.3585,
      "step": 2387
    },
    {
      "epoch": 4.241563055062167,
      "grad_norm": 1.2984449863433838,
      "learning_rate": 1.1180190169700421e-05,
      "loss": 1.2861,
      "step": 2388
    },
    {
      "epoch": 4.243339253996448,
      "grad_norm": 1.2201039791107178,
      "learning_rate": 1.1128877776286384e-05,
      "loss": 1.1925,
      "step": 2389
    },
    {
      "epoch": 4.245115452930728,
      "grad_norm": 1.2240862846374512,
      "learning_rate": 1.1077676465904208e-05,
      "loss": 1.2907,
      "step": 2390
    },
    {
      "epoch": 4.246891651865009,
      "grad_norm": 1.2260692119598389,
      "learning_rate": 1.10265863025521e-05,
      "loss": 1.2214,
      "step": 2391
    },
    {
      "epoch": 4.24866785079929,
      "grad_norm": 1.1475636959075928,
      "learning_rate": 1.0975607350089411e-05,
      "loss": 1.4767,
      "step": 2392
    },
    {
      "epoch": 4.25044404973357,
      "grad_norm": 1.2264866828918457,
      "learning_rate": 1.092473967223646e-05,
      "loss": 1.5045,
      "step": 2393
    },
    {
      "epoch": 4.252220248667851,
      "grad_norm": 1.261672019958496,
      "learning_rate": 1.0873983332574512e-05,
      "loss": 1.4702,
      "step": 2394
    },
    {
      "epoch": 4.253996447602131,
      "grad_norm": 1.2199386358261108,
      "learning_rate": 1.082333839454559e-05,
      "loss": 1.1589,
      "step": 2395
    },
    {
      "epoch": 4.255772646536412,
      "grad_norm": 1.2256343364715576,
      "learning_rate": 1.077280492145254e-05,
      "loss": 1.1694,
      "step": 2396
    },
    {
      "epoch": 4.2575488454706925,
      "grad_norm": 1.301529049873352,
      "learning_rate": 1.07223829764589e-05,
      "loss": 1.3276,
      "step": 2397
    },
    {
      "epoch": 4.2593250444049735,
      "grad_norm": 1.1466197967529297,
      "learning_rate": 1.0672072622588702e-05,
      "loss": 1.3399,
      "step": 2398
    },
    {
      "epoch": 4.261101243339254,
      "grad_norm": 1.2721261978149414,
      "learning_rate": 1.0621873922726632e-05,
      "loss": 1.1812,
      "step": 2399
    },
    {
      "epoch": 4.262877442273535,
      "grad_norm": 1.2233885526657104,
      "learning_rate": 1.0571786939617712e-05,
      "loss": 1.3316,
      "step": 2400
    },
    {
      "epoch": 4.262877442273535,
      "eval_loss": 2.12848162651062,
      "eval_runtime": 17.5564,
      "eval_samples_per_second": 57.016,
      "eval_steps_per_second": 28.537,
      "step": 2400
    },
    {
      "epoch": 4.264653641207815,
      "grad_norm": 1.149707317352295,
      "learning_rate": 1.0521811735867359e-05,
      "loss": 1.1879,
      "step": 2401
    },
    {
      "epoch": 4.266429840142096,
      "grad_norm": 1.237004041671753,
      "learning_rate": 1.0471948373941321e-05,
      "loss": 1.465,
      "step": 2402
    },
    {
      "epoch": 4.268206039076377,
      "grad_norm": 1.2785277366638184,
      "learning_rate": 1.042219691616544e-05,
      "loss": 1.3101,
      "step": 2403
    },
    {
      "epoch": 4.269982238010657,
      "grad_norm": 1.2426360845565796,
      "learning_rate": 1.0372557424725803e-05,
      "loss": 1.3517,
      "step": 2404
    },
    {
      "epoch": 4.271758436944938,
      "grad_norm": 1.1908115148544312,
      "learning_rate": 1.0323029961668462e-05,
      "loss": 1.5095,
      "step": 2405
    },
    {
      "epoch": 4.273534635879218,
      "grad_norm": 1.3173952102661133,
      "learning_rate": 1.0273614588899493e-05,
      "loss": 1.1788,
      "step": 2406
    },
    {
      "epoch": 4.275310834813499,
      "grad_norm": 1.2281615734100342,
      "learning_rate": 1.0224311368184858e-05,
      "loss": 1.3784,
      "step": 2407
    },
    {
      "epoch": 4.2770870337477795,
      "grad_norm": 1.1998788118362427,
      "learning_rate": 1.017512036115027e-05,
      "loss": 0.983,
      "step": 2408
    },
    {
      "epoch": 4.278863232682061,
      "grad_norm": 1.2813928127288818,
      "learning_rate": 1.0126041629281313e-05,
      "loss": 1.496,
      "step": 2409
    },
    {
      "epoch": 4.280639431616341,
      "grad_norm": 1.1466646194458008,
      "learning_rate": 1.0077075233923116e-05,
      "loss": 1.3654,
      "step": 2410
    },
    {
      "epoch": 4.282415630550622,
      "grad_norm": 1.2122209072113037,
      "learning_rate": 1.0028221236280444e-05,
      "loss": 1.4173,
      "step": 2411
    },
    {
      "epoch": 4.284191829484902,
      "grad_norm": 1.22955322265625,
      "learning_rate": 9.979479697417593e-06,
      "loss": 1.122,
      "step": 2412
    },
    {
      "epoch": 4.285968028419183,
      "grad_norm": 1.2256712913513184,
      "learning_rate": 9.930850678258219e-06,
      "loss": 1.235,
      "step": 2413
    },
    {
      "epoch": 4.287744227353464,
      "grad_norm": 1.254604458808899,
      "learning_rate": 9.882334239585412e-06,
      "loss": 1.4677,
      "step": 2414
    },
    {
      "epoch": 4.289520426287744,
      "grad_norm": 1.2681370973587036,
      "learning_rate": 9.833930442041506e-06,
      "loss": 1.3185,
      "step": 2415
    },
    {
      "epoch": 4.291296625222025,
      "grad_norm": 1.2263211011886597,
      "learning_rate": 9.785639346128084e-06,
      "loss": 1.268,
      "step": 2416
    },
    {
      "epoch": 4.293072824156305,
      "grad_norm": 1.230762243270874,
      "learning_rate": 9.737461012205773e-06,
      "loss": 1.1683,
      "step": 2417
    },
    {
      "epoch": 4.2948490230905865,
      "grad_norm": 1.2064087390899658,
      "learning_rate": 9.689395500494335e-06,
      "loss": 1.1967,
      "step": 2418
    },
    {
      "epoch": 4.296625222024867,
      "grad_norm": 1.3024893999099731,
      "learning_rate": 9.641442871072493e-06,
      "loss": 1.2672,
      "step": 2419
    },
    {
      "epoch": 4.298401420959148,
      "grad_norm": 1.249682903289795,
      "learning_rate": 9.593603183877841e-06,
      "loss": 1.3628,
      "step": 2420
    },
    {
      "epoch": 4.300177619893428,
      "grad_norm": 1.2317190170288086,
      "learning_rate": 9.545876498706852e-06,
      "loss": 1.2894,
      "step": 2421
    },
    {
      "epoch": 4.301953818827709,
      "grad_norm": 1.2375208139419556,
      "learning_rate": 9.498262875214726e-06,
      "loss": 1.2185,
      "step": 2422
    },
    {
      "epoch": 4.303730017761989,
      "grad_norm": 1.3436561822891235,
      "learning_rate": 9.450762372915322e-06,
      "loss": 1.4351,
      "step": 2423
    },
    {
      "epoch": 4.30550621669627,
      "grad_norm": 1.2440221309661865,
      "learning_rate": 9.403375051181151e-06,
      "loss": 1.2552,
      "step": 2424
    },
    {
      "epoch": 4.30728241563055,
      "grad_norm": 1.2483760118484497,
      "learning_rate": 9.35610096924323e-06,
      "loss": 1.237,
      "step": 2425
    },
    {
      "epoch": 4.309058614564831,
      "grad_norm": 1.2058310508728027,
      "learning_rate": 9.308940186191062e-06,
      "loss": 1.2139,
      "step": 2426
    },
    {
      "epoch": 4.3108348134991115,
      "grad_norm": 1.200130820274353,
      "learning_rate": 9.261892760972469e-06,
      "loss": 1.4473,
      "step": 2427
    },
    {
      "epoch": 4.3126110124333925,
      "grad_norm": 1.3830727338790894,
      "learning_rate": 9.214958752393643e-06,
      "loss": 1.247,
      "step": 2428
    },
    {
      "epoch": 4.314387211367674,
      "grad_norm": 1.3152127265930176,
      "learning_rate": 9.168138219119004e-06,
      "loss": 1.3278,
      "step": 2429
    },
    {
      "epoch": 4.316163410301954,
      "grad_norm": 1.1823190450668335,
      "learning_rate": 9.121431219671095e-06,
      "loss": 1.2659,
      "step": 2430
    },
    {
      "epoch": 4.317939609236235,
      "grad_norm": 1.2334281206130981,
      "learning_rate": 9.074837812430626e-06,
      "loss": 1.3671,
      "step": 2431
    },
    {
      "epoch": 4.319715808170515,
      "grad_norm": 1.214759111404419,
      "learning_rate": 9.028358055636243e-06,
      "loss": 1.278,
      "step": 2432
    },
    {
      "epoch": 4.321492007104796,
      "grad_norm": 1.2016905546188354,
      "learning_rate": 8.981992007384599e-06,
      "loss": 1.3561,
      "step": 2433
    },
    {
      "epoch": 4.323268206039076,
      "grad_norm": 1.2031980752944946,
      "learning_rate": 8.935739725630154e-06,
      "loss": 1.3214,
      "step": 2434
    },
    {
      "epoch": 4.325044404973357,
      "grad_norm": 1.1821718215942383,
      "learning_rate": 8.889601268185232e-06,
      "loss": 1.2524,
      "step": 2435
    },
    {
      "epoch": 4.326820603907637,
      "grad_norm": 1.2187086343765259,
      "learning_rate": 8.843576692719857e-06,
      "loss": 1.3913,
      "step": 2436
    },
    {
      "epoch": 4.328596802841918,
      "grad_norm": 1.176281452178955,
      "learning_rate": 8.797666056761678e-06,
      "loss": 1.2679,
      "step": 2437
    },
    {
      "epoch": 4.3303730017761985,
      "grad_norm": 1.1439664363861084,
      "learning_rate": 8.751869417696002e-06,
      "loss": 1.4129,
      "step": 2438
    },
    {
      "epoch": 4.33214920071048,
      "grad_norm": 1.1988803148269653,
      "learning_rate": 8.706186832765583e-06,
      "loss": 1.4281,
      "step": 2439
    },
    {
      "epoch": 4.333925399644761,
      "grad_norm": 1.2340657711029053,
      "learning_rate": 8.660618359070604e-06,
      "loss": 1.3112,
      "step": 2440
    },
    {
      "epoch": 4.335701598579041,
      "grad_norm": 1.1805073022842407,
      "learning_rate": 8.615164053568703e-06,
      "loss": 1.3075,
      "step": 2441
    },
    {
      "epoch": 4.337477797513322,
      "grad_norm": 1.239097237586975,
      "learning_rate": 8.56982397307471e-06,
      "loss": 1.2292,
      "step": 2442
    },
    {
      "epoch": 4.339253996447602,
      "grad_norm": 1.287329912185669,
      "learning_rate": 8.524598174260757e-06,
      "loss": 1.3437,
      "step": 2443
    },
    {
      "epoch": 4.341030195381883,
      "grad_norm": 1.189213514328003,
      "learning_rate": 8.479486713656115e-06,
      "loss": 1.0076,
      "step": 2444
    },
    {
      "epoch": 4.342806394316163,
      "grad_norm": 1.1823257207870483,
      "learning_rate": 8.434489647647092e-06,
      "loss": 1.3035,
      "step": 2445
    },
    {
      "epoch": 4.344582593250444,
      "grad_norm": 1.1296766996383667,
      "learning_rate": 8.389607032477097e-06,
      "loss": 1.4426,
      "step": 2446
    },
    {
      "epoch": 4.346358792184724,
      "grad_norm": 1.187520146369934,
      "learning_rate": 8.344838924246357e-06,
      "loss": 1.4116,
      "step": 2447
    },
    {
      "epoch": 4.3481349911190055,
      "grad_norm": 1.2717878818511963,
      "learning_rate": 8.30018537891214e-06,
      "loss": 1.4136,
      "step": 2448
    },
    {
      "epoch": 4.349911190053286,
      "grad_norm": 1.1967384815216064,
      "learning_rate": 8.255646452288379e-06,
      "loss": 1.3112,
      "step": 2449
    },
    {
      "epoch": 4.351687388987567,
      "grad_norm": 1.2353882789611816,
      "learning_rate": 8.211222200045788e-06,
      "loss": 1.2639,
      "step": 2450
    },
    {
      "epoch": 4.353463587921847,
      "grad_norm": 1.2300360202789307,
      "learning_rate": 8.166912677711768e-06,
      "loss": 1.4764,
      "step": 2451
    },
    {
      "epoch": 4.355239786856128,
      "grad_norm": 1.2914810180664062,
      "learning_rate": 8.122717940670277e-06,
      "loss": 1.3562,
      "step": 2452
    },
    {
      "epoch": 4.357015985790409,
      "grad_norm": 1.244256854057312,
      "learning_rate": 8.078638044161824e-06,
      "loss": 1.3386,
      "step": 2453
    },
    {
      "epoch": 4.358792184724689,
      "grad_norm": 1.2137106657028198,
      "learning_rate": 8.034673043283359e-06,
      "loss": 1.4218,
      "step": 2454
    },
    {
      "epoch": 4.36056838365897,
      "grad_norm": 1.3328946828842163,
      "learning_rate": 7.990822992988267e-06,
      "loss": 1.3759,
      "step": 2455
    },
    {
      "epoch": 4.36234458259325,
      "grad_norm": 1.228165626525879,
      "learning_rate": 7.947087948086163e-06,
      "loss": 1.4225,
      "step": 2456
    },
    {
      "epoch": 4.364120781527531,
      "grad_norm": 1.2087116241455078,
      "learning_rate": 7.903467963242972e-06,
      "loss": 1.3274,
      "step": 2457
    },
    {
      "epoch": 4.3658969804618115,
      "grad_norm": 1.2085005044937134,
      "learning_rate": 7.859963092980838e-06,
      "loss": 1.1501,
      "step": 2458
    },
    {
      "epoch": 4.367673179396093,
      "grad_norm": 1.258297085762024,
      "learning_rate": 7.816573391677928e-06,
      "loss": 1.4681,
      "step": 2459
    },
    {
      "epoch": 4.369449378330373,
      "grad_norm": 1.2403879165649414,
      "learning_rate": 7.773298913568505e-06,
      "loss": 1.3147,
      "step": 2460
    },
    {
      "epoch": 4.371225577264654,
      "grad_norm": 1.2067070007324219,
      "learning_rate": 7.730139712742845e-06,
      "loss": 1.4978,
      "step": 2461
    },
    {
      "epoch": 4.373001776198934,
      "grad_norm": 1.2237366437911987,
      "learning_rate": 7.687095843147063e-06,
      "loss": 1.2608,
      "step": 2462
    },
    {
      "epoch": 4.374777975133215,
      "grad_norm": 1.1647396087646484,
      "learning_rate": 7.644167358583153e-06,
      "loss": 1.2565,
      "step": 2463
    },
    {
      "epoch": 4.376554174067495,
      "grad_norm": 1.2672470808029175,
      "learning_rate": 7.601354312708897e-06,
      "loss": 1.2147,
      "step": 2464
    },
    {
      "epoch": 4.378330373001776,
      "grad_norm": 1.1534253358840942,
      "learning_rate": 7.558656759037797e-06,
      "loss": 1.1985,
      "step": 2465
    },
    {
      "epoch": 4.380106571936057,
      "grad_norm": 1.2666330337524414,
      "learning_rate": 7.516074750938928e-06,
      "loss": 1.4425,
      "step": 2466
    },
    {
      "epoch": 4.381882770870337,
      "grad_norm": 1.2914787530899048,
      "learning_rate": 7.4736083416370065e-06,
      "loss": 1.31,
      "step": 2467
    },
    {
      "epoch": 4.383658969804618,
      "grad_norm": 1.1588983535766602,
      "learning_rate": 7.431257584212248e-06,
      "loss": 1.2966,
      "step": 2468
    },
    {
      "epoch": 4.385435168738899,
      "grad_norm": 1.255048394203186,
      "learning_rate": 7.389022531600276e-06,
      "loss": 1.4409,
      "step": 2469
    },
    {
      "epoch": 4.38721136767318,
      "grad_norm": 1.1951839923858643,
      "learning_rate": 7.346903236592162e-06,
      "loss": 1.1558,
      "step": 2470
    },
    {
      "epoch": 4.38898756660746,
      "grad_norm": 1.2774215936660767,
      "learning_rate": 7.304899751834193e-06,
      "loss": 1.4365,
      "step": 2471
    },
    {
      "epoch": 4.390763765541741,
      "grad_norm": 1.2670059204101562,
      "learning_rate": 7.263012129827973e-06,
      "loss": 1.2383,
      "step": 2472
    },
    {
      "epoch": 4.392539964476021,
      "grad_norm": 1.231420874595642,
      "learning_rate": 7.221240422930287e-06,
      "loss": 1.4423,
      "step": 2473
    },
    {
      "epoch": 4.394316163410302,
      "grad_norm": 1.2951555252075195,
      "learning_rate": 7.179584683352958e-06,
      "loss": 1.3997,
      "step": 2474
    },
    {
      "epoch": 4.396092362344582,
      "grad_norm": 1.2625596523284912,
      "learning_rate": 7.13804496316296e-06,
      "loss": 1.4635,
      "step": 2475
    },
    {
      "epoch": 4.397868561278863,
      "grad_norm": 1.2614682912826538,
      "learning_rate": 7.096621314282148e-06,
      "loss": 1.42,
      "step": 2476
    },
    {
      "epoch": 4.399644760213144,
      "grad_norm": 1.2159409523010254,
      "learning_rate": 7.055313788487372e-06,
      "loss": 1.2277,
      "step": 2477
    },
    {
      "epoch": 4.4014209591474245,
      "grad_norm": 1.2137564420700073,
      "learning_rate": 7.01412243741032e-06,
      "loss": 1.2748,
      "step": 2478
    },
    {
      "epoch": 4.4031971580817055,
      "grad_norm": 1.263540506362915,
      "learning_rate": 6.9730473125374285e-06,
      "loss": 1.2806,
      "step": 2479
    },
    {
      "epoch": 4.404973357015986,
      "grad_norm": 1.2377517223358154,
      "learning_rate": 6.9320884652099406e-06,
      "loss": 1.532,
      "step": 2480
    },
    {
      "epoch": 4.406749555950267,
      "grad_norm": 1.2258555889129639,
      "learning_rate": 6.891245946623659e-06,
      "loss": 1.3007,
      "step": 2481
    },
    {
      "epoch": 4.408525754884547,
      "grad_norm": 1.140043020248413,
      "learning_rate": 6.850519807829059e-06,
      "loss": 1.4515,
      "step": 2482
    },
    {
      "epoch": 4.410301953818828,
      "grad_norm": 1.2961570024490356,
      "learning_rate": 6.809910099731143e-06,
      "loss": 1.3806,
      "step": 2483
    },
    {
      "epoch": 4.412078152753108,
      "grad_norm": 1.2029472589492798,
      "learning_rate": 6.769416873089318e-06,
      "loss": 1.2267,
      "step": 2484
    },
    {
      "epoch": 4.413854351687389,
      "grad_norm": 1.2939221858978271,
      "learning_rate": 6.729040178517454e-06,
      "loss": 1.3751,
      "step": 2485
    },
    {
      "epoch": 4.415630550621669,
      "grad_norm": 1.1427432298660278,
      "learning_rate": 6.68878006648378e-06,
      "loss": 1.4086,
      "step": 2486
    },
    {
      "epoch": 4.41740674955595,
      "grad_norm": 1.2597284317016602,
      "learning_rate": 6.648636587310764e-06,
      "loss": 1.4036,
      "step": 2487
    },
    {
      "epoch": 4.4191829484902305,
      "grad_norm": 1.23549222946167,
      "learning_rate": 6.60860979117508e-06,
      "loss": 1.1426,
      "step": 2488
    },
    {
      "epoch": 4.420959147424512,
      "grad_norm": 1.2381912469863892,
      "learning_rate": 6.568699728107608e-06,
      "loss": 1.3253,
      "step": 2489
    },
    {
      "epoch": 4.422735346358792,
      "grad_norm": 1.2190715074539185,
      "learning_rate": 6.528906447993288e-06,
      "loss": 1.2798,
      "step": 2490
    },
    {
      "epoch": 4.424511545293073,
      "grad_norm": 1.2200431823730469,
      "learning_rate": 6.489230000571067e-06,
      "loss": 1.3074,
      "step": 2491
    },
    {
      "epoch": 4.426287744227354,
      "grad_norm": 1.1401267051696777,
      "learning_rate": 6.449670435433897e-06,
      "loss": 1.3978,
      "step": 2492
    },
    {
      "epoch": 4.428063943161634,
      "grad_norm": 1.2873504161834717,
      "learning_rate": 6.410227802028635e-06,
      "loss": 1.2146,
      "step": 2493
    },
    {
      "epoch": 4.429840142095915,
      "grad_norm": 1.3713831901550293,
      "learning_rate": 6.370902149655944e-06,
      "loss": 1.2622,
      "step": 2494
    },
    {
      "epoch": 4.431616341030195,
      "grad_norm": 1.2930597066879272,
      "learning_rate": 6.331693527470306e-06,
      "loss": 1.3724,
      "step": 2495
    },
    {
      "epoch": 4.433392539964476,
      "grad_norm": 1.2756136655807495,
      "learning_rate": 6.292601984479907e-06,
      "loss": 1.3612,
      "step": 2496
    },
    {
      "epoch": 4.435168738898756,
      "grad_norm": 1.1775438785552979,
      "learning_rate": 6.253627569546594e-06,
      "loss": 1.3088,
      "step": 2497
    },
    {
      "epoch": 4.4369449378330375,
      "grad_norm": 1.2266281843185425,
      "learning_rate": 6.2147703313858e-06,
      "loss": 1.4625,
      "step": 2498
    },
    {
      "epoch": 4.438721136767318,
      "grad_norm": 1.3010329008102417,
      "learning_rate": 6.176030318566517e-06,
      "loss": 1.2971,
      "step": 2499
    },
    {
      "epoch": 4.440497335701599,
      "grad_norm": 1.172834038734436,
      "learning_rate": 6.137407579511212e-06,
      "loss": 1.3191,
      "step": 2500
    },
    {
      "epoch": 4.440497335701599,
      "eval_loss": 2.1278631687164307,
      "eval_runtime": 17.4923,
      "eval_samples_per_second": 57.225,
      "eval_steps_per_second": 28.641,
      "step": 2500
    },
    {
      "epoch": 4.442273534635879,
      "grad_norm": 1.3035197257995605,
      "learning_rate": 6.098902162495712e-06,
      "loss": 1.4632,
      "step": 2501
    },
    {
      "epoch": 4.44404973357016,
      "grad_norm": 1.2456331253051758,
      "learning_rate": 6.060514115649307e-06,
      "loss": 1.1151,
      "step": 2502
    },
    {
      "epoch": 4.445825932504441,
      "grad_norm": 1.127403736114502,
      "learning_rate": 6.022243486954471e-06,
      "loss": 1.1453,
      "step": 2503
    },
    {
      "epoch": 4.447602131438721,
      "grad_norm": 1.2756823301315308,
      "learning_rate": 5.984090324246994e-06,
      "loss": 1.2389,
      "step": 2504
    },
    {
      "epoch": 4.449378330373002,
      "grad_norm": 1.2095539569854736,
      "learning_rate": 5.946054675215784e-06,
      "loss": 1.3108,
      "step": 2505
    },
    {
      "epoch": 4.451154529307282,
      "grad_norm": 1.3339042663574219,
      "learning_rate": 5.9081365874029015e-06,
      "loss": 1.4537,
      "step": 2506
    },
    {
      "epoch": 4.452930728241563,
      "grad_norm": 1.2483389377593994,
      "learning_rate": 5.870336108203467e-06,
      "loss": 1.219,
      "step": 2507
    },
    {
      "epoch": 4.4547069271758435,
      "grad_norm": 1.1905529499053955,
      "learning_rate": 5.83265328486553e-06,
      "loss": 1.215,
      "step": 2508
    },
    {
      "epoch": 4.4564831261101245,
      "grad_norm": 1.248869776725769,
      "learning_rate": 5.795088164490203e-06,
      "loss": 1.5202,
      "step": 2509
    },
    {
      "epoch": 4.458259325044405,
      "grad_norm": 1.3279807567596436,
      "learning_rate": 5.757640794031361e-06,
      "loss": 1.2446,
      "step": 2510
    },
    {
      "epoch": 4.460035523978686,
      "grad_norm": 1.2439855337142944,
      "learning_rate": 5.720311220295727e-06,
      "loss": 1.4435,
      "step": 2511
    },
    {
      "epoch": 4.461811722912966,
      "grad_norm": 1.274762511253357,
      "learning_rate": 5.683099489942845e-06,
      "loss": 1.2784,
      "step": 2512
    },
    {
      "epoch": 4.463587921847247,
      "grad_norm": 1.2499973773956299,
      "learning_rate": 5.646005649484887e-06,
      "loss": 1.3484,
      "step": 2513
    },
    {
      "epoch": 4.465364120781528,
      "grad_norm": 1.193535327911377,
      "learning_rate": 5.6090297452867204e-06,
      "loss": 1.3955,
      "step": 2514
    },
    {
      "epoch": 4.467140319715808,
      "grad_norm": 1.1888364553451538,
      "learning_rate": 5.572171823565797e-06,
      "loss": 1.1991,
      "step": 2515
    },
    {
      "epoch": 4.468916518650089,
      "grad_norm": 1.380021333694458,
      "learning_rate": 5.535431930392043e-06,
      "loss": 1.3865,
      "step": 2516
    },
    {
      "epoch": 4.470692717584369,
      "grad_norm": 1.1759960651397705,
      "learning_rate": 5.498810111687924e-06,
      "loss": 1.2958,
      "step": 2517
    },
    {
      "epoch": 4.47246891651865,
      "grad_norm": 1.2752842903137207,
      "learning_rate": 5.462306413228291e-06,
      "loss": 1.335,
      "step": 2518
    },
    {
      "epoch": 4.474245115452931,
      "grad_norm": 1.3032326698303223,
      "learning_rate": 5.4259208806403674e-06,
      "loss": 1.3309,
      "step": 2519
    },
    {
      "epoch": 4.476021314387212,
      "grad_norm": 1.2281222343444824,
      "learning_rate": 5.389653559403629e-06,
      "loss": 1.2805,
      "step": 2520
    },
    {
      "epoch": 4.477797513321492,
      "grad_norm": 1.196459412574768,
      "learning_rate": 5.353504494849871e-06,
      "loss": 1.2288,
      "step": 2521
    },
    {
      "epoch": 4.479573712255773,
      "grad_norm": 1.0709962844848633,
      "learning_rate": 5.317473732163025e-06,
      "loss": 1.1952,
      "step": 2522
    },
    {
      "epoch": 4.481349911190053,
      "grad_norm": 1.1450316905975342,
      "learning_rate": 5.281561316379147e-06,
      "loss": 1.1907,
      "step": 2523
    },
    {
      "epoch": 4.483126110124334,
      "grad_norm": 1.161328673362732,
      "learning_rate": 5.245767292386395e-06,
      "loss": 1.3345,
      "step": 2524
    },
    {
      "epoch": 4.484902309058614,
      "grad_norm": 1.225113868713379,
      "learning_rate": 5.210091704924946e-06,
      "loss": 1.5104,
      "step": 2525
    },
    {
      "epoch": 4.486678507992895,
      "grad_norm": 1.3822064399719238,
      "learning_rate": 5.174534598586922e-06,
      "loss": 1.3138,
      "step": 2526
    },
    {
      "epoch": 4.488454706927175,
      "grad_norm": 1.2324459552764893,
      "learning_rate": 5.139096017816336e-06,
      "loss": 1.3304,
      "step": 2527
    },
    {
      "epoch": 4.4902309058614565,
      "grad_norm": 1.2807413339614868,
      "learning_rate": 5.1037760069091e-06,
      "loss": 1.4658,
      "step": 2528
    },
    {
      "epoch": 4.4920071047957375,
      "grad_norm": 1.3501557111740112,
      "learning_rate": 5.0685746100128885e-06,
      "loss": 1.363,
      "step": 2529
    },
    {
      "epoch": 4.493783303730018,
      "grad_norm": 1.1676323413848877,
      "learning_rate": 5.033491871127105e-06,
      "loss": 1.1515,
      "step": 2530
    },
    {
      "epoch": 4.495559502664299,
      "grad_norm": 1.2028398513793945,
      "learning_rate": 4.998527834102873e-06,
      "loss": 1.0517,
      "step": 2531
    },
    {
      "epoch": 4.497335701598579,
      "grad_norm": 1.1907544136047363,
      "learning_rate": 4.963682542642934e-06,
      "loss": 1.0805,
      "step": 2532
    },
    {
      "epoch": 4.49911190053286,
      "grad_norm": 1.3624621629714966,
      "learning_rate": 4.92895604030158e-06,
      "loss": 1.3937,
      "step": 2533
    },
    {
      "epoch": 4.50088809946714,
      "grad_norm": 1.1753349304199219,
      "learning_rate": 4.8943483704846475e-06,
      "loss": 1.3908,
      "step": 2534
    },
    {
      "epoch": 4.502664298401421,
      "grad_norm": 1.2760969400405884,
      "learning_rate": 4.859859576449444e-06,
      "loss": 1.3062,
      "step": 2535
    },
    {
      "epoch": 4.504440497335701,
      "grad_norm": 1.31659734249115,
      "learning_rate": 4.825489701304709e-06,
      "loss": 1.2225,
      "step": 2536
    },
    {
      "epoch": 4.506216696269982,
      "grad_norm": 1.2854493856430054,
      "learning_rate": 4.791238788010477e-06,
      "loss": 1.5783,
      "step": 2537
    },
    {
      "epoch": 4.5079928952042625,
      "grad_norm": 1.2222851514816284,
      "learning_rate": 4.757106879378137e-06,
      "loss": 1.1287,
      "step": 2538
    },
    {
      "epoch": 4.5097690941385435,
      "grad_norm": 1.1746646165847778,
      "learning_rate": 4.72309401807034e-06,
      "loss": 1.2255,
      "step": 2539
    },
    {
      "epoch": 4.511545293072825,
      "grad_norm": 1.2460962533950806,
      "learning_rate": 4.689200246600867e-06,
      "loss": 1.1799,
      "step": 2540
    },
    {
      "epoch": 4.513321492007105,
      "grad_norm": 1.2862975597381592,
      "learning_rate": 4.655425607334762e-06,
      "loss": 1.1459,
      "step": 2541
    },
    {
      "epoch": 4.515097690941386,
      "grad_norm": 1.2345212697982788,
      "learning_rate": 4.621770142488036e-06,
      "loss": 1.2598,
      "step": 2542
    },
    {
      "epoch": 4.516873889875666,
      "grad_norm": 1.1820404529571533,
      "learning_rate": 4.588233894127825e-06,
      "loss": 1.3467,
      "step": 2543
    },
    {
      "epoch": 4.518650088809947,
      "grad_norm": 1.218204379081726,
      "learning_rate": 4.554816904172232e-06,
      "loss": 1.5613,
      "step": 2544
    },
    {
      "epoch": 4.520426287744227,
      "grad_norm": 1.2762664556503296,
      "learning_rate": 4.521519214390257e-06,
      "loss": 1.2689,
      "step": 2545
    },
    {
      "epoch": 4.522202486678508,
      "grad_norm": 1.1717495918273926,
      "learning_rate": 4.48834086640183e-06,
      "loss": 1.1236,
      "step": 2546
    },
    {
      "epoch": 4.523978685612788,
      "grad_norm": 1.2278209924697876,
      "learning_rate": 4.455281901677699e-06,
      "loss": 1.4146,
      "step": 2547
    },
    {
      "epoch": 4.525754884547069,
      "grad_norm": 1.1621167659759521,
      "learning_rate": 4.422342361539367e-06,
      "loss": 1.6357,
      "step": 2548
    },
    {
      "epoch": 4.52753108348135,
      "grad_norm": 1.224961519241333,
      "learning_rate": 4.389522287159109e-06,
      "loss": 1.3667,
      "step": 2549
    },
    {
      "epoch": 4.529307282415631,
      "grad_norm": 1.1871691942214966,
      "learning_rate": 4.356821719559812e-06,
      "loss": 1.3267,
      "step": 2550
    },
    {
      "epoch": 4.531083481349912,
      "grad_norm": 1.149631142616272,
      "learning_rate": 4.3242406996150674e-06,
      "loss": 1.1583,
      "step": 2551
    },
    {
      "epoch": 4.532859680284192,
      "grad_norm": 1.2243447303771973,
      "learning_rate": 4.291779268048979e-06,
      "loss": 1.2409,
      "step": 2552
    },
    {
      "epoch": 4.534635879218472,
      "grad_norm": 1.2617900371551514,
      "learning_rate": 4.259437465436189e-06,
      "loss": 1.1396,
      "step": 2553
    },
    {
      "epoch": 4.536412078152753,
      "grad_norm": 1.2456881999969482,
      "learning_rate": 4.2272153322018286e-06,
      "loss": 1.1995,
      "step": 2554
    },
    {
      "epoch": 4.538188277087034,
      "grad_norm": 1.362443447113037,
      "learning_rate": 4.195112908621402e-06,
      "loss": 1.225,
      "step": 2555
    },
    {
      "epoch": 4.539964476021314,
      "grad_norm": 1.2505989074707031,
      "learning_rate": 4.163130234820856e-06,
      "loss": 1.4603,
      "step": 2556
    },
    {
      "epoch": 4.541740674955595,
      "grad_norm": 1.2324550151824951,
      "learning_rate": 4.131267350776391e-06,
      "loss": 1.3839,
      "step": 2557
    },
    {
      "epoch": 4.5435168738898755,
      "grad_norm": 1.188165307044983,
      "learning_rate": 4.099524296314538e-06,
      "loss": 1.1176,
      "step": 2558
    },
    {
      "epoch": 4.5452930728241565,
      "grad_norm": 1.2444899082183838,
      "learning_rate": 4.067901111111994e-06,
      "loss": 1.4887,
      "step": 2559
    },
    {
      "epoch": 4.547069271758437,
      "grad_norm": 1.2637624740600586,
      "learning_rate": 4.03639783469566e-06,
      "loss": 1.3267,
      "step": 2560
    },
    {
      "epoch": 4.548845470692718,
      "grad_norm": 1.28956139087677,
      "learning_rate": 4.005014506442551e-06,
      "loss": 1.3968,
      "step": 2561
    },
    {
      "epoch": 4.550621669626998,
      "grad_norm": 1.2221788167953491,
      "learning_rate": 3.973751165579742e-06,
      "loss": 1.2126,
      "step": 2562
    },
    {
      "epoch": 4.552397868561279,
      "grad_norm": 1.25483238697052,
      "learning_rate": 3.942607851184354e-06,
      "loss": 1.2396,
      "step": 2563
    },
    {
      "epoch": 4.554174067495559,
      "grad_norm": 1.3280293941497803,
      "learning_rate": 3.911584602183482e-06,
      "loss": 1.3166,
      "step": 2564
    },
    {
      "epoch": 4.55595026642984,
      "grad_norm": 1.191909670829773,
      "learning_rate": 3.880681457354118e-06,
      "loss": 1.1953,
      "step": 2565
    },
    {
      "epoch": 4.557726465364121,
      "grad_norm": 1.2529376745224,
      "learning_rate": 3.8498984553231644e-06,
      "loss": 1.4445,
      "step": 2566
    },
    {
      "epoch": 4.559502664298401,
      "grad_norm": 1.2135896682739258,
      "learning_rate": 3.819235634567342e-06,
      "loss": 1.5926,
      "step": 2567
    },
    {
      "epoch": 4.561278863232682,
      "grad_norm": 1.1469433307647705,
      "learning_rate": 3.7886930334131486e-06,
      "loss": 1.5698,
      "step": 2568
    },
    {
      "epoch": 4.5630550621669625,
      "grad_norm": 1.2232372760772705,
      "learning_rate": 3.7582706900368026e-06,
      "loss": 1.3916,
      "step": 2569
    },
    {
      "epoch": 4.564831261101244,
      "grad_norm": 1.2802042961120605,
      "learning_rate": 3.727968642464241e-06,
      "loss": 1.3184,
      "step": 2570
    },
    {
      "epoch": 4.566607460035524,
      "grad_norm": 1.3188339471817017,
      "learning_rate": 3.6977869285710345e-06,
      "loss": 1.2763,
      "step": 2571
    },
    {
      "epoch": 4.568383658969805,
      "grad_norm": 1.2664029598236084,
      "learning_rate": 3.6677255860822824e-06,
      "loss": 1.2003,
      "step": 2572
    },
    {
      "epoch": 4.570159857904085,
      "grad_norm": 1.238287329673767,
      "learning_rate": 3.637784652572729e-06,
      "loss": 1.2929,
      "step": 2573
    },
    {
      "epoch": 4.571936056838366,
      "grad_norm": 1.1014071702957153,
      "learning_rate": 3.6079641654665376e-06,
      "loss": 0.9796,
      "step": 2574
    },
    {
      "epoch": 4.573712255772646,
      "grad_norm": 1.2888424396514893,
      "learning_rate": 3.578264162037348e-06,
      "loss": 1.3426,
      "step": 2575
    },
    {
      "epoch": 4.575488454706927,
      "grad_norm": 1.1988343000411987,
      "learning_rate": 3.5486846794082318e-06,
      "loss": 1.2194,
      "step": 2576
    },
    {
      "epoch": 4.577264653641208,
      "grad_norm": 1.3132469654083252,
      "learning_rate": 3.5192257545515363e-06,
      "loss": 1.5629,
      "step": 2577
    },
    {
      "epoch": 4.579040852575488,
      "grad_norm": 1.2359319925308228,
      "learning_rate": 3.4898874242890178e-06,
      "loss": 1.4094,
      "step": 2578
    },
    {
      "epoch": 4.5808170515097695,
      "grad_norm": 1.2011895179748535,
      "learning_rate": 3.4606697252915986e-06,
      "loss": 1.2735,
      "step": 2579
    },
    {
      "epoch": 4.58259325044405,
      "grad_norm": 1.1961915493011475,
      "learning_rate": 3.4315726940795433e-06,
      "loss": 1.0808,
      "step": 2580
    },
    {
      "epoch": 4.584369449378331,
      "grad_norm": 1.2221485376358032,
      "learning_rate": 3.4025963670221705e-06,
      "loss": 1.2608,
      "step": 2581
    },
    {
      "epoch": 4.586145648312611,
      "grad_norm": 1.27096426486969,
      "learning_rate": 3.3737407803379637e-06,
      "loss": 1.2415,
      "step": 2582
    },
    {
      "epoch": 4.587921847246892,
      "grad_norm": 1.209713339805603,
      "learning_rate": 3.34500597009455e-06,
      "loss": 1.276,
      "step": 2583
    },
    {
      "epoch": 4.589698046181172,
      "grad_norm": 1.1724587678909302,
      "learning_rate": 3.316391972208499e-06,
      "loss": 1.3563,
      "step": 2584
    },
    {
      "epoch": 4.591474245115453,
      "grad_norm": 1.3092381954193115,
      "learning_rate": 3.2878988224454344e-06,
      "loss": 1.4822,
      "step": 2585
    },
    {
      "epoch": 4.593250444049733,
      "grad_norm": 1.2389764785766602,
      "learning_rate": 3.259526556419923e-06,
      "loss": 1.2634,
      "step": 2586
    },
    {
      "epoch": 4.595026642984014,
      "grad_norm": 1.3597147464752197,
      "learning_rate": 3.231275209595397e-06,
      "loss": 1.5577,
      "step": 2587
    },
    {
      "epoch": 4.5968028419182945,
      "grad_norm": 1.244462490081787,
      "learning_rate": 3.2031448172841873e-06,
      "loss": 1.3657,
      "step": 2588
    },
    {
      "epoch": 4.5985790408525755,
      "grad_norm": 1.2067551612854004,
      "learning_rate": 3.175135414647423e-06,
      "loss": 1.3416,
      "step": 2589
    },
    {
      "epoch": 4.600355239786856,
      "grad_norm": 1.1954821348190308,
      "learning_rate": 3.1472470366950334e-06,
      "loss": 1.1456,
      "step": 2590
    },
    {
      "epoch": 4.602131438721137,
      "grad_norm": 1.2543452978134155,
      "learning_rate": 3.1194797182856007e-06,
      "loss": 1.1849,
      "step": 2591
    },
    {
      "epoch": 4.603907637655418,
      "grad_norm": 1.2202397584915161,
      "learning_rate": 3.0918334941264725e-06,
      "loss": 1.46,
      "step": 2592
    },
    {
      "epoch": 4.605683836589698,
      "grad_norm": 1.2584004402160645,
      "learning_rate": 3.0643083987736077e-06,
      "loss": 1.2654,
      "step": 2593
    },
    {
      "epoch": 4.607460035523979,
      "grad_norm": 1.246022343635559,
      "learning_rate": 3.036904466631518e-06,
      "loss": 1.3546,
      "step": 2594
    },
    {
      "epoch": 4.609236234458259,
      "grad_norm": 1.286407470703125,
      "learning_rate": 3.0096217319533382e-06,
      "loss": 1.417,
      "step": 2595
    },
    {
      "epoch": 4.61101243339254,
      "grad_norm": 1.142033338546753,
      "learning_rate": 2.982460228840689e-06,
      "loss": 1.3034,
      "step": 2596
    },
    {
      "epoch": 4.61278863232682,
      "grad_norm": 1.2320330142974854,
      "learning_rate": 2.9554199912436464e-06,
      "loss": 1.1691,
      "step": 2597
    },
    {
      "epoch": 4.614564831261101,
      "grad_norm": 1.3658803701400757,
      "learning_rate": 2.928501052960708e-06,
      "loss": 1.1778,
      "step": 2598
    },
    {
      "epoch": 4.616341030195382,
      "grad_norm": 1.1682523488998413,
      "learning_rate": 2.901703447638793e-06,
      "loss": 1.1357,
      "step": 2599
    },
    {
      "epoch": 4.618117229129663,
      "grad_norm": 1.1870968341827393,
      "learning_rate": 2.875027208773118e-06,
      "loss": 1.3351,
      "step": 2600
    },
    {
      "epoch": 4.618117229129663,
      "eval_loss": 2.1283442974090576,
      "eval_runtime": 17.5554,
      "eval_samples_per_second": 57.02,
      "eval_steps_per_second": 28.538,
      "step": 2600
    },
    {
      "epoch": 4.619893428063943,
      "grad_norm": 1.2084606885910034,
      "learning_rate": 2.8484723697072225e-06,
      "loss": 1.431,
      "step": 2601
    },
    {
      "epoch": 4.621669626998224,
      "grad_norm": 1.1829395294189453,
      "learning_rate": 2.8220389636329113e-06,
      "loss": 1.221,
      "step": 2602
    },
    {
      "epoch": 4.623445825932505,
      "grad_norm": 1.2141461372375488,
      "learning_rate": 2.795727023590189e-06,
      "loss": 1.191,
      "step": 2603
    },
    {
      "epoch": 4.625222024866785,
      "grad_norm": 1.2234174013137817,
      "learning_rate": 2.769536582467236e-06,
      "loss": 1.44,
      "step": 2604
    },
    {
      "epoch": 4.626998223801066,
      "grad_norm": 1.2433631420135498,
      "learning_rate": 2.7434676730003884e-06,
      "loss": 1.3371,
      "step": 2605
    },
    {
      "epoch": 4.628774422735346,
      "grad_norm": 1.207512617111206,
      "learning_rate": 2.717520327774048e-06,
      "loss": 1.51,
      "step": 2606
    },
    {
      "epoch": 4.630550621669627,
      "grad_norm": 1.2723119258880615,
      "learning_rate": 2.691694579220694e-06,
      "loss": 1.3055,
      "step": 2607
    },
    {
      "epoch": 4.632326820603907,
      "grad_norm": 1.1905165910720825,
      "learning_rate": 2.6659904596207928e-06,
      "loss": 1.5193,
      "step": 2608
    },
    {
      "epoch": 4.6341030195381885,
      "grad_norm": 1.2471593618392944,
      "learning_rate": 2.640408001102801e-06,
      "loss": 1.1833,
      "step": 2609
    },
    {
      "epoch": 4.635879218472469,
      "grad_norm": 1.282938003540039,
      "learning_rate": 2.614947235643106e-06,
      "loss": 1.4485,
      "step": 2610
    },
    {
      "epoch": 4.63765541740675,
      "grad_norm": 1.2294453382492065,
      "learning_rate": 2.589608195065973e-06,
      "loss": 1.2706,
      "step": 2611
    },
    {
      "epoch": 4.63943161634103,
      "grad_norm": 1.3018922805786133,
      "learning_rate": 2.564390911043546e-06,
      "loss": 1.4061,
      "step": 2612
    },
    {
      "epoch": 4.641207815275311,
      "grad_norm": 1.1919581890106201,
      "learning_rate": 2.539295415095744e-06,
      "loss": 1.2867,
      "step": 2613
    },
    {
      "epoch": 4.642984014209592,
      "grad_norm": 1.2331904172897339,
      "learning_rate": 2.5143217385902863e-06,
      "loss": 1.4027,
      "step": 2614
    },
    {
      "epoch": 4.644760213143872,
      "grad_norm": 1.2086310386657715,
      "learning_rate": 2.4894699127426367e-06,
      "loss": 1.2352,
      "step": 2615
    },
    {
      "epoch": 4.646536412078152,
      "grad_norm": 1.3053361177444458,
      "learning_rate": 2.4647399686159035e-06,
      "loss": 1.5128,
      "step": 2616
    },
    {
      "epoch": 4.648312611012433,
      "grad_norm": 1.2669808864593506,
      "learning_rate": 2.440131937120904e-06,
      "loss": 1.3806,
      "step": 2617
    },
    {
      "epoch": 4.650088809946714,
      "grad_norm": 1.2764605283737183,
      "learning_rate": 2.415645849016057e-06,
      "loss": 1.2749,
      "step": 2618
    },
    {
      "epoch": 4.6518650088809945,
      "grad_norm": 1.2991211414337158,
      "learning_rate": 2.3912817349073357e-06,
      "loss": 1.4161,
      "step": 2619
    },
    {
      "epoch": 4.653641207815276,
      "grad_norm": 1.2555477619171143,
      "learning_rate": 2.367039625248302e-06,
      "loss": 1.302,
      "step": 2620
    },
    {
      "epoch": 4.655417406749556,
      "grad_norm": 1.177780032157898,
      "learning_rate": 2.3429195503399393e-06,
      "loss": 1.4228,
      "step": 2621
    },
    {
      "epoch": 4.657193605683837,
      "grad_norm": 1.2675985097885132,
      "learning_rate": 2.3189215403308097e-06,
      "loss": 1.3689,
      "step": 2622
    },
    {
      "epoch": 4.658969804618117,
      "grad_norm": 1.5986095666885376,
      "learning_rate": 2.2950456252167962e-06,
      "loss": 1.3133,
      "step": 2623
    },
    {
      "epoch": 4.660746003552398,
      "grad_norm": 1.2751880884170532,
      "learning_rate": 2.271291834841227e-06,
      "loss": 1.3178,
      "step": 2624
    },
    {
      "epoch": 4.662522202486678,
      "grad_norm": 1.435442566871643,
      "learning_rate": 2.2476601988947966e-06,
      "loss": 1.537,
      "step": 2625
    },
    {
      "epoch": 4.664298401420959,
      "grad_norm": 1.2510478496551514,
      "learning_rate": 2.2241507469154432e-06,
      "loss": 1.3832,
      "step": 2626
    },
    {
      "epoch": 4.666074600355239,
      "grad_norm": 1.1721614599227905,
      "learning_rate": 2.2007635082884612e-06,
      "loss": 1.1729,
      "step": 2627
    },
    {
      "epoch": 4.66785079928952,
      "grad_norm": 1.1787397861480713,
      "learning_rate": 2.1774985122463456e-06,
      "loss": 1.2638,
      "step": 2628
    },
    {
      "epoch": 4.6696269982238015,
      "grad_norm": 1.2566325664520264,
      "learning_rate": 2.1543557878688113e-06,
      "loss": 1.3313,
      "step": 2629
    },
    {
      "epoch": 4.671403197158082,
      "grad_norm": 1.2586134672164917,
      "learning_rate": 2.1313353640827206e-06,
      "loss": 1.337,
      "step": 2630
    },
    {
      "epoch": 4.673179396092363,
      "grad_norm": 1.1836131811141968,
      "learning_rate": 2.1084372696620892e-06,
      "loss": 1.1783,
      "step": 2631
    },
    {
      "epoch": 4.674955595026643,
      "grad_norm": 1.2367753982543945,
      "learning_rate": 2.085661533228034e-06,
      "loss": 1.4502,
      "step": 2632
    },
    {
      "epoch": 4.676731793960924,
      "grad_norm": 1.207155704498291,
      "learning_rate": 2.0630081832486937e-06,
      "loss": 1.4308,
      "step": 2633
    },
    {
      "epoch": 4.678507992895204,
      "grad_norm": 1.1923253536224365,
      "learning_rate": 2.0404772480392852e-06,
      "loss": 1.001,
      "step": 2634
    },
    {
      "epoch": 4.680284191829485,
      "grad_norm": 1.0890735387802124,
      "learning_rate": 2.0180687557619816e-06,
      "loss": 1.4785,
      "step": 2635
    },
    {
      "epoch": 4.682060390763765,
      "grad_norm": 1.1253732442855835,
      "learning_rate": 1.9957827344259105e-06,
      "loss": 1.0616,
      "step": 2636
    },
    {
      "epoch": 4.683836589698046,
      "grad_norm": 1.2171945571899414,
      "learning_rate": 1.9736192118871344e-06,
      "loss": 1.202,
      "step": 2637
    },
    {
      "epoch": 4.685612788632326,
      "grad_norm": 1.2766793966293335,
      "learning_rate": 1.9515782158485817e-06,
      "loss": 1.3711,
      "step": 2638
    },
    {
      "epoch": 4.6873889875666075,
      "grad_norm": 1.2046490907669067,
      "learning_rate": 1.92965977386006e-06,
      "loss": 1.1593,
      "step": 2639
    },
    {
      "epoch": 4.6891651865008885,
      "grad_norm": 1.3064717054367065,
      "learning_rate": 1.907863913318153e-06,
      "loss": 1.2036,
      "step": 2640
    },
    {
      "epoch": 4.690941385435169,
      "grad_norm": 1.265303373336792,
      "learning_rate": 1.8861906614662693e-06,
      "loss": 1.2137,
      "step": 2641
    },
    {
      "epoch": 4.69271758436945,
      "grad_norm": 1.1793931722640991,
      "learning_rate": 1.8646400453945278e-06,
      "loss": 1.6364,
      "step": 2642
    },
    {
      "epoch": 4.69449378330373,
      "grad_norm": 1.2836805582046509,
      "learning_rate": 1.843212092039759e-06,
      "loss": 1.134,
      "step": 2643
    },
    {
      "epoch": 4.696269982238011,
      "grad_norm": 1.1510850191116333,
      "learning_rate": 1.8219068281855155e-06,
      "loss": 1.008,
      "step": 2644
    },
    {
      "epoch": 4.698046181172291,
      "grad_norm": 1.2667627334594727,
      "learning_rate": 1.8007242804619628e-06,
      "loss": 1.21,
      "step": 2645
    },
    {
      "epoch": 4.699822380106572,
      "grad_norm": 1.118364930152893,
      "learning_rate": 1.7796644753458769e-06,
      "loss": 1.0982,
      "step": 2646
    },
    {
      "epoch": 4.701598579040852,
      "grad_norm": 1.2731666564941406,
      "learning_rate": 1.7587274391606456e-06,
      "loss": 1.3656,
      "step": 2647
    },
    {
      "epoch": 4.703374777975133,
      "grad_norm": 1.241353988647461,
      "learning_rate": 1.7379131980761575e-06,
      "loss": 1.6204,
      "step": 2648
    },
    {
      "epoch": 4.7051509769094135,
      "grad_norm": 1.3483392000198364,
      "learning_rate": 1.7172217781088461e-06,
      "loss": 1.2607,
      "step": 2649
    },
    {
      "epoch": 4.706927175843695,
      "grad_norm": 1.3554072380065918,
      "learning_rate": 1.696653205121612e-06,
      "loss": 1.3223,
      "step": 2650
    },
    {
      "epoch": 4.708703374777976,
      "grad_norm": 1.1453256607055664,
      "learning_rate": 1.6762075048238347e-06,
      "loss": 1.0886,
      "step": 2651
    },
    {
      "epoch": 4.710479573712256,
      "grad_norm": 1.2103523015975952,
      "learning_rate": 1.655884702771271e-06,
      "loss": 1.4235,
      "step": 2652
    },
    {
      "epoch": 4.712255772646536,
      "grad_norm": 1.2068229913711548,
      "learning_rate": 1.6356848243660682e-06,
      "loss": 1.3435,
      "step": 2653
    },
    {
      "epoch": 4.714031971580817,
      "grad_norm": 1.1875008344650269,
      "learning_rate": 1.6156078948567743e-06,
      "loss": 1.3063,
      "step": 2654
    },
    {
      "epoch": 4.715808170515098,
      "grad_norm": 1.3085154294967651,
      "learning_rate": 1.595653939338204e-06,
      "loss": 1.4347,
      "step": 2655
    },
    {
      "epoch": 4.717584369449378,
      "grad_norm": 1.116800308227539,
      "learning_rate": 1.5758229827514848e-06,
      "loss": 1.1255,
      "step": 2656
    },
    {
      "epoch": 4.719360568383659,
      "grad_norm": 1.3264358043670654,
      "learning_rate": 1.5561150498840105e-06,
      "loss": 1.3784,
      "step": 2657
    },
    {
      "epoch": 4.721136767317939,
      "grad_norm": 1.339858889579773,
      "learning_rate": 1.5365301653693875e-06,
      "loss": 1.3271,
      "step": 2658
    },
    {
      "epoch": 4.7229129662522205,
      "grad_norm": 1.279174566268921,
      "learning_rate": 1.517068353687423e-06,
      "loss": 1.4416,
      "step": 2659
    },
    {
      "epoch": 4.724689165186501,
      "grad_norm": 1.2441668510437012,
      "learning_rate": 1.4977296391641026e-06,
      "loss": 1.2669,
      "step": 2660
    },
    {
      "epoch": 4.726465364120782,
      "grad_norm": 1.371970772743225,
      "learning_rate": 1.4785140459715463e-06,
      "loss": 1.3836,
      "step": 2661
    },
    {
      "epoch": 4.728241563055062,
      "grad_norm": 1.163944959640503,
      "learning_rate": 1.4594215981279635e-06,
      "loss": 1.2978,
      "step": 2662
    },
    {
      "epoch": 4.730017761989343,
      "grad_norm": 1.302632212638855,
      "learning_rate": 1.4404523194976648e-06,
      "loss": 1.1778,
      "step": 2663
    },
    {
      "epoch": 4.731793960923623,
      "grad_norm": 1.2837646007537842,
      "learning_rate": 1.4216062337909953e-06,
      "loss": 1.443,
      "step": 2664
    },
    {
      "epoch": 4.733570159857904,
      "grad_norm": 1.3721659183502197,
      "learning_rate": 1.4028833645643113e-06,
      "loss": 1.3515,
      "step": 2665
    },
    {
      "epoch": 4.735346358792185,
      "grad_norm": 1.2223520278930664,
      "learning_rate": 1.3842837352199488e-06,
      "loss": 1.1856,
      "step": 2666
    },
    {
      "epoch": 4.737122557726465,
      "grad_norm": 1.2024587392807007,
      "learning_rate": 1.3658073690062446e-06,
      "loss": 1.4111,
      "step": 2667
    },
    {
      "epoch": 4.738898756660746,
      "grad_norm": 1.276482105255127,
      "learning_rate": 1.3474542890174246e-06,
      "loss": 1.5673,
      "step": 2668
    },
    {
      "epoch": 4.7406749555950265,
      "grad_norm": 1.209638237953186,
      "learning_rate": 1.329224518193628e-06,
      "loss": 1.1277,
      "step": 2669
    },
    {
      "epoch": 4.7424511545293075,
      "grad_norm": 1.2437320947647095,
      "learning_rate": 1.31111807932085e-06,
      "loss": 1.332,
      "step": 2670
    },
    {
      "epoch": 4.744227353463588,
      "grad_norm": 1.2138280868530273,
      "learning_rate": 1.2931349950309535e-06,
      "loss": 1.1708,
      "step": 2671
    },
    {
      "epoch": 4.746003552397869,
      "grad_norm": 1.3184157609939575,
      "learning_rate": 1.275275287801614e-06,
      "loss": 1.4442,
      "step": 2672
    },
    {
      "epoch": 4.747779751332149,
      "grad_norm": 1.1483205556869507,
      "learning_rate": 1.2575389799562632e-06,
      "loss": 1.514,
      "step": 2673
    },
    {
      "epoch": 4.74955595026643,
      "grad_norm": 1.3170256614685059,
      "learning_rate": 1.2399260936641455e-06,
      "loss": 1.3938,
      "step": 2674
    },
    {
      "epoch": 4.75133214920071,
      "grad_norm": 1.2520159482955933,
      "learning_rate": 1.222436650940173e-06,
      "loss": 1.2568,
      "step": 2675
    },
    {
      "epoch": 4.753108348134991,
      "grad_norm": 1.1921190023422241,
      "learning_rate": 1.2050706736450147e-06,
      "loss": 1.2517,
      "step": 2676
    },
    {
      "epoch": 4.754884547069272,
      "grad_norm": 1.174001932144165,
      "learning_rate": 1.1878281834849847e-06,
      "loss": 1.3256,
      "step": 2677
    },
    {
      "epoch": 4.756660746003552,
      "grad_norm": 1.3164125680923462,
      "learning_rate": 1.170709202012077e-06,
      "loss": 1.2572,
      "step": 2678
    },
    {
      "epoch": 4.758436944937833,
      "grad_norm": 1.2814563512802124,
      "learning_rate": 1.1537137506238527e-06,
      "loss": 1.4888,
      "step": 2679
    },
    {
      "epoch": 4.760213143872114,
      "grad_norm": 1.1962329149246216,
      "learning_rate": 1.1368418505635302e-06,
      "loss": 1.2511,
      "step": 2680
    },
    {
      "epoch": 4.761989342806395,
      "grad_norm": 1.1670013666152954,
      "learning_rate": 1.1200935229198628e-06,
      "loss": 1.154,
      "step": 2681
    },
    {
      "epoch": 4.763765541740675,
      "grad_norm": 1.3227853775024414,
      "learning_rate": 1.1034687886271379e-06,
      "loss": 1.3366,
      "step": 2682
    },
    {
      "epoch": 4.765541740674956,
      "grad_norm": 1.2934702634811401,
      "learning_rate": 1.0869676684652108e-06,
      "loss": 1.3156,
      "step": 2683
    },
    {
      "epoch": 4.767317939609236,
      "grad_norm": 1.2277668714523315,
      "learning_rate": 1.0705901830593612e-06,
      "loss": 1.3966,
      "step": 2684
    },
    {
      "epoch": 4.769094138543517,
      "grad_norm": 1.2199625968933105,
      "learning_rate": 1.0543363528803696e-06,
      "loss": 1.1623,
      "step": 2685
    },
    {
      "epoch": 4.770870337477797,
      "grad_norm": 1.3511923551559448,
      "learning_rate": 1.0382061982444735e-06,
      "loss": 1.5321,
      "step": 2686
    },
    {
      "epoch": 4.772646536412078,
      "grad_norm": 1.2613236904144287,
      "learning_rate": 1.0221997393132564e-06,
      "loss": 1.3507,
      "step": 2687
    },
    {
      "epoch": 4.774422735346358,
      "grad_norm": 1.2481199502944946,
      "learning_rate": 1.0063169960937702e-06,
      "loss": 1.3339,
      "step": 2688
    },
    {
      "epoch": 4.7761989342806395,
      "grad_norm": 1.1761113405227661,
      "learning_rate": 9.905579884383787e-07,
      "loss": 1.143,
      "step": 2689
    },
    {
      "epoch": 4.77797513321492,
      "grad_norm": 1.3918659687042236,
      "learning_rate": 9.749227360448143e-07,
      "loss": 1.1972,
      "step": 2690
    },
    {
      "epoch": 4.779751332149201,
      "grad_norm": 1.400816559791565,
      "learning_rate": 9.594112584561e-07,
      "loss": 1.6249,
      "step": 2691
    },
    {
      "epoch": 4.781527531083482,
      "grad_norm": 1.1638349294662476,
      "learning_rate": 9.440235750605486e-07,
      "loss": 1.2524,
      "step": 2692
    },
    {
      "epoch": 4.783303730017762,
      "grad_norm": 1.2636785507202148,
      "learning_rate": 9.28759705091764e-07,
      "loss": 1.419,
      "step": 2693
    },
    {
      "epoch": 4.785079928952043,
      "grad_norm": 1.278276801109314,
      "learning_rate": 9.136196676285735e-07,
      "loss": 1.2873,
      "step": 2694
    },
    {
      "epoch": 4.786856127886323,
      "grad_norm": 1.2607709169387817,
      "learning_rate": 8.986034815950172e-07,
      "loss": 1.2661,
      "step": 2695
    },
    {
      "epoch": 4.788632326820604,
      "grad_norm": 1.155995488166809,
      "learning_rate": 8.83711165760337e-07,
      "loss": 1.161,
      "step": 2696
    },
    {
      "epoch": 4.790408525754884,
      "grad_norm": 1.2184901237487793,
      "learning_rate": 8.689427387389426e-07,
      "loss": 1.2918,
      "step": 2697
    },
    {
      "epoch": 4.792184724689165,
      "grad_norm": 1.2188822031021118,
      "learning_rate": 8.542982189903903e-07,
      "loss": 1.425,
      "step": 2698
    },
    {
      "epoch": 4.7939609236234455,
      "grad_norm": 1.192332148551941,
      "learning_rate": 8.397776248193712e-07,
      "loss": 1.471,
      "step": 2699
    },
    {
      "epoch": 4.7957371225577266,
      "grad_norm": 1.284149408340454,
      "learning_rate": 8.253809743756668e-07,
      "loss": 1.2312,
      "step": 2700
    },
    {
      "epoch": 4.7957371225577266,
      "eval_loss": 2.1285109519958496,
      "eval_runtime": 17.6338,
      "eval_samples_per_second": 56.766,
      "eval_steps_per_second": 28.411,
      "step": 2700
    },
    {
      "epoch": 4.797513321492007,
      "grad_norm": 1.2944841384887695,
      "learning_rate": 8.111082856541385e-07,
      "loss": 1.2966,
      "step": 2701
    },
    {
      "epoch": 4.799289520426288,
      "grad_norm": 1.3524878025054932,
      "learning_rate": 7.969595764946936e-07,
      "loss": 1.3174,
      "step": 2702
    },
    {
      "epoch": 4.801065719360569,
      "grad_norm": 1.311034083366394,
      "learning_rate": 7.829348645822965e-07,
      "loss": 1.2455,
      "step": 2703
    },
    {
      "epoch": 4.802841918294849,
      "grad_norm": 1.3349539041519165,
      "learning_rate": 7.690341674469026e-07,
      "loss": 1.4837,
      "step": 2704
    },
    {
      "epoch": 4.80461811722913,
      "grad_norm": 1.241076946258545,
      "learning_rate": 7.552575024634689e-07,
      "loss": 1.4716,
      "step": 2705
    },
    {
      "epoch": 4.80639431616341,
      "grad_norm": 1.1353073120117188,
      "learning_rate": 7.41604886851921e-07,
      "loss": 1.1791,
      "step": 2706
    },
    {
      "epoch": 4.808170515097691,
      "grad_norm": 1.2307628393173218,
      "learning_rate": 7.280763376771304e-07,
      "loss": 1.4386,
      "step": 2707
    },
    {
      "epoch": 4.809946714031971,
      "grad_norm": 1.2833586931228638,
      "learning_rate": 7.146718718488709e-07,
      "loss": 1.2435,
      "step": 2708
    },
    {
      "epoch": 4.811722912966252,
      "grad_norm": 1.2440038919448853,
      "learning_rate": 7.013915061218513e-07,
      "loss": 1.3569,
      "step": 2709
    },
    {
      "epoch": 4.813499111900533,
      "grad_norm": 1.262081265449524,
      "learning_rate": 6.882352570956485e-07,
      "loss": 1.1653,
      "step": 2710
    },
    {
      "epoch": 4.815275310834814,
      "grad_norm": 1.2056461572647095,
      "learning_rate": 6.752031412147086e-07,
      "loss": 1.3997,
      "step": 2711
    },
    {
      "epoch": 4.817051509769094,
      "grad_norm": 1.224522590637207,
      "learning_rate": 6.622951747683126e-07,
      "loss": 1.208,
      "step": 2712
    },
    {
      "epoch": 4.818827708703375,
      "grad_norm": 1.2017239332199097,
      "learning_rate": 6.495113738905656e-07,
      "loss": 1.286,
      "step": 2713
    },
    {
      "epoch": 4.820603907637656,
      "grad_norm": 1.2419912815093994,
      "learning_rate": 6.368517545603636e-07,
      "loss": 1.1572,
      "step": 2714
    },
    {
      "epoch": 4.822380106571936,
      "grad_norm": 1.201696515083313,
      "learning_rate": 6.243163326014267e-07,
      "loss": 1.4343,
      "step": 2715
    },
    {
      "epoch": 4.824156305506216,
      "grad_norm": 1.3039659261703491,
      "learning_rate": 6.11905123682166e-07,
      "loss": 1.3408,
      "step": 2716
    },
    {
      "epoch": 4.825932504440497,
      "grad_norm": 1.2412657737731934,
      "learning_rate": 5.996181433158054e-07,
      "loss": 1.4259,
      "step": 2717
    },
    {
      "epoch": 4.827708703374778,
      "grad_norm": 1.2799042463302612,
      "learning_rate": 5.874554068602378e-07,
      "loss": 1.4209,
      "step": 2718
    },
    {
      "epoch": 4.8294849023090585,
      "grad_norm": 1.1603227853775024,
      "learning_rate": 5.7541692951808e-07,
      "loss": 1.2491,
      "step": 2719
    },
    {
      "epoch": 4.8312611012433395,
      "grad_norm": 1.1639766693115234,
      "learning_rate": 5.635027263366399e-07,
      "loss": 1.2122,
      "step": 2720
    },
    {
      "epoch": 4.83303730017762,
      "grad_norm": 1.1931298971176147,
      "learning_rate": 5.51712812207883e-07,
      "loss": 1.1407,
      "step": 2721
    },
    {
      "epoch": 4.834813499111901,
      "grad_norm": 1.2841533422470093,
      "learning_rate": 5.400472018684322e-07,
      "loss": 1.3446,
      "step": 2722
    },
    {
      "epoch": 4.836589698046181,
      "grad_norm": 1.2431780099868774,
      "learning_rate": 5.285059098995126e-07,
      "loss": 1.0934,
      "step": 2723
    },
    {
      "epoch": 4.838365896980462,
      "grad_norm": 1.2014931440353394,
      "learning_rate": 5.17088950726996e-07,
      "loss": 1.5228,
      "step": 2724
    },
    {
      "epoch": 4.840142095914742,
      "grad_norm": 1.1592535972595215,
      "learning_rate": 5.057963386213116e-07,
      "loss": 0.8612,
      "step": 2725
    },
    {
      "epoch": 4.841918294849023,
      "grad_norm": 1.3047428131103516,
      "learning_rate": 4.946280876974796e-07,
      "loss": 1.201,
      "step": 2726
    },
    {
      "epoch": 4.843694493783303,
      "grad_norm": 1.3095287084579468,
      "learning_rate": 4.83584211915078e-07,
      "loss": 1.0811,
      "step": 2727
    },
    {
      "epoch": 4.845470692717584,
      "grad_norm": 1.2432520389556885,
      "learning_rate": 4.726647250782312e-07,
      "loss": 1.212,
      "step": 2728
    },
    {
      "epoch": 4.847246891651865,
      "grad_norm": 1.0903348922729492,
      "learning_rate": 4.6186964083556604e-07,
      "loss": 1.0064,
      "step": 2729
    },
    {
      "epoch": 4.849023090586146,
      "grad_norm": 1.1476950645446777,
      "learning_rate": 4.5119897268023347e-07,
      "loss": 1.3278,
      "step": 2730
    },
    {
      "epoch": 4.850799289520427,
      "grad_norm": 1.146775245666504,
      "learning_rate": 4.406527339498423e-07,
      "loss": 1.3587,
      "step": 2731
    },
    {
      "epoch": 4.852575488454707,
      "grad_norm": 1.211555004119873,
      "learning_rate": 4.3023093782652566e-07,
      "loss": 1.3119,
      "step": 2732
    },
    {
      "epoch": 4.854351687388988,
      "grad_norm": 1.2235983610153198,
      "learning_rate": 4.1993359733681905e-07,
      "loss": 1.2739,
      "step": 2733
    },
    {
      "epoch": 4.856127886323268,
      "grad_norm": 1.306159257888794,
      "learning_rate": 4.097607253517266e-07,
      "loss": 1.4812,
      "step": 2734
    },
    {
      "epoch": 4.857904085257549,
      "grad_norm": 1.2804489135742188,
      "learning_rate": 3.9971233458665493e-07,
      "loss": 1.191,
      "step": 2735
    },
    {
      "epoch": 4.859680284191829,
      "grad_norm": 1.209984540939331,
      "learning_rate": 3.89788437601446e-07,
      "loss": 1.4534,
      "step": 2736
    },
    {
      "epoch": 4.86145648312611,
      "grad_norm": 1.2836124897003174,
      "learning_rate": 3.799890468002998e-07,
      "loss": 1.5946,
      "step": 2737
    },
    {
      "epoch": 4.86323268206039,
      "grad_norm": 1.2250785827636719,
      "learning_rate": 3.7031417443180727e-07,
      "loss": 1.4574,
      "step": 2738
    },
    {
      "epoch": 4.865008880994671,
      "grad_norm": 1.1570045948028564,
      "learning_rate": 3.607638325889395e-07,
      "loss": 1.4387,
      "step": 2739
    },
    {
      "epoch": 4.8667850799289525,
      "grad_norm": 1.2246911525726318,
      "learning_rate": 3.5133803320896994e-07,
      "loss": 1.4437,
      "step": 2740
    },
    {
      "epoch": 4.868561278863233,
      "grad_norm": 1.0957883596420288,
      "learning_rate": 3.420367880735409e-07,
      "loss": 1.2644,
      "step": 2741
    },
    {
      "epoch": 4.870337477797514,
      "grad_norm": 1.2238246202468872,
      "learning_rate": 3.3286010880858586e-07,
      "loss": 1.4431,
      "step": 2742
    },
    {
      "epoch": 4.872113676731794,
      "grad_norm": 1.15561044216156,
      "learning_rate": 3.2380800688435185e-07,
      "loss": 1.1851,
      "step": 2743
    },
    {
      "epoch": 4.873889875666075,
      "grad_norm": 1.2291994094848633,
      "learning_rate": 3.1488049361536595e-07,
      "loss": 1.2889,
      "step": 2744
    },
    {
      "epoch": 4.875666074600355,
      "grad_norm": 1.339746117591858,
      "learning_rate": 3.060775801604354e-07,
      "loss": 1.5835,
      "step": 2745
    },
    {
      "epoch": 4.877442273534636,
      "grad_norm": 1.28108811378479,
      "learning_rate": 2.973992775226031e-07,
      "loss": 1.4404,
      "step": 2746
    },
    {
      "epoch": 4.879218472468916,
      "grad_norm": 1.5110136270523071,
      "learning_rate": 2.888455965491921e-07,
      "loss": 1.3244,
      "step": 2747
    },
    {
      "epoch": 4.880994671403197,
      "grad_norm": 1.187223196029663,
      "learning_rate": 2.804165479317389e-07,
      "loss": 1.3696,
      "step": 2748
    },
    {
      "epoch": 4.8827708703374775,
      "grad_norm": 1.3405297994613647,
      "learning_rate": 2.721121422059825e-07,
      "loss": 1.3602,
      "step": 2749
    },
    {
      "epoch": 4.8845470692717585,
      "grad_norm": 1.2410194873809814,
      "learning_rate": 2.639323897518975e-07,
      "loss": 1.3011,
      "step": 2750
    },
    {
      "epoch": 4.88632326820604,
      "grad_norm": 1.2214300632476807,
      "learning_rate": 2.558773007936277e-07,
      "loss": 1.2865,
      "step": 2751
    },
    {
      "epoch": 4.88809946714032,
      "grad_norm": 1.2657369375228882,
      "learning_rate": 2.479468853994971e-07,
      "loss": 1.4015,
      "step": 2752
    },
    {
      "epoch": 4.8898756660746,
      "grad_norm": 1.240657925605774,
      "learning_rate": 2.401411534819986e-07,
      "loss": 1.0311,
      "step": 2753
    },
    {
      "epoch": 4.891651865008881,
      "grad_norm": 1.2469576597213745,
      "learning_rate": 2.324601147978056e-07,
      "loss": 1.3614,
      "step": 2754
    },
    {
      "epoch": 4.893428063943162,
      "grad_norm": 1.233723759651184,
      "learning_rate": 2.2490377894768267e-07,
      "loss": 1.1669,
      "step": 2755
    },
    {
      "epoch": 4.895204262877442,
      "grad_norm": 1.2540767192840576,
      "learning_rate": 2.1747215537655242e-07,
      "loss": 1.326,
      "step": 2756
    },
    {
      "epoch": 4.896980461811723,
      "grad_norm": 1.3162319660186768,
      "learning_rate": 2.1016525337347326e-07,
      "loss": 1.3423,
      "step": 2757
    },
    {
      "epoch": 4.898756660746003,
      "grad_norm": 1.2159725427627563,
      "learning_rate": 2.029830820715728e-07,
      "loss": 1.2718,
      "step": 2758
    },
    {
      "epoch": 4.900532859680284,
      "grad_norm": 1.1966946125030518,
      "learning_rate": 1.9592565044809219e-07,
      "loss": 1.3499,
      "step": 2759
    },
    {
      "epoch": 4.902309058614565,
      "grad_norm": 1.1971229314804077,
      "learning_rate": 1.889929673243529e-07,
      "loss": 1.1581,
      "step": 2760
    },
    {
      "epoch": 4.904085257548846,
      "grad_norm": 1.1592243909835815,
      "learning_rate": 1.8218504136576776e-07,
      "loss": 1.0526,
      "step": 2761
    },
    {
      "epoch": 4.905861456483126,
      "grad_norm": 1.3094004392623901,
      "learning_rate": 1.7550188108177433e-07,
      "loss": 1.1781,
      "step": 2762
    },
    {
      "epoch": 4.907637655417407,
      "grad_norm": 1.2804594039916992,
      "learning_rate": 1.6894349482589055e-07,
      "loss": 1.3022,
      "step": 2763
    },
    {
      "epoch": 4.909413854351687,
      "grad_norm": 1.3667536973953247,
      "learning_rate": 1.6250989079567013e-07,
      "loss": 1.4192,
      "step": 2764
    },
    {
      "epoch": 4.911190053285968,
      "grad_norm": 1.5023633241653442,
      "learning_rate": 1.562010770326916e-07,
      "loss": 1.2557,
      "step": 2765
    },
    {
      "epoch": 4.912966252220249,
      "grad_norm": 1.291579246520996,
      "learning_rate": 1.5001706142253603e-07,
      "loss": 1.333,
      "step": 2766
    },
    {
      "epoch": 4.914742451154529,
      "grad_norm": 1.362815499305725,
      "learning_rate": 1.4395785169484255e-07,
      "loss": 1.3238,
      "step": 2767
    },
    {
      "epoch": 4.91651865008881,
      "grad_norm": 1.203292727470398,
      "learning_rate": 1.380234554232085e-07,
      "loss": 1.3792,
      "step": 2768
    },
    {
      "epoch": 4.91829484902309,
      "grad_norm": 1.192392110824585,
      "learning_rate": 1.322138800252337e-07,
      "loss": 1.2034,
      "step": 2769
    },
    {
      "epoch": 4.9200710479573715,
      "grad_norm": 1.1944301128387451,
      "learning_rate": 1.2652913276250955e-07,
      "loss": 1.0732,
      "step": 2770
    },
    {
      "epoch": 4.921847246891652,
      "grad_norm": 1.2445124387741089,
      "learning_rate": 1.2096922074060768e-07,
      "loss": 1.3459,
      "step": 2771
    },
    {
      "epoch": 4.923623445825933,
      "grad_norm": 1.3282572031021118,
      "learning_rate": 1.1553415090902464e-07,
      "loss": 1.3577,
      "step": 2772
    },
    {
      "epoch": 4.925399644760213,
      "grad_norm": 1.2688840627670288,
      "learning_rate": 1.1022393006124843e-07,
      "loss": 1.3556,
      "step": 2773
    },
    {
      "epoch": 4.927175843694494,
      "grad_norm": 1.2146180868148804,
      "learning_rate": 1.0503856483471408e-07,
      "loss": 1.2572,
      "step": 2774
    },
    {
      "epoch": 4.928952042628774,
      "grad_norm": 1.4259967803955078,
      "learning_rate": 9.99780617107815e-08,
      "loss": 1.4588,
      "step": 2775
    },
    {
      "epoch": 4.930728241563055,
      "grad_norm": 1.2452341318130493,
      "learning_rate": 9.504242701473542e-08,
      "loss": 1.1339,
      "step": 2776
    },
    {
      "epoch": 4.932504440497336,
      "grad_norm": 1.3178210258483887,
      "learning_rate": 9.023166691580764e-08,
      "loss": 1.3717,
      "step": 2777
    },
    {
      "epoch": 4.934280639431616,
      "grad_norm": 1.1494072675704956,
      "learning_rate": 8.55457874271215e-08,
      "loss": 1.309,
      "step": 2778
    },
    {
      "epoch": 4.9360568383658965,
      "grad_norm": 1.3621633052825928,
      "learning_rate": 8.098479440571405e-08,
      "loss": 1.4542,
      "step": 2779
    },
    {
      "epoch": 4.9378330373001775,
      "grad_norm": 1.171095609664917,
      "learning_rate": 7.654869355252504e-08,
      "loss": 1.414,
      "step": 2780
    },
    {
      "epoch": 4.939609236234459,
      "grad_norm": 1.1982126235961914,
      "learning_rate": 7.22374904124079e-08,
      "loss": 1.4731,
      "step": 2781
    },
    {
      "epoch": 4.941385435168739,
      "grad_norm": 1.2545669078826904,
      "learning_rate": 6.805119037405216e-08,
      "loss": 1.2819,
      "step": 2782
    },
    {
      "epoch": 4.94316163410302,
      "grad_norm": 1.1240475177764893,
      "learning_rate": 6.398979867007215e-08,
      "loss": 1.1346,
      "step": 2783
    },
    {
      "epoch": 4.9449378330373,
      "grad_norm": 1.3230018615722656,
      "learning_rate": 6.005332037694045e-08,
      "loss": 1.3317,
      "step": 2784
    },
    {
      "epoch": 4.946714031971581,
      "grad_norm": 1.2349779605865479,
      "learning_rate": 5.6241760414987856e-08,
      "loss": 1.2155,
      "step": 2785
    },
    {
      "epoch": 4.948490230905861,
      "grad_norm": 1.3231866359710693,
      "learning_rate": 5.255512354842562e-08,
      "loss": 1.4377,
      "step": 2786
    },
    {
      "epoch": 4.950266429840142,
      "grad_norm": 1.2742197513580322,
      "learning_rate": 4.8993414385289923e-08,
      "loss": 1.0486,
      "step": 2787
    },
    {
      "epoch": 4.952042628774422,
      "grad_norm": 1.3181558847427368,
      "learning_rate": 4.555663737748628e-08,
      "loss": 1.5401,
      "step": 2788
    },
    {
      "epoch": 4.953818827708703,
      "grad_norm": 1.3980392217636108,
      "learning_rate": 4.2244796820767316e-08,
      "loss": 1.2635,
      "step": 2789
    },
    {
      "epoch": 4.955595026642984,
      "grad_norm": 1.2931008338928223,
      "learning_rate": 3.905789685471062e-08,
      "loss": 1.1294,
      "step": 2790
    },
    {
      "epoch": 4.957371225577265,
      "grad_norm": 1.1792025566101074,
      "learning_rate": 3.599594146271867e-08,
      "loss": 1.2357,
      "step": 2791
    },
    {
      "epoch": 4.959147424511546,
      "grad_norm": 1.1946519613265991,
      "learning_rate": 3.305893447205222e-08,
      "loss": 1.5032,
      "step": 2792
    },
    {
      "epoch": 4.960923623445826,
      "grad_norm": 1.3013267517089844,
      "learning_rate": 3.0246879553763596e-08,
      "loss": 1.4581,
      "step": 2793
    },
    {
      "epoch": 4.962699822380107,
      "grad_norm": 1.1874427795410156,
      "learning_rate": 2.7559780222741193e-08,
      "loss": 1.3472,
      "step": 2794
    },
    {
      "epoch": 4.964476021314387,
      "grad_norm": 1.181624412536621,
      "learning_rate": 2.4997639837687213e-08,
      "loss": 1.4408,
      "step": 2795
    },
    {
      "epoch": 4.966252220248668,
      "grad_norm": 1.2828317880630493,
      "learning_rate": 2.2560461601095485e-08,
      "loss": 1.3032,
      "step": 2796
    },
    {
      "epoch": 4.968028419182948,
      "grad_norm": 1.2018686532974243,
      "learning_rate": 2.0248248559273654e-08,
      "loss": 1.3532,
      "step": 2797
    },
    {
      "epoch": 4.969804618117229,
      "grad_norm": 1.173392415046692,
      "learning_rate": 1.8061003602354298e-08,
      "loss": 1.1382,
      "step": 2798
    },
    {
      "epoch": 4.9715808170515094,
      "grad_norm": 1.3412151336669922,
      "learning_rate": 1.5998729464239415e-08,
      "loss": 1.4277,
      "step": 2799
    },
    {
      "epoch": 4.9733570159857905,
      "grad_norm": 1.2267355918884277,
      "learning_rate": 1.4061428722633718e-08,
      "loss": 1.2418,
      "step": 2800
    },
    {
      "epoch": 4.9733570159857905,
      "eval_loss": 2.1283342838287354,
      "eval_runtime": 17.4958,
      "eval_samples_per_second": 57.214,
      "eval_steps_per_second": 28.635,
      "step": 2800
    },
    {
      "epoch": 4.975133214920071,
      "grad_norm": 1.2063041925430298,
      "learning_rate": 1.2249103799044647e-08,
      "loss": 1.1416,
      "step": 2801
    },
    {
      "epoch": 4.976909413854352,
      "grad_norm": 1.1833281517028809,
      "learning_rate": 1.0561756958737955e-08,
      "loss": 1.0059,
      "step": 2802
    },
    {
      "epoch": 4.978685612788633,
      "grad_norm": 1.3578083515167236,
      "learning_rate": 8.999390310815425e-09,
      "loss": 1.3718,
      "step": 2803
    },
    {
      "epoch": 4.980461811722913,
      "grad_norm": 1.2096835374832153,
      "learning_rate": 7.562005808103844e-09,
      "loss": 1.3545,
      "step": 2804
    },
    {
      "epoch": 4.982238010657194,
      "grad_norm": 1.234529972076416,
      "learning_rate": 6.2496052472549304e-09,
      "loss": 1.1086,
      "step": 2805
    },
    {
      "epoch": 4.984014209591474,
      "grad_norm": 1.090363621711731,
      "learning_rate": 5.062190268667611e-09,
      "loss": 1.1212,
      "step": 2806
    },
    {
      "epoch": 4.985790408525755,
      "grad_norm": 1.279275894165039,
      "learning_rate": 3.999762356554637e-09,
      "loss": 1.4263,
      "step": 2807
    },
    {
      "epoch": 4.987566607460035,
      "grad_norm": 1.282999873161316,
      "learning_rate": 3.062322838864873e-09,
      "loss": 1.2871,
      "step": 2808
    },
    {
      "epoch": 4.989342806394316,
      "grad_norm": 1.2213886976242065,
      "learning_rate": 2.2498728873387996e-09,
      "loss": 1.404,
      "step": 2809
    },
    {
      "epoch": 4.9911190053285965,
      "grad_norm": 1.1816531419754028,
      "learning_rate": 1.5624135174974186e-09,
      "loss": 1.2306,
      "step": 2810
    },
    {
      "epoch": 4.992895204262878,
      "grad_norm": 1.2020853757858276,
      "learning_rate": 9.999455885978394e-10,
      "loss": 1.4602,
      "step": 2811
    },
    {
      "epoch": 4.994671403197158,
      "grad_norm": 1.3105171918869019,
      "learning_rate": 5.624698036998944e-10,
      "loss": 1.3674,
      "step": 2812
    },
    {
      "epoch": 4.996447602131439,
      "grad_norm": 1.1861218214035034,
      "learning_rate": 2.4998670961062784e-10,
      "loss": 1.2834,
      "step": 2813
    },
    {
      "epoch": 4.99822380106572,
      "grad_norm": 1.2959518432617188,
      "learning_rate": 6.249669692870441e-11,
      "loss": 1.1604,
      "step": 2814
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.600921630859375,
      "learning_rate": 0.0,
      "loss": 1.1658,
      "step": 2815
    }
  ],
  "logging_steps": 1,
  "max_steps": 2815,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.5314839135891456e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
